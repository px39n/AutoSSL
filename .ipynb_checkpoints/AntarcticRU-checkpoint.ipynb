{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcde884-76b6-451f-902b-46ede0ee7e1f",
   "metadata": {},
   "source": [
    "# This a a script to auto scrap the Antarctic RU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a7d609c8-b124-44e5-a9ad-f5bc3c4548b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "root_dir = \"D:/Antarctica/\"\n",
    "html_links = [\"https://web.archive.org/web/20160218060422if_/http://www.aari.nw.ru/projects/Antarctic/data/data.asp?lang=0&station=3\",\n",
    "              \"https://web.archive.org/web/20131012015513if_/http://www.aari.nw.ru/projects/Antarctic/data/data.asp?lang=0&station=2\",\n",
    "              \"https://web.archive.org/web/20060504000027if_/http://www.aari.nw.ru:80/projects/Antarctic/data/data.asp?lang=0&station=4\",\n",
    "              \"https://web.archive.org/web/20151105020149if_/http://www.aari.nw.ru/projects/Antarctic/data/data.asp?lang=0&station=6\",\n",
    "              \"https://web.archive.org/web/20151208132246if_/http://www.aari.nw.ru/projects/Antarctic/data/data.asp?lang=0&station=7\",\n",
    "              \"https://web.archive.org/web/20090629053933if_/http://www.aari.nw.ru/projects/Antarctic/data/data.asp?lang=0&station=1\",\n",
    "              \"https://web.archive.org/web/20230609161030if_/http://www.aari.aq/data/data.php?lang=0&station=0\",\n",
    "             ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1c135d5d-17f5-4861-adf7-5bc1b7977111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.69s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for link in tqdm(html_links):\n",
    "    # Fetch HTML content\n",
    "    response = requests.get(link)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Define file name from link\n",
    "    file_name = soup.select('body blockquote table tr td h1')[0].text\n",
    "    file_path = os.path.join(root_dir, file_name)\n",
    "\n",
    "    # Create directory for station if not exists\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "        \n",
    "    # Find all table names\n",
    "    \n",
    "    tables_name = soup.select('body blockquote table[border=\"0\"][cellpadding=\"0\"][cellspacing=\"0\"] tr td h2')\n",
    "    \n",
    "    if len(tables_name)==0:\n",
    "        tables_name=soup.select('body blockquote h2 b')\n",
    "\n",
    "    names = [name.text for name in tables_name] \n",
    "    # Find all tables starting with 'Data table'\n",
    "    tables = soup.select('body blockquote b font[color=\"green\"]')\n",
    "\n",
    "    # Loop over tables, find 'Data table', and extract the table\n",
    "    if int(len(tables)/3)==len(names):\n",
    "        for index, table in enumerate(tables):\n",
    "            if table.text.strip() in [\"Таблица данных\",'Data table']:\n",
    "                index=int(index/3)\n",
    "                # Following table tag after 'Data table' tag\n",
    "                data_table = table.find_next('table')\n",
    "\n",
    "                # Convert html table to pandas DataFrame\n",
    "                df = pd.read_html(str(data_table), flavor='bs4')[0]\n",
    "                first_nan_index = df.isnull().any(axis=1).idxmax()\n",
    "                df = df.loc[:first_nan_index-1]\n",
    "                # Save DataFrame to csv\n",
    "                table_name = names[index].replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "                csv_file_path = os.path.join(file_path, f\"{table_name}.csv\")\n",
    "                df.to_csv(csv_file_path, index=False)\n",
    "    elif int(len(tables))==len(names):            \n",
    "        for index, table in enumerate(tables):\n",
    "            data_table = table.find_next('table').find_next('table')\n",
    "            # Convert html table to pandas DataFrame\n",
    "            df = pd.read_html(str(data_table), flavor='bs4')[0]\n",
    "            first_nan_index = df.isnull().any(axis=1).idxmax()\n",
    "            df = df.loc[:first_nan_index-1]\n",
    "            # Save DataFrame to csv\n",
    "            table_name = names[index].replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "            csv_file_path = os.path.join(file_path, f\"{table_name}.csv\")\n",
    "            df.to_csv(csv_file_path, index=False)\n",
    "                \n",
    "    else:\n",
    "        raise NameError('unmatch')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e501ba-5e63-4dd9-99c9-c9a127a9b25f",
   "metadata": {},
   "source": [
    "# This a a script to translate all csv from RU to EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "15c0b824-55cb-413b-830f-1d61d365fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "import os\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "root_dir = \"D:/Antarctica/Станция Беллинсгаузен (89050)/\"\n",
    "for root, dirs, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            # Translate the file name (without extension)\n",
    "            file_name, file_extension = os.path.splitext(file)\n",
    "            translated = GoogleTranslator(source='ru', target='en').translate(file_name) \n",
    "            \n",
    "            # Rename the file\n",
    "            new_name = translated + file_extension\n",
    "            os.rename(os.path.join(root, file), os.path.join(root, new_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3b282321-98bd-4b24-9997-564cc62edebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Albedo_of_underlying_surface_(%)'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792db281-129d-46c6-a645-a116d6abdc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed8e5c-62be-40f5-8390-3398ad8484b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff364cee-1575-4bbc-89b2-17c0d3e43f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
