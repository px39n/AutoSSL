{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4533ee81-d579-4867-b15b-aa7f3f8f385f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiment Introduction\n",
    "\n",
    "Background, \n",
    "In order to do ablation experiment, we have diffculty like\n",
    "1. most of them are same, we have to repeat many times.\n",
    "2. too many model, config, code hard to manage and save. too messy\n",
    "\n",
    "\n",
    "For all kind of SSL training workflow, we have to define the hyperparameters includes 4 aspect,\n",
    "Dataset\n",
    "Model\n",
    "Training\n",
    "Saving COnfig\n",
    "\n",
    "\n",
    "\n",
    "How to Use this Experiment?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff5e60f-6c7d-47b6-8b30-5ad2e6c94b09",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71222088-d62f-40fc-b935-f9c8ee99a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import config\n",
    "import sys \n",
    "sys.path.append(\"C:/Users/isxzl/OneDrive/Code/AutoSSL\")\n",
    "import torch\n",
    "import yaml\n",
    "from torchvision.transforms import RandomRotation,GaussianBlur,ColorJitter\n",
    "from autoSSL.evaluate import eval_KNN,eval_linear,eval_KNNplot,pipe_collate\n",
    "from autoSSL.models import BarlowTwins, BYOL, MoCo, SimCLR, SimSiam, VICReg ,pipe_model \n",
    "from autoSSL.utils import embedding_feature,ck_callback,dict2transformer,join_dir,ContinuousCSVLogger  \n",
    "from autoSSL.data import PipeDataset\n",
    "from autoSSL.train import Trainer\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eba85e-c2d4-4ed3-a276-2b03ff82f764",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Global Baseline Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb0e5479-43a3-4b63-b9e2-b8ce70103290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML file\n",
    "with open('global.yaml', 'r') as file:\n",
    "    global_config = yaml.safe_load(file)\n",
    "\n",
    "# Write your experiment notebook name here\n",
    "global_config[\"experiment\"]=\"backbone VS model\"     \n",
    "    \n",
    "# Define global view function\n",
    "global_SSL_augmentation = {\n",
    "    'RandomResizedCrop': {'size': (global_config[\"input_size\"], global_config[\"input_size\"])},\n",
    "    'RandomApply':{'transforms':[RandomRotation(degrees=90)], 'p':0.8},\n",
    "    'RandomHorizontalFlip': {'p': 0.5},\n",
    "    'RandomVerticalFlip':  {'p':0.5},\n",
    "    'RandomApply':{'transforms': [ColorJitter(brightness=0.04,contrast=0.04,saturation=0.02,hue=0.01)], 'p':0.8},\n",
    "    'RandomGrayscale' :{'p':0.2},\n",
    "    'RandomSolarize':{'threshold':128, 'p':0.1},\n",
    "    'RandomApply':{'transforms':[GaussianBlur(kernel_size=3,sigma=(0.2, 2))],'p':0.8},\n",
    "    'ToTensor': {},\n",
    "    'Normalize': {\"mean\": [0.485, 0.456, 0.406], \"std\": [0.229, 0.224, 0.225]}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f84c3-1365-48e8-8ac9-6fdeb5c7a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a869f3e-3e94-4d05-bec5-6fdd31695b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4e6970-4604-485e-bd37-60383de96b81",
   "metadata": {
    "tags": []
   },
   "source": [
    "## vicreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51c7c16f-0305-4293-a8e6-2920c08a6954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU RAM Free: 16283MB | Used: 7872MB | Util  32% | Total 24564MB\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    943 MiB |  13048 MiB |  60598 GiB |  60597 GiB |\n",
      "|       from large pool |    768 MiB |  12913 MiB |  60293 GiB |  60292 GiB |\n",
      "|       from small pool |    174 MiB |    197 MiB |    305 GiB |    305 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    943 MiB |  13048 MiB |  60598 GiB |  60597 GiB |\n",
      "|       from large pool |    768 MiB |  12913 MiB |  60293 GiB |  60292 GiB |\n",
      "|       from small pool |    174 MiB |    197 MiB |    305 GiB |    305 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |    926 MiB |  12980 MiB |  60384 GiB |  60383 GiB |\n",
      "|       from large pool |    752 MiB |  12846 MiB |  60079 GiB |  60078 GiB |\n",
      "|       from small pool |    173 MiB |    196 MiB |    305 GiB |    304 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   3740 MiB |  13484 MiB |  30700 MiB |  26960 MiB |\n",
      "|       from large pool |   3542 MiB |  13298 MiB |  30138 MiB |  26596 MiB |\n",
      "|       from small pool |    198 MiB |    198 MiB |    562 MiB |    364 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 150525 KiB |   3259 MiB |  30217 GiB |  30217 GiB |\n",
      "|       from large pool | 149054 KiB |   3196 MiB |  29901 GiB |  29901 GiB |\n",
      "|       from small pool |   1471 KiB |     93 MiB |    316 GiB |    316 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    4341    |    4524    |    5589 K  |    5585 K  |\n",
      "|       from large pool |     203    |     872    |    2716 K  |    2716 K  |\n",
      "|       from small pool |    4138    |    4210    |    2873 K  |    2868 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    4341    |    4524    |    5589 K  |    5585 K  |\n",
      "|       from large pool |     203    |     872    |    2716 K  |    2716 K  |\n",
      "|       from small pool |    4138    |    4210    |    2873 K  |    2868 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     163    |     258    |     675    |     512    |\n",
      "|       from large pool |      64    |     165    |     394    |     330    |\n",
      "|       from small pool |      99    |      99    |     281    |     182    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      11    |     310    |    2550 K  |    2550 K  |\n",
      "|       from large pool |       6    |     105    |    1130 K  |    1130 K  |\n",
      "|       from small pool |       5    |     278    |    1419 K  |    1419 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import GPUtil\n",
    "GPUs = GPUtil.getGPUs()\n",
    "gpu = GPUs[0]\n",
    "import torch\n",
    "print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "print(torch.cuda.memory_summary ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793fbb9-671d-4286-a3bc-9fcf592bd8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb1352c0-9e27-470a-919d-af85ba9365b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at experiment_checkpoints/backbone VS model\\VICReg_backbone_resnet18\\checkpoints-epoch=07-train_loss=20.23.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                      | Params\n",
      "--------------------------------------------------------------\n",
      "0 | backbone        | Sequential                | 11.2 M\n",
      "1 | projection_head | BarlowTwinsProjectionHead | 9.4 M \n",
      "2 | criterion       | VICRegLoss                | 0     \n",
      "--------------------------------------------------------------\n",
      "20.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "20.6 M    Total params\n",
      "82.496    Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint at experiment_checkpoints/backbone VS model\\VICReg_backbone_resnet18\\checkpoints-epoch=07-train_loss=20.23.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous max epoch: 8. Continue to train for extra 3 epochs.\n",
      "Loading checkpoint from: experiment_checkpoints/backbone VS model\\VICReg_backbone_resnet18\\checkpoints-epoch=07-train_loss=20.23.ckpt\n",
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.69it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.68it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:612: UserWarning: Checkpoint directory C:\\Users\\isxzl\\OneDrive\\Code\\AutoSSL\\experiment\\experiment_checkpoints\\backbone VS model\\VICReg_backbone_resnet18_pretrained exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Restoring states from the checkpoint path at experiment_checkpoints/backbone VS model\\VICReg_backbone_resnet18_pretrained\\checkpoints-epoch=04-train_loss=19.83.ckpt\n",
      "C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:337: UserWarning: The dirpath has changed from 'C:\\\\Users\\\\isxzl\\\\OneDrive\\\\Code\\\\VICReg-BarlowTwins-Ablation-Study\\\\experiment\\\\experiment_checkpoints\\\\backbone VS model\\\\VICReg_backbone_resnet18_pretrained' to 'C:\\\\Users\\\\isxzl\\\\OneDrive\\\\Code\\\\AutoSSL\\\\experiment\\\\experiment_checkpoints\\\\backbone VS model\\\\VICReg_backbone_resnet18_pretrained', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "  warnings.warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                      | Params\n",
      "--------------------------------------------------------------\n",
      "0 | backbone        | Sequential                | 11.2 M\n",
      "1 | projection_head | BarlowTwinsProjectionHead | 9.4 M \n",
      "2 | criterion       | VICRegLoss                | 0     \n",
      "--------------------------------------------------------------\n",
      "20.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "20.6 M    Total params\n",
      "82.496    Total estimated model params size (MB)\n",
      "C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\lightning_fabric\\loggers\\csv_logs.py:188: UserWarning: Experiment logs directory experiment_checkpoints/backbone VS model\\VICReg_backbone_resnet18_pretrained\\VICReg_backbone_resnet18_pretrained\\version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n",
      "Restored all states from the checkpoint at experiment_checkpoints/backbone VS model\\VICReg_backbone_resnet18_pretrained\\checkpoints-epoch=04-train_loss=19.83.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous max epoch: 5. Continue to train for extra 3 epochs.\n",
      "Loading checkpoint from: experiment_checkpoints/backbone VS model\\VICReg_backbone_resnet18_pretrained\\checkpoints-epoch=04-train_loss=19.83.ckpt\n",
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████| 98/98 [00:33<00:00,  2.96it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████| 98/98 [00:33<00:00,  2.95it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:612: UserWarning: Checkpoint directory C:\\Users\\isxzl\\OneDrive\\Code\\AutoSSL\\experiment\\experiment_checkpoints\\backbone VS model\\VICReg_backbone_resnet50 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Restoring states from the checkpoint path at experiment_checkpoints/backbone VS model\\VICReg_backbone_resnet50\\checkpoints-epoch=02-train_loss=nan-v1.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous max epoch: 5. Continue to train for extra 3 epochs.\n",
      "Loading checkpoint from: experiment_checkpoints/backbone VS model\\VICReg_backbone_resnet50\\checkpoints-epoch=02-train_loss=nan-v1.ckpt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VICReg:\n\tsize mismatch for projection_head.layers.0.weight: copying a param with shape torch.Size([1024, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for projection_head.layers.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for projection_head.layers.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for projection_head.layers.1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for projection_head.layers.1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for projection_head.layers.3.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for projection_head.layers.4.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for projection_head.layers.4.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for projection_head.layers.4.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for projection_head.layers.4.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for projection_head.layers.6.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for projection_head.layers.6.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Use this if you want to START a train\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#trainer=Trainer(config, model_mode=\"start\",precision='16-mixed')\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#trainer.fit(pmodel, pdata.dataloader)  \u001b[39;00m\n\u001b[0;32m     21\u001b[0m trainer\u001b[38;5;241m=\u001b[39mTrainer(config, model_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinue\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m16-mixed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m pdata\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m pmodel\n",
      "File \u001b[1;32mC:\\Users/isxzl/OneDrive/Code/AutoSSL\\autoSSL\\train\\pipe_train.py:62\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, dataloader, ckpt_path)\u001b[0m\n\u001b[0;32m     59\u001b[0m             ckpt_path \u001b[38;5;241m=\u001b[39m file\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:520\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    518\u001b[0m model \u001b[38;5;241m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 520\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:559\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[0;32m    550\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[0;32m    551\u001b[0m )\n\u001b[0;32m    553\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    555\u001b[0m     ckpt_path,\n\u001b[0;32m    556\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    557\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    558\u001b[0m )\n\u001b[1;32m--> 559\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:901\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mrestore_checkpoint_after_setup:\n\u001b[0;32m    900\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: restoring module and callbacks from checkpoint path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkpoint_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore_modules_and_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    903\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: configuring sharded model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    904\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_configure_sharded_model(\u001b[38;5;28mself\u001b[39m)  \u001b[38;5;66;03m# allow user to setup in model sharded environment\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:396\u001b[0m, in \u001b[0;36m_CheckpointConnector._restore_modules_and_callbacks\u001b[1;34m(self, checkpoint_path)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_restore_modules_and_callbacks\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path: Optional[_PATH] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;66;03m# restore modules after setup\u001b[39;00m\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_start(checkpoint_path)\n\u001b[1;32m--> 396\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_datamodule()\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;66;03m# restore callback states\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:278\u001b[0m, in \u001b[0;36m_CheckpointConnector.restore_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    275\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_load_checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_checkpoint)\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m# restore model state_dict\u001b[39;00m\n\u001b[1;32m--> 278\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loaded_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:351\u001b[0m, in \u001b[0;36mStrategy.load_model_state_dict\u001b[1;34m(self, checkpoint)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint: Mapping[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VICReg:\n\tsize mismatch for projection_head.layers.0.weight: copying a param with shape torch.Size([1024, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for projection_head.layers.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for projection_head.layers.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for projection_head.layers.1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for projection_head.layers.1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for projection_head.layers.3.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for projection_head.layers.4.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for projection_head.layers.4.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for projection_head.layers.4.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for projection_head.layers.4.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for projection_head.layers.6.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for projection_head.layers.6.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048])."
     ]
    }
   ],
   "source": [
    "# \"vit_64\",\"mobilenet_v3\", \"mobilenet_v3_pretrained\", \"resnet50\",\"efficientnet_b5\", \"efficientnet_b5_pretrained\"\n",
    "for backbone_name in[\"resnet18\",\"resnet18_pretrained\", \"resnet50\",\"resnet50_pretrained\"]:\n",
    "\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation.copy()\n",
    "    config[\"name\"]=f\"VICReg_backbone_{backbone_name}\"\n",
    "    config[\"backbone\"]=backbone_name\n",
    "    config[\"model\"]=\"VICReg\"\n",
    "    config[\"batch\"]=512\n",
    "    config[\"prjhead_dim\"]=2048\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=dict2transformer(SSL_augmentation,view=config[\"view\"]))\n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    pmodel=pipe_model(config=config)\n",
    "\n",
    "    # Use this if you want to START a train\n",
    "    #trainer=Trainer(config, model_mode=\"start\",precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader)  \n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=3,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    # Use this if you want to CONTINUE to train\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a70d5bb-279d-47b1-8850-dd2288e5e568",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb66c1a-89fa-4ce2-9f35-92ba04726170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5d877c7-b5f9-4dd9-ae01-04f29327fe52",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67682be1-2079-464a-97e5-26c474f5bdfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50686839-6777-4c26-b7f6-02b871ac184a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b5cdb99-4cac-4e1d-8b01-eb338e9fcb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def embed(x, embedding_model, device):\n",
    "    embedding_model.eval()\n",
    "    embedding_model.to(device)\n",
    "    x = x.float().to(device)   # remove the unsqueeze operation\n",
    "    return embedding_model(x).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def eval_linear(pipe_data, models, device='cuda', split=None, test=None):\n",
    "    '''\n",
    "    Parameters:\n",
    "    pipe_data (PipeDataset): The whole data as a PipeDataset object\n",
    "    models (dict/torch.nn.Module): Model dict returned by `pipe_collate` or a single model to create embeddings\n",
    "    device (str): Device to perform computations on. Default is 'cuda' if available.\n",
    "    split (float, optional): The ratio of samples to include in the train split.\n",
    "    test (PipeDataset, optional): The test data as a PipeDataset object\n",
    "    '''\n",
    "\n",
    "    # Use split parameter to divide data into train and test if test is not provided\n",
    "    if split is not None and test is None:\n",
    "        train_data, test_data = pipe_data.split(split)\n",
    "    else:\n",
    "        train_data = pipe_data\n",
    "        test_data = test\n",
    "\n",
    "    # Extract features and labels from train and test data\n",
    "    print(\"Load the training and testing dataset\")\n",
    "    X_train, y_train = train_data.array[0], train_data.array[1]\n",
    "    X_test, y_test = test_data.array[0], test_data.array[1]\n",
    "\n",
    "    if isinstance(models, torch.nn.Module):\n",
    "       models = {'name': ['model_0'], 'model': [models], 'address': None} \n",
    "    if isinstance(models, list):\n",
    "        models = {'name': ['model_'+str(i) for i in range(0,len(models))], 'model': models, 'address': None}\n",
    "   \n",
    "    # Initialize the results list\n",
    "    results = []\n",
    "\n",
    "    # Iterate over models and calculate accuracy\n",
    "    for i, embedding_model in enumerate(tqdm(models['model'])):\n",
    "        # Get the embeddings for train and test data\n",
    "        X_train_embedding = [embed(x, embedding_model, device) for x in DataLoader(X_train, batch_size=128)]\n",
    "        X_train_embedding = np.concatenate(X_train_embedding)\n",
    "\n",
    "        X_test_embedding = [embed(x, embedding_model, device) for x in DataLoader(X_test, batch_size=128)]\n",
    "        X_test_embedding = np.concatenate(X_test_embedding)\n",
    "\n",
    "        if X_test_embedding is None or np.any(np.isnan(X_train_embedding)):\n",
    "            accuracy = 'model_collapse'\n",
    "        else:\n",
    "            # Initialize the SGDClassifier and train it\n",
    "            clf = SGDClassifier()\n",
    "            clf.fit(X_train_embedding, y_train)\n",
    "\n",
    "            # Predict labels for the test data and calculate accuracy\n",
    "            X_test_predicted = clf.predict(X_test_embedding)\n",
    "            accuracy = accuracy_score(y_test, X_test_predicted)\n",
    "\n",
    "        namee=models[\"name\"][i]\n",
    "        # Append the accuracy to the results\n",
    "        results.append((namee, accuracy))\n",
    "        \n",
    "    if models['address'] is not None:\n",
    "        df = pd.read_csv(models['address'])\n",
    "        df['linear_accuracy'] = [result[1] for result in results]\n",
    "        df.to_csv(models['address'], index=False)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8ad6455-b8c3-4d5d-b6eb-5627bd52fbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collating the models' (evaluating) information to experiment_checkpoints/backbone VS model/^VICReg_backbone_._$.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_augmentation = {\n",
    "    'RandomResizedCrop': {'size': (global_config[\"input_size\"], global_config[\"input_size\"])},\n",
    "    'ToTensor': {},\n",
    "    'Normalize': {\"mean\": [0.485, 0.456, 0.406], \"std\": [0.229, 0.224, 0.225]}\n",
    "}\n",
    "collate =pipe_collate(address=\"experiment_checkpoints/backbone VS model/\", reg=\"^VICReg_backbone_.*$\")\n",
    "\n",
    "pdata = PipeDataset(input_dir=global_config[\"path_to_test_cifar10\"],\n",
    "    augmentation=dict2transformer(test_augmentation,view=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca362de1-b68b-45c8-94f6-853afff878aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the training and testing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:04<00:00, 1884.21it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:04<00:00, 1917.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2001/2001 [00:01<00:00, 1503.07it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2001/2001 [00:01<00:00, 1804.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:54<00:00,  6.02s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aaa=eval_linear(pdata, models=collate, device=global_config[\"device\"], split=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f9096-89ab-4926-8c60-cca73920c337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VICReg_backbone_efficientnet_b5', 'model_collapse'),\n",
       " ('VICReg_backbone_efficientnet_b5_pretrained', 0.30684657671164417),\n",
       " ('VICReg_backbone_mobilenet_v3', 'model_collapse'),\n",
       " ('VICReg_backbone_mobilenet_v3_pretrained', 0.43228385807096453),\n",
       " ('VICReg_backbone_resnet18', 0.22938530734632684),\n",
       " ('VICReg_backbone_resnet18_pretrained', 0.32233883058470764),\n",
       " ('VICReg_backbone_resnet50', 'model_collapse'),\n",
       " ('VICReg_backbone_resnet50_pretrained', 0.3363318340829585),\n",
       " ('VICReg_backbone_vit_64', 0.1054472763618191)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " aaa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
