{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4533ee81-d579-4867-b15b-aa7f3f8f385f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiment Introduction\n",
    "\n",
    "Background, \n",
    "In order to do ablation experiment, we have diffculty like\n",
    "1. most of them are same, we have to repeat many times.\n",
    "2. too many model, config, code hard to manage and save. too messy\n",
    "\n",
    "\n",
    "For all kind of SSL training workflow, we have to define the hyperparameters includes 4 aspect,\n",
    "Dataset\n",
    "Model\n",
    "Training\n",
    "Saving COnfig\n",
    "\n",
    "\n",
    "\n",
    "How to Use this Experiment?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff5e60f-6c7d-47b6-8b30-5ad2e6c94b09",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64951e3b-e012-4ed5-8846-03eed103c696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71222088-d62f-40fc-b935-f9c8ee99a579",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T23:38:03.867109300Z",
     "start_time": "2023-06-22T23:38:00.578593500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import config\n",
    "import sys \n",
    "sys.path.append(\"C:/Users/isxzl/OneDrive/Code/AutoSSL\")\n",
    "import yaml\n",
    "from torchvision.transforms import RandomRotation,GaussianBlur,ColorJitter\n",
    "from autoSSL.evaluate import eval_KNN,eval_linear,eval_KNNplot,pipe_collate\n",
    "from autoSSL.models import BarlowTwins, BYOL, MoCo, SimCLR, SimSiam, VICReg ,Toymodel, pipe_model \n",
    "from autoSSL.utils import embedding_feature,ck_callback,dict2transformer,trans2multi,join_dir,ContinuousCSVLogger, load_config\n",
    "from autoSSL.data import PipeDataset\n",
    "from autoSSL.train import Trainer\n",
    "\n",
    "from lightly.transforms import SimCLRTransform\n",
    "from lightly.transforms.rotation import random_rotation_transform\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65338b3-eb1d-4b05-b9e0-b4b3af9719e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06eba85e-c2d4-4ed3-a276-2b03ff82f764",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Global Baseline Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0e5479-43a3-4b63-b9e2-b8ce70103290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T23:38:04.131453400Z",
     "start_time": "2023-06-22T23:38:03.872092600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the YAML file\n",
    "with open('global.yaml', 'r') as file:\n",
    "    global_config = yaml.safe_load(file)\n",
    "\n",
    "# Write your experiment notebook name here\n",
    "global_config[\"experiment\"]=\"views VS model\"   \n",
    "global_config[\"dataset_dir\"]=\"D:/Datasets/cifar10/train/\"\n",
    "global_config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,None,None]]    \n",
    "\n",
    "global_config[\"loss_func\"]=\"BarlowTwinsLoss\"     \n",
    "global_config[\"view_model\"]=\"None\"     \n",
    "global_config[\"view\"]=2 \n",
    "global_config[\"stop_gradient\"]=False   \n",
    "global_config[\"optimizer\"]=\"SGD\"      \n",
    "global_config[\"schedule\"]=\"cos\"   \n",
    "global_config[\"model\"]=\"Toymodel\"\n",
    "global_config[\"batch_size\"]=128\n",
    "global_config[\"input_size\"]=32\n",
    "global_config[\"max_epochs\"]=50\n",
    "\n",
    "#global_config[\"backbone\"]=\"resnet18_pretrained\"\n",
    "\n",
    "# Define global Training Augmentation\n",
    "global_SSL_augmentation=T.Compose([\n",
    "    T.RandomResizedCrop(size=global_config[\"input_size\"], scale=(0.08, 1.0)),\n",
    "    T.RandomApply([T.RandomRotation(degrees=90)], p=0),\n",
    "    T.RandomHorizontalFlip(p= 0.5),\n",
    "    T.RandomVerticalFlip(p=0),\n",
    "    T.RandomApply([T.ColorJitter(\n",
    "            brightness=0.4,contrast=0.4,saturation=0.4, hue=0.1)], p=0.8),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "# Define the Testing Augmentation\n",
    "test_SSL_augmentation=T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "p_knndata= PipeDataset(input_dir=global_config[\"path_to_test_cifar10\"], \n",
    "        augmentation=trans2multi(test_SSL_augmentation,view=1), \n",
    "        batch_size=global_config[\"batch_size\"],num_workers=global_config[\"num_workers\"]).dataloader\n",
    "p_knndata=[p_knndata,10]  # The second number is the classes number of this datasets\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f8a6e-bfc8-4b5b-98b0-4c3797a6f437",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config 1 Baseline for everything (Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa842a-b5ad-4667-b051-2d91f4358bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "st=True\n",
    "\n",
    "\n",
    "for baseline in [[pd,st,True],[[],False,False],[pd,False,True],[[],st,False]]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=2\n",
    "    config[\"view_model\"]= \"None\"\n",
    "    config[\"name\"]=f\"LNeg_VMNone_PR{baseline[2]}_st{baseline[1]}_b128_m50_v{4}\"\n",
    "    config[\"loss_func\"]=\"NegativeCosineSimilarity\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= baseline[1]\n",
    "    config[\"predhead_dim\"]= baseline[0]\n",
    "    config[\"max_epochs\"]= 50\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a869f3e-3e94-4d05-bec5-6fdd31695b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config2 Grid Strategy View 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709ee2f3-8ea5-4e81-a382-b493b7acc894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9dd33db-b0f5-4406-a9d7-42f5d3b0b566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:615: UserWarning: Checkpoint directory C:\\Users\\isxzl\\OneDrive\\Code\\AutoSSL\\experiment\\experiment_checkpoints\\views VS model\\Grid_LNeg_VMn_mean_PRTruenoBN_stTrue_b128_m50_v4 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                     | Params\n",
      "-------------------------------------------------------------\n",
      "0 | backbone        | Sequential               | 11.2 M\n",
      "1 | criterion       | NegativeCosineSimilarity | 0     \n",
      "2 | projection_head | ProjectionHead           | 5.3 M \n",
      "3 | prediction_head | ProjectionHead           | 2.1 M \n",
      "-------------------------------------------------------------\n",
      "18.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "18.5 M    Total params\n",
      "74.111    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  41%|█████████████████████████▍                                    | 160/390 [00:19<00:28,  8.11it/s, v_num=0]"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:/Users/isxzl/OneDrive/Code/AutoSSL/experiment/experiment_checkpoints/views VS model/Grid_LNeg_VMn_mean_PRTruenoBN_stTrue_b128_m50_v4/default/version_0/hparams.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:42\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:570\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    564\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    566\u001b[0m     ckpt_path,\n\u001b[0;32m    567\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    568\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    569\u001b[0m )\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:975\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1018\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1018\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:201\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 201\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:354\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:133\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:239\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# -----------------------------------------\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# SAVE METRICS TO LOGGERS AND PROGRESS_BAR\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# -----------------------------------------\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logger_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_train_step_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:154\u001b[0m, in \u001b[0;36m_LoggerConnector.update_train_step_metrics\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_update_logs \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mfast_dev_run:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:109\u001b[0m, in \u001b[0;36m_LoggerConnector.log_metrics\u001b[1;34m(self, metrics, step)\u001b[0m\n\u001b[0;32m    108\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog_metrics(metrics\u001b[38;5;241m=\u001b[39mscalar_metrics, step\u001b[38;5;241m=\u001b[39mstep)\n\u001b[1;32m--> 109\u001b[0m \u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\lightning_utilities\\core\\rank_zero.py:27\u001b[0m, in \u001b[0;36mrank_zero_only.<locals>.wrapped_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\lightning_fabric\\loggers\\csv_logs.py:141\u001b[0m, in \u001b[0;36mCSVLogger.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m--> 141\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loggers\\csv_logs.py:61\u001b[0m, in \u001b[0;36mExperimentWriter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m hparams_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNAME_HPARAMS_FILE)\n\u001b[1;32m---> 61\u001b[0m \u001b[43msave_hparams_to_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:303\u001b[0m, in \u001b[0;36msave_hparams_to_yaml\u001b[1;34m(config_yaml, hparams, use_omegaconf)\u001b[0m\n\u001b[0;32m    302\u001b[0m hparams \u001b[38;5;241m=\u001b[39m apply_to_collection(hparams, DictConfig, OmegaConf\u001b[38;5;241m.\u001b[39mto_container, resolve\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_yaml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\fsspec\\spec.py:1142\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[1;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[0;32m   1136\u001b[0m     text_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1137\u001b[0m         k: kwargs\u001b[38;5;241m.\u001b[39mpop(k)\n\u001b[0;32m   1138\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnewline\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1139\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[0;32m   1140\u001b[0m     }\n\u001b[0;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\n\u001b[1;32m-> 1142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(\n\u001b[0;32m   1143\u001b[0m             path,\n\u001b[0;32m   1144\u001b[0m             mode,\n\u001b[0;32m   1145\u001b[0m             block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[0;32m   1146\u001b[0m             cache_options\u001b[38;5;241m=\u001b[39mcache_options,\n\u001b[0;32m   1147\u001b[0m             compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1148\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1149\u001b[0m         ),\n\u001b[0;32m   1150\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtext_kwargs,\n\u001b[0;32m   1151\u001b[0m     )\n\u001b[0;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\fsspec\\spec.py:1154\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[1;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[0;32m   1153\u001b[0m ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[1;32m-> 1154\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(\n\u001b[0;32m   1155\u001b[0m     path,\n\u001b[0;32m   1156\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   1157\u001b[0m     block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[0;32m   1158\u001b[0m     autocommit\u001b[38;5;241m=\u001b[39mac,\n\u001b[0;32m   1159\u001b[0m     cache_options\u001b[38;5;241m=\u001b[39mcache_options,\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[0;32m   1162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\fsspec\\implementations\\local.py:183\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[1;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LocalFileOpener(path, mode, fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\fsspec\\implementations\\local.py:287\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[1;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\fsspec\\implementations\\local.py:292\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:/Users/isxzl/OneDrive/Code/AutoSSL/experiment/experiment_checkpoints/views VS model/Grid_LNeg_VMn_mean_PRTruenoBN_stTrue_b128_m50_v4/default/version_0/hparams.yaml'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Use this if you want to START a train\u001b[39;00m\n\u001b[0;32m     30\u001b[0m trainer\u001b[38;5;241m=\u001b[39mTrainer(config, model_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m,check_val_every_n_epoch \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m) \u001b[38;5;66;03m#precision='16-mixed',\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Use this if you want to CONTINUE a train\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(pmodel\u001b[38;5;241m.\u001b[39mdebug)\n",
      "File \u001b[1;32mC:\\Users/isxzl/OneDrive/Code/AutoSSL\\autoSSL\\train\\pipe_train.py:81\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, dataloader, val_dataloaders, ckpt_path)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, current epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader_kNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     85\u001b[0m a, b,c,d,e \u001b[38;5;241m=\u001b[39m get_total_times(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/fit-save.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:531\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    529\u001b[0m model \u001b[38;5;241m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 531\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:65\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mon_exception(exception)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m logger \u001b[38;5;129;01min\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mloggers:\n\u001b[1;32m---> 65\u001b[0m     \u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfailed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_teardown()\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# teardown might access the stage so we reset it after\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\lightning_utilities\\core\\rank_zero.py:27\u001b[0m, in \u001b[0;36mrank_zero_only.<locals>.wrapped_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `rank_zero_only.rank` needs to be set before use\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\lightning_fabric\\loggers\\csv_logs.py:149\u001b[0m, in \u001b[0;36mCSVLogger.finalize\u001b[1;34m(self, status)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;66;03m# When using multiprocessing, finalize() should be a no-op on the main process, as no experiment has been\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;66;03m# initialized there\u001b[39;00m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\lightning_utilities\\core\\rank_zero.py:27\u001b[0m, in \u001b[0;36mrank_zero_only.<locals>.wrapped_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `rank_zero_only.rank` needs to be set before use\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\lightning_fabric\\loggers\\csv_logs.py:141\u001b[0m, in \u001b[0;36mCSVLogger.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@rank_zero_only\u001b[39m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loggers\\csv_logs.py:61\u001b[0m, in \u001b[0;36mExperimentWriter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save recorded hparams and metrics into files.\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m hparams_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNAME_HPARAMS_FILE)\n\u001b[1;32m---> 61\u001b[0m \u001b[43msave_hparams_to_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:303\u001b[0m, in \u001b[0;36msave_hparams_to_yaml\u001b[1;34m(config_yaml, hparams, use_omegaconf)\u001b[0m\n\u001b[0;32m    301\u001b[0m hparams \u001b[38;5;241m=\u001b[39m deepcopy(hparams)\n\u001b[0;32m    302\u001b[0m hparams \u001b[38;5;241m=\u001b[39m apply_to_collection(hparams, DictConfig, OmegaConf\u001b[38;5;241m.\u001b[39mto_container, resolve\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_yaml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m         OmegaConf\u001b[38;5;241m.\u001b[39msave(hparams, fp)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\fsspec\\spec.py:1142\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[1;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[0;32m   1134\u001b[0m     mode \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1136\u001b[0m     text_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1137\u001b[0m         k: kwargs\u001b[38;5;241m.\u001b[39mpop(k)\n\u001b[0;32m   1138\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnewline\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1139\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[0;32m   1140\u001b[0m     }\n\u001b[0;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\n\u001b[1;32m-> 1142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(\n\u001b[0;32m   1143\u001b[0m             path,\n\u001b[0;32m   1144\u001b[0m             mode,\n\u001b[0;32m   1145\u001b[0m             block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[0;32m   1146\u001b[0m             cache_options\u001b[38;5;241m=\u001b[39mcache_options,\n\u001b[0;32m   1147\u001b[0m             compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1148\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1149\u001b[0m         ),\n\u001b[0;32m   1150\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtext_kwargs,\n\u001b[0;32m   1151\u001b[0m     )\n\u001b[0;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1153\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\fsspec\\spec.py:1154\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[1;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1153\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[1;32m-> 1154\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(\n\u001b[0;32m   1155\u001b[0m         path,\n\u001b[0;32m   1156\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   1157\u001b[0m         block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[0;32m   1158\u001b[0m         autocommit\u001b[38;5;241m=\u001b[39mac,\n\u001b[0;32m   1159\u001b[0m         cache_options\u001b[38;5;241m=\u001b[39mcache_options,\n\u001b[0;32m   1160\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1161\u001b[0m     )\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1163\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\fsspec\\implementations\\local.py:183\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[1;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LocalFileOpener(path, mode, fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\fsspec\\implementations\\local.py:287\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[1;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\fsspec\\implementations\\local.py:292\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m--> 292\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[0;32m    294\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:/Users/isxzl/OneDrive/Code/AutoSSL/experiment/experiment_checkpoints/views VS model/Grid_LNeg_VMn_mean_PRTruenoBN_stTrue_b128_m50_v4/default/version_0/hparams.yaml'"
     ]
    }
   ],
   "source": [
    "\n",
    "for baseline0 in [\"n_mean\", \"n_mean_sym\",\"mean_n\"]:    # ,\"std_view\",\"var_view\",  \"mean_n\" ,\"fastsim\", \"pair-pair\" , \"1_n\" ,\"1_fastsim\" 1_mean  #me_me\n",
    "    \n",
    "    \n",
    "    pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    st=True\n",
    "    pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]  \n",
    "    for baseline in [[pd,st,\"TruenoBN\"]]: #,[[],False,False],[pd,False,\"TruewithBN\"],[[],st,False]\n",
    "        # MAKE YOUR OWN CONFIG\n",
    "        config=global_config.copy()\n",
    "        # Fill the config\n",
    "        SSL_augmentation=global_SSL_augmentation\n",
    "        config[\"view\"]=4\n",
    "        config[\"view_model\"]= baseline0\n",
    "        config[\"name\"]=f\"Grid_LNeg_VM{baseline0}_PR{baseline[2]}_st{baseline[1]}_b128_m50_v{4}\"\n",
    "        config[\"loss_func\"]=\"NegativeCosineSimilarity\" \n",
    "        config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "        config[\"stop_gradient\"]= baseline[1]\n",
    "        config[\"predhead_dim\"]= baseline[0]\n",
    "        config[\"max_epochs\"]= 50\n",
    "        global_config[\"batch_size\"]=256\n",
    "        # THIS IS THE CODE TO LOAD DATASET\n",
    "        pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "\n",
    "        # THIS IS THE CODE TO LOAD MODEL\n",
    "        #pmodel=pipe_model(config=config) \n",
    "        pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "\n",
    "\n",
    "        # Use this if you want to START a train\n",
    "        trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "        trainer.fit(pmodel, pdata.dataloader,)  \n",
    "\n",
    "        # Use this if you want to CONTINUE a train\n",
    "        #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "        #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "        print(pmodel.debug)\n",
    "        del pdata\n",
    "        del pmodel\n",
    "        del trainer    \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c3b5a-d63b-4964-8e48-e06bb4d2be8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d4e6970-4604-485e-bd37-60383de96b81",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Config3: view can fix stop or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1352c0-9e27-470a-919d-af85ba9365b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]    \n",
    "st=True\n",
    "for baseline in [[pd2,\"TrueBN\"]]:  #[pd,\"TruenoBN\"],\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=4\n",
    "    config[\"view_model\"]= \"mean_n\"\n",
    "    config[\"name\"]=f\"LNeg_VMmean_n_PR{baseline[1]}_stFalse_b128_m50_v{4}\"\n",
    "    config[\"loss_func\"]=\"NegativeCosineSimilarity\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= False\n",
    "    config[\"predhead_dim\"]= baseline[0]\n",
    "    config[\"max_epochs\"]= 50\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =1) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a70d5bb-279d-47b1-8850-dd2288e5e568",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Config4: Check different view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb66c1a-89fa-4ce2-9f35-92ba04726170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]    \n",
    "st=True\n",
    "for baseline in [6]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=baseline\n",
    "    config[\"view_model\"]= \"mean_n\" #\"1_fastsim\"  \n",
    "    config[\"name\"]=f\"LNeg_VMmean_n_PRTruenoBN_stTrue_b128_m50_v{baseline}\"\n",
    "    config[\"loss_func\"]=\"NegativeCosineSimilarity\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= True\n",
    "    config[\"predhead_dim\"]= [[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    config[\"max_epochs\"]= 50\n",
    "\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d877c7-b5f9-4dd9-ae01-04f29327fe52",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config5: Check how it improve the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c066a-22d5-459d-9e6a-5262202bc83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]    \n",
    "st=True\n",
    "\n",
    "for baseline in [[\"None\",2],[\"mean_n\",4]]: #\"mean_n\",\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=baseline[1]\n",
    "    config[\"view_model\"]= baseline[0]#\"1_fastsim\"  \n",
    "    config[\"name\"]=f\"cube_Model_{'VICREG'}_VM{baseline[0]}_v{baseline[1]}\"\n",
    "    config[\"loss_func\"]=\"VICRegLoss\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= False\n",
    "    #config[\"predhead_dim\"]= [[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    config[\"max_epochs\"]= 25\n",
    "\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b96d25f-650f-4f34-bbb6-506951b05885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67682be1-2079-464a-97e5-26c474f5bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "for baseline in [[\"mean_n\",4],[\"None\",2]]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=baseline[1]\n",
    "    config[\"view_model\"]= baseline[0] #\"1_fastsim\"  \n",
    "    config[\"name\"]=f\"cube_Model_{'BT'}_VM{baseline[0]}_v{baseline[1]}\"\n",
    "    config[\"loss_func\"]=\"BarlowTwinsLoss\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= False\n",
    "    #config[\"predhead_dim\"]= [[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    config[\"max_epochs\"]= 25\n",
    "\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9f088-0da9-46fc-ae90-6bbc2a94ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sota\n",
    "for baseline in [[\"None\",2]]:  #,[\"None\",2][\"mean_n\",4],\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=baseline[1]\n",
    "    config[\"view_model\"]=baseline[0] #\"1_fastsim\"  \n",
    "    config[\"name\"]=f\"cube_Model_{'SimCLR'}_VM{baseline[0]}_v{baseline[1]}\"\n",
    "    config[\"loss_func\"]=\"SimCLR\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= False\n",
    "    #config[\"predhead_dim\"]= [[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    config[\"max_epochs\"]=25\n",
    "\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d19076b-5bed-46ac-8eda-150493dff469",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CONFIG 5: SOTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a9863-19e0-458e-8e24-911a45adb86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sota\n",
    "for baseline in [[\"mean_n\",4]]:  #,[\"None\",2]\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=baseline[1]\n",
    "    config[\"view_model\"]=baseline[0] #\"1_fastsim\"  \n",
    "    config[\"name\"]=f\"SOTA200_Model_{'SimCLR'}_VM{baseline[0]}_v{baseline[1]}\"\n",
    "    config[\"loss_func\"]=\"SimCLR\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= False\n",
    "    #config[\"predhead_dim\"]= [[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    config[\"max_epochs\"]= 200\n",
    "\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c000950-f854-4901-b001-79a712b2fcc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Config 6: Check different batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e13b52-9b87-4072-9103-b8fd93f3de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]    \n",
    "st=True\n",
    "for baseline in [32,64,256,512,1024,2048]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy() \n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=4\n",
    "    config[\"view_model\"]= \"mean_n\" #\"1_fastsim\"  \n",
    "    config[\"name\"]=f\"LNeg_VMmean_n_PRTruenoBN_stTrue_b{baseline}_m50_v{4}\"\n",
    "    config[\"loss_func\"]=\"NegativeCosineSimilarity\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]\n",
    "    config[\"predhead_dim\"]= [[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]\n",
    "    config[\"stop_gradient\"]= True\n",
    "    # Print 5\n",
    "\n",
    "    config[\"max_epochs\"]= 50\n",
    "    config[\"batch_size\"]= baseline\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca85c57f-80ce-4eed-b007-51e7d6823163",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]    \n",
    "st=True\n",
    "for baseline in [32,64,256,512,1024,2048]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=2\n",
    "    config[\"view_model\"]= \"None\" #\"1_fastsim\"  \n",
    "    config[\"name\"]=f\"LNeg_VMNone_PRTruenoBN_stTrue_b{baseline}_m50_v{4}\"\n",
    "    config[\"loss_func\"]=\"NegativeCosineSimilarity\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]\n",
    "    config[\"predhead_dim\"]= [[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]\n",
    "    config[\"stop_gradient\"]= True\n",
    "    # Print 5\n",
    "\n",
    "    config[\"max_epochs\"]= 50\n",
    "    config[\"batch_size\"]= baseline\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2d148b-7455-46e8-a7c8-ba4ae27f54c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Config7: Normalization or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856534f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]    \n",
    "st=True\n",
    "for baseline in [4]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=baseline\n",
    "    config[\"view_model\"]= \"mean_n\" #\"1_fastsim\"  \n",
    "    config[\"name\"]=f\"Test_LNeg_VMmean_n_PRTruenoBN_stTrue_b128_m50_v{baseline}\"\n",
    "    config[\"loss_func\"]=\"NegativeCosineSimilarity\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= True\n",
    "    config[\"predhead_dim\"]= pd   \n",
    "    config[\"max_epochs\"]= 50\n",
    "\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61531c2-98d2-43ca-8e4f-4dbd9ad59817",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Config 8 : Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8371660",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]    \n",
    "st=True\n",
    "for baseline in [[\"LARS\",\"cos\"],[\"LARS\",\"LambdaLR\"]]: #\"SGD\", ,\"Adam\"\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=4\n",
    "    config[\"view_model\"]= \"mean_n\" #\"1_fastsim\"  \n",
    "    config[\"name\"]=f\"ModelSia_VMmean_n_Op{baseline[0]}_sch{baseline[1]}_b128_m50_v4\"\n",
    "    config[\"loss_func\"]=\"NegativeCosineSimilarity\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= True\n",
    "    config[\"optimizer\"]= baseline[0]\n",
    "    config[\"predhead_dim\"]= pd   \n",
    "    config[\"max_epochs\"]= 50\n",
    "    config[\"schedule\"]= baseline[1]\n",
    "     \n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618bcb76-63c1-4a68-9164-a3860f9f281e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Config 9 Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5202b434-f254-42b6-abee-86e8aef0c195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc44178f-1ec2-416a-8c90-e27bfa62c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for baseline0 in [\"test\"]:    # ,\"std_view\",\"var_view\",\"Stop\", \"mean_n\" ,\"fastsim\", \"pair-pair\" , \"1_n\" ,\"1_fastsim\" 1_mean  #me_me\n",
    "    \n",
    "    \n",
    "    pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    st=True\n",
    "    pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]  \n",
    "    for baseline in [[pd,st,\"TruenoBN\"]]: #,[[],False,False],[pd,False,\"TruewithBN\"],[[],st,False]\n",
    "        # MAKE YOUR OWN CONFIG\n",
    "        config=global_config.copy()\n",
    "        # Fill the config\n",
    "        SSL_augmentation=global_SSL_augmentation\n",
    "        config[\"view\"]=4\n",
    "        config[\"view_model\"]= baseline0\n",
    "        config[\"name\"]=f\"LNeg_VM{baseline0}_PR{baseline[2]}_st{baseline[1]}_b128_m50_v{4}\"\n",
    "        config[\"loss_func\"]={\"Var_Fea_2dBV\":1,\"VarBatch_2dFV\":22,\"VarView_2dBF\":22} \n",
    "        config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "        config[\"stop_gradient\"]= baseline[1]\n",
    "        config[\"predhead_dim\"]=[]\n",
    "        config[\"max_epochs\"]= 50\n",
    "        config[\"batch_size\"]= 128\n",
    "        # THIS IS THE CODE TO LOAD DATASET\n",
    "        pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "\n",
    "        # THIS IS THE CODE TO LOAD MODEL\n",
    "        #pmodel=pipe_model(config=config) \n",
    "        pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "\n",
    "\n",
    "        # Use this if you want to START a train\n",
    "        trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "        trainer.fit(pmodel, pdata.dataloader,)  \n",
    "\n",
    "        # Use this if you want to CONTINUE a train\n",
    "        #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "        #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "        print(pmodel.debug)\n",
    "        del pdata\n",
    "        del pmodel\n",
    "        del trainer    \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b516dbe1-09af-4d1b-be45-4b139153b341",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e4a0997-caf7-4991-a1ab-efa81e8822a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:615: UserWarning: Checkpoint directory C:\\Users\\isxzl\\OneDrive\\Code\\AutoSSL\\experiment\\experiment_checkpoints\\views VS model\\Test_1 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type           | Params\n",
      "---------------------------------------------------\n",
      "0 | backbone        | Sequential     | 11.2 M\n",
      "1 | criterion       | VICRegLoss     | 0     \n",
      "2 | projection_head | ProjectionHead | 5.3 M \n",
      "---------------------------------------------------\n",
      "16.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "16.4 M    Total params\n",
      "65.710    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  67%|█████████████████████████████████████████▊                    | 263/390 [00:25<00:12, 10.14it/s, v_num=0]{'stop': 8, 'optim': [50000, 128, 5]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "st=True\n",
    "\n",
    "\n",
    "for baseline in [1]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    \n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=4\n",
    "    config[\"view_model\"]= \"mean_n\"\n",
    "    config[\"name\"]=f\"Test_{baseline}\"\n",
    "    config[\"loss_func\"]=\"VICRegLoss\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= False\n",
    "    config[\"predhead_dim\"]= []\n",
    "    config[\"max_epochs\"]= 5 \n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "691e9bf7-6f0b-4be6-8adf-ecea41ec60fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'global_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m st\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m baseline \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# MAKE YOUR OWN CONFIG\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     config\u001b[38;5;241m=\u001b[39m\u001b[43mglobal_config\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Fill the config\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     SSL_augmentation\u001b[38;5;241m=\u001b[39mglobal_SSL_augmentation\n",
      "\u001b[1;31mNameError\u001b[0m: name 'global_config' is not defined"
     ]
    }
   ],
   "source": [
    "pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "st=True\n",
    "\n",
    "\n",
    "for baseline in [2]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=2\n",
    "    config[\"view_model\"]= \"None\"\n",
    "    config[\"name\"]=f\"Test_{baseline}\"\n",
    "    config[\"loss_func\"]=\"VICRegLoss\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= False\n",
    "    config[\"predhead_dim\"]= []\n",
    "    config[\"max_epochs\"]= 5\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50686839-6777-4c26-b7f6-02b871ac184a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "404d98da-b3a5-4e58-8628-eb1acb25fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from sklearn.preprocessing import normalize\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from autoSSL.models.Backbone import pipe_backbone\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def embed(x, embedding_model, device):\n",
    "    embedding_model.eval()\n",
    "    embedding_model.to(device)\n",
    "    x = x.float().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = embedding_model(x)\n",
    "        pooled_embeddings = torch.nn.functional.adaptive_avg_pool2d(embeddings, (1, 1))\n",
    "\n",
    "    return pooled_embeddings.view(pooled_embeddings.size(0), -1).cpu().numpy()\n",
    "\n",
    "def eval_everything(pipe_data, models, device='cuda', split=None, test=None, baseline=None):\n",
    "    if split is not None:\n",
    "        train_data, test_data = pipe_data.split(split)\n",
    "    elif test is None:\n",
    "        train_data = pipe_data\n",
    "        test_data = pipe_data\n",
    "    else:         \n",
    "        train_data = pipe_data\n",
    "        test_data = test\n",
    "\n",
    "    print(\"Load the training and testing dataset\")\n",
    "    X_train, y_train = train_data.array[0], train_data.array[1]\n",
    "    X_test, y_test = test_data.array[0], test_data.array[1]\n",
    "\n",
    "    if isinstance(models, torch.nn.Module):\n",
    "        models = {'name': ['model_0'], 'model': [models], 'address': None} \n",
    "    elif isinstance(models, list):\n",
    "        models = {'name': ['model_'+str(i) for i in range(len(models))], 'model': models, 'address': None}\n",
    "     \n",
    "    baselines=[]\n",
    "    results = []\n",
    "    baselines_name=[]\n",
    "    if baseline:\n",
    "        for base in baseline:\n",
    "            baseline_backbone, _ = pipe_backbone(backbone=base)\n",
    "            models['model'].append(baseline_backbone)\n",
    "            models['name'].append('baseline_' + base)\n",
    "            baselines_name.append('baseline_' + base)\n",
    "            baselines.append(baseline_backbone)\n",
    "        \n",
    "    writer = pd.ExcelWriter(models['address'].replace('.csv', '_confusion.xlsx'))\n",
    "\n",
    "    for i, embedding_model in enumerate(tqdm(models['model'])):\n",
    "        if embedding_model in baselines:\n",
    "            pass\n",
    "        else:\n",
    "            embedding_model=embedding_model.backbone\n",
    "        X_train_embedding = [embed(x, embedding_model, device) for x in DataLoader(X_train, batch_size=16)]\n",
    "        X_train_embedding = np.concatenate(X_train_embedding)\n",
    "\n",
    "        X_test_embedding = [embed(x, embedding_model, device) for x in DataLoader(X_test, batch_size=16)]\n",
    "        X_test_embedding = np.concatenate(X_test_embedding)\n",
    "\n",
    "        X_train_embedding = normalize(X_train_embedding)  # Normalize train embeddings\n",
    "\n",
    "        X_test_embedding = normalize(X_test_embedding)  # Normalize test embeddings\n",
    "\n",
    "        \n",
    "        if X_test_embedding is None:\n",
    "            accuracy = 'model_collapse'\n",
    "            confusion = None\n",
    "        else:\n",
    "            clf = SGDClassifier(loss='log_loss')\n",
    "\n",
    "            clf.fit(X_train_embedding, y_train)\n",
    "\n",
    "            # Get class probabilities for each sample\n",
    "            class_probs = clf.predict_proba(X_test_embedding)\n",
    "\n",
    "            # Get the top 1 predictions\n",
    "            top1_preds = np.argmax(class_probs, axis=1)\n",
    "            top3_preds = np.argpartition(class_probs, -3, axis=1)[:,-3:]\n",
    "\n",
    "            # Calculate confusion matrix\n",
    "            confusion = confusion_matrix(y_test, top1_preds)\n",
    "            # Normalize confusion matrix by row (i.e by the number of samples in each class)\n",
    "            confusion = confusion.astype('float') / confusion.sum(axis=1)[:, np.newaxis]\n",
    "            # Average accuracy is the mean of the diagonal elements (the correctly classified instances)\n",
    "            top1_average_accuracy = np.mean(np.diag(confusion))\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            top1_accuracy = accuracy_score(y_test, top1_preds)\n",
    "            top3_accuracy = np.mean([1 if y in top3 else 0 for y, top3 in zip(y_test, top3_preds)])\n",
    "                            \n",
    "            # K-Nearest Neighbors classifier\n",
    "            knn = KNeighborsClassifier(n_neighbors=20)\n",
    "            knn.fit(X_train_embedding, y_train)\n",
    "            knn_preds = knn.predict(X_test_embedding)\n",
    "            knn_accuracy = accuracy_score(y_test, knn_preds)\n",
    "\n",
    "            accuracy = {\n",
    "                \"Top-1 Accuracy\": top1_accuracy,\n",
    "                \"Top-3 Accuracy\": top3_accuracy,\n",
    "                \"Top-1 Average Accuracy\": top1_average_accuracy,\n",
    "                \"KNN Top-1 Accuracy\": knn_accuracy  # KNN accuracy\n",
    "            }\n",
    "\n",
    "        namee = models[\"name\"][i]\n",
    "        results.append((namee, accuracy))\n",
    "\n",
    "        \n",
    "        del embedding_model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Save confusion matrix to Excel file\n",
    "        if confusion is not None:\n",
    "            df_confusion = pd.DataFrame(confusion)\n",
    "            df_confusion.to_excel(writer, sheet_name=namee)\n",
    "\n",
    "    writer.save()\n",
    "\n",
    "    if models['address'] is not None:\n",
    "        df = pd.read_csv(models['address'])\n",
    "\n",
    "        # If baselines are present, add new rows in the dataframe for them\n",
    "        if baselines:\n",
    "            for base, base_name in zip(baselines, baselines_name):\n",
    "                # Initialize a new row with default values\n",
    "                new_row = {col: None for col in df.columns}\n",
    "                # Update the values we know\n",
    "                new_row.update({\n",
    "                    'dir_name': base_name,\n",
    "                })\n",
    "                # Append the new row to the dataframe\n",
    "                df = df.append(new_row, ignore_index=True)\n",
    "\n",
    "        # Assuming the results are in the same order as the models in the dataframe\n",
    "        df['linear_top1_accuracy'] = [result[1][\"Top-1 Accuracy\"] for result in results]\n",
    "        df['linear_top3_accuracy'] = [result[1][\"Top-3 Accuracy\"] for result in results]\n",
    "        df['linear_top1_average_accuracy'] = [result[1][\"Top-1 Average Accuracy\"] for result in results]\n",
    "        df['linear_knn_top1_accuracy'] = [result[1][\"KNN Top-1 Accuracy\"] for result in results]\n",
    "\n",
    "        df.to_csv(models['address'], index=False)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ad6455-b8c3-4d5d-b6eb-5627bd52fbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collating the models' (evaluating) information to experiment_checkpoints/views VS model/LNeg_VMNone_PRTruenoBN_stTrue_b.__m50_v4.csv\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append(\"C:/Users/isxzl/OneDrive/Code/AutoSSL\")\n",
    "from autoSSL.data import PipeDataset \n",
    "from autoSSL.utils import  dict2transformer,trans2multi  \n",
    "import torchvision.transforms as T\n",
    "from autoSSL.evaluate import eval_KNN,eval_linear,eval_KNNplot,pipe_collate#,eval_everything\n",
    "import yaml\n",
    "# Load the YAML file\n",
    "with open('global.yaml', 'r') as file:\n",
    "    global_config = yaml.safe_load(file)\n",
    "\n",
    "\n",
    "test_augmentation=T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# \"LNeg_VM.*_PRTruewithBN_stTrue_b128_m50_v4\"\n",
    "\n",
    "collate =pipe_collate(address=\"experiment_checkpoints/views VS model/\", reg=\"LNeg_VMNone_PRTruenoBN_stTrue_b.*_m50_v4\")\n",
    "\n",
    "pdata = PipeDataset(input_dir=global_config[\"path_to_test_cifar10\"],\n",
    "    augmentation=trans2multi(test_augmentation,view=1))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a555af-de04-4858-a6e4-88fc67cbfd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca362de1-b68b-45c8-94f6-853afff878aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the training and testing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10001/10001 [00:03<00:00, 2551.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 10001/10001 [00:04<00:00, 2460.79it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 10001/10001 [00:03<00:00, 2610.48it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 10001/10001 [00:03<00:00, 2656.13it/s]\n",
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      " 17%|██████████████                                                                      | 1/6 [00:07<00:35,  7.16s/it]C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      " 33%|████████████████████████████                                                        | 2/6 [00:12<00:23,  5.83s/it]C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      " 50%|██████████████████████████████████████████                                          | 3/6 [00:16<00:15,  5.27s/it]C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      " 67%|████████████████████████████████████████████████████████                            | 4/6 [00:21<00:10,  5.04s/it]C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      " 83%|██████████████████████████████████████████████████████████████████████              | 5/6 [00:26<00:04,  4.92s/it]C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:30<00:00,  5.12s/it]\n",
      "C:\\Users\\isxzl\\AppData\\Local\\Temp\\ipykernel_15008\\357591954.py:127: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
      "  writer.save()\n"
     ]
    }
   ],
   "source": [
    "aaa=eval_everything(pdata, models=collate, device=global_config[\"device\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff8f9096-89ab-4926-8c60-cca73920c337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LNeg_VM1_n_PRTruenoBN_stTrue_b128_m50_v4',\n",
       "  {'Top-1 Accuracy': 0.43465653434656537,\n",
       "   'Top-3 Accuracy': 0.8000199980002,\n",
       "   'Top-1 Average Accuracy': 0.4346606393606393,\n",
       "   'KNN Top-1 Accuracy': 0.48235176482351766}),\n",
       " ('LNeg_VMfastsim_PRTruenoBN_stTrue_b128_m50_v4',\n",
       "  {'Top-1 Accuracy': 0.6818318168183182,\n",
       "   'Top-3 Accuracy': 0.9202079792020798,\n",
       "   'Top-1 Average Accuracy': 0.681833966033966,\n",
       "   'KNN Top-1 Accuracy': 0.698030196980302}),\n",
       " ('LNeg_VMmean_n_PRTruenoBN_stTrue_b128_m50_v4',\n",
       "  {'Top-1 Accuracy': 0.6973302669733027,\n",
       "   'Top-3 Accuracy': 0.9234076592340766,\n",
       "   'Top-1 Average Accuracy': 0.6973241758241759,\n",
       "   'KNN Top-1 Accuracy': 0.7037296270372962}),\n",
       " ('LNeg_VMpair-pair_PRTruenoBN_stTrue_b128_m50_v4',\n",
       "  {'Top-1 Accuracy': 0.6356364363563644,\n",
       "   'Top-3 Accuracy': 0.904009599040096,\n",
       "   'Top-1 Average Accuracy': 0.6356408591408592,\n",
       "   'KNN Top-1 Accuracy': 0.6566343365663434}),\n",
       " ('LNeg_VMstd_view_PRTruenoBN_stTrue_b128_m50_v4',\n",
       "  {'Top-1 Accuracy': 0.6854314568543146,\n",
       "   'Top-3 Accuracy': 0.9175082491750824,\n",
       "   'Top-1 Average Accuracy': 0.6854292707292708,\n",
       "   'KNN Top-1 Accuracy': 0.6931306869313069}),\n",
       " ('LNeg_VMvar_view_PRTruenoBN_stTrue_b128_m50_v4',\n",
       "  {'Top-1 Accuracy': 0.6767323267673233,\n",
       "   'Top-3 Accuracy': 0.9186081391860814,\n",
       "   'Top-1 Average Accuracy': 0.6767269730269729,\n",
       "   'KNN Top-1 Accuracy': 0.6875312468753124})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed89fa6-f9b9-4af6-8531-da9c2729d020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a19fd-fa88-4c31-a829-e689313f04d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a881acc-f9b8-4f25-a5e9-1ab011d77b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01778bba-2d6b-4cf2-b94c-356887aac840",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
