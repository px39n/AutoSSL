{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4533ee81-d579-4867-b15b-aa7f3f8f385f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiment Introduction\n",
    "\n",
    "Background, \n",
    "In order to do ablation experiment, we have diffculty like\n",
    "1. most of them are same, we have to repeat many times.\n",
    "2. too many model, config, code hard to manage and save. too messy\n",
    "\n",
    "\n",
    "For all kind of SSL training workflow, we have to define the hyperparameters includes 4 aspect,\n",
    "Dataset\n",
    "Model\n",
    "Training\n",
    "Saving COnfig\n",
    "\n",
    "\n",
    "\n",
    "How to Use this Experiment?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff5e60f-6c7d-47b6-8b30-5ad2e6c94b09",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71222088-d62f-40fc-b935-f9c8ee99a579",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T23:38:03.867109300Z",
     "start_time": "2023-06-22T23:38:00.578593500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import config\n",
    "import sys \n",
    "sys.path.append(\"C:/Users/isxzl/OneDrive/Code/AutoSSL\")\n",
    "import yaml\n",
    "from torchvision.transforms import RandomRotation,GaussianBlur,ColorJitter\n",
    "from autoSSL.evaluate import eval_KNN,eval_linear,eval_KNNplot,pipe_collate\n",
    "from autoSSL.models import Toymodel, pipe_model \n",
    "from autoSSL.utils import embedding_feature,ck_callback,dict2transformer,trans2multi,join_dir,ContinuousCSVLogger, load_config\n",
    "from autoSSL.data import PipeDataset\n",
    "from autoSSL.train import Trainer\n",
    "\n",
    "from lightly.transforms import SimCLRTransform\n",
    "from lightly.transforms.rotation import random_rotation_transform\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65338b3-eb1d-4b05-b9e0-b4b3af9719e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.validate(pmodel, dataloaders=p_test)    \n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#input_tensor = torch.rand((32, 3, 64, 64))\n",
    "#pmodel.forward(input_tensor)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eba85e-c2d4-4ed3-a276-2b03ff82f764",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Global Baseline Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0e5479-43a3-4b63-b9e2-b8ce70103290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T23:38:04.131453400Z",
     "start_time": "2023-06-22T23:38:03.872092600Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the YAML file\n",
    "with open('global.yaml', 'r') as file:\n",
    "    global_config = yaml.safe_load(file)\n",
    "\n",
    "# Write your experiment notebook name here\n",
    "global_config[\"experiment\"]=\"Magic_cube\"   \n",
    "global_config[\"dataset_dir\"]=\"D:/Datasets/cifar10/train/\"\n",
    "global_config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,None,None]]    \n",
    "\n",
    "global_config[\"loss_func\"]=\"BarlowTwinsLoss\"     \n",
    "global_config[\"view_model\"]=\"None\"     \n",
    "global_config[\"view\"]=2 \n",
    "global_config[\"stop_gradient\"]=False   \n",
    "global_config[\"optimizer\"]=\"SGD\"      \n",
    "global_config[\"schedule\"]=\"cos\"   \n",
    "global_config[\"model\"]=\"Toymodel\"\n",
    "global_config[\"batch_size\"]=256\n",
    "global_config[\"input_size\"]=32\n",
    "global_config[\"max_epochs\"]=50\n",
    " \n",
    "#global_config[\"backbone\"]=\"resnet18_pretrained\"\n",
    "global_SSL_augmentation=T.Compose([\n",
    "    T.RandomResizedCrop(size=global_config[\"input_size\"], scale=(0.08, 1.0)),\n",
    "    T.RandomApply([T.RandomRotation(degrees=90)], p=0),\n",
    "    T.RandomHorizontalFlip(p= 0.5),\n",
    "    T.RandomVerticalFlip(p=0),\n",
    "    T.RandomApply([T.ColorJitter(\n",
    "            brightness=0.4,contrast=0.4,saturation=0.4, hue=0.1)], p=0.8),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Define global Training Augmentation\n",
    "\n",
    "\n",
    "\n",
    "# Define the Testing Augmentation\n",
    "test_SSL_augmentation=T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "p_knndata= PipeDataset(input_dir=global_config[\"path_to_train_cifar10\"], \n",
    "        augmentation=trans2multi(test_SSL_augmentation,view=1), \n",
    "        batch_size=global_config[\"batch_size\"],num_workers=global_config[\"num_workers\"]).dataloader\n",
    "\n",
    "p_test= PipeDataset(input_dir=global_config[\"path_to_test_cifar10\"], \n",
    "        augmentation=trans2multi(test_SSL_augmentation,view=1), \n",
    "        batch_size=512,num_workers=global_config[\"num_workers\"]).dataloader \n",
    "\n",
    "\n",
    "p_knndata=[p_knndata,10,p_test]  # The second number is the classes number of this datasets\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f8a6e-bfc8-4b5b-98b0-4c3797a6f437",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config1: Monitor all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa842a-b5ad-4667-b051-2d91f4358bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for baseline in  [  \"SimCLR\" ,\"BYOL\",\"BarlowTwins\", \"SimSiam\"]: # #[\"SimSiam\",\"BarlowTwins\",\"VICReg\",\"DCL\",\"MoCo\", \"BYOL\", \"SimCLR\" ]\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy() \n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config=load_config(config,baseline)\n",
    "    config[\"view\"]=2\n",
    "    config[\"view_model\"]= \"None\"\n",
    "    config[\"name\"]=f\"Base_md{baseline}_VM{config['view_model']}_ep2_v{config['view']}\"\n",
    "    config[\"batch_size\"]=256\n",
    "    config[\"max_epochs\"]= 50\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f52ba-c306-4175-8a3d-76592412e4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for baseline in  [\"VICReg\",\"MoCo\", \"BYOL\", \"SimCLR\",\"DCL\" ]: # #[\"SimSiam\",\"BarlowTwins\",\"VICReg\",\"DCL\",\"MoCo\", \"BYOL\", \"SimCLR\" ]\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy() \n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config=load_config(config,baseline)\n",
    "    config[\"view\"]=4\n",
    "    config[\"view_model\"]= \"mean_n\"\n",
    "    config[\"name\"]=f\"Base_md{baseline}_VM{config['view_model']}_ep200_v{config['view']}\"\n",
    "    config[\"batch_size\"]=256\n",
    "    config[\"max_epochs\"]= 200\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6acbec-1986-41c5-bf7e-3d65b81d11ff",
   "metadata": {},
   "source": [
    "### Continue to train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9eab9e-33fd-46c8-b5ac-dcc4e09efb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for baseline in  [ \"SimSiam\",\"VICReg\",\"DCL\",\"MoCo\", \"BYOL\", \"SimCLR\"]: # #[\"SimSiam\",\"BarlowTwins\",\"VICReg\",\"DCL\",\"MoCo\", \"BYOL\", \"SimCLR\" ]\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy() \n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config=load_config(config,baseline)\n",
    "    config[\"view\"]=2\n",
    "    config[\"view_model\"]= \"None\"\n",
    "    config[\"name\"]=f\"Base_md{baseline}_VM{config['view_model']}_ep50_v{config['view']}\"\n",
    "    config[\"batch_size\"]=256\n",
    "    config[\"max_epochs\"]= 100\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    #trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    trainer=Trainer(config, model_mode=\"continue\", extra_epoch=50,check_val_every_n_epoch =6)\n",
    "    trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4691ac00-51e5-4c29-a50f-ca77689070d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config 1.2 Continue train on BYOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b3371-97e4-4954-be22-a71461a1d4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for baseline in  [ \"BYOL\"]: # #[\"SimSiam\",\"BarlowTwins\",\"VICReg\",\"DCL\",\"MoCo\", \"BYOL\", \"SimCLR\" ]\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy() \n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config=load_config(config,baseline)\n",
    "    config[\"view\"]=4\n",
    "    config[\"view_model\"]= \"None\"\n",
    "    config[\"name\"]=f\"Con_md{baseline}_VMNone_ep50_v{config['view']}_Default\"\n",
    "    config[\"batch_size\"]=256\n",
    "    config[\"max_epochs\"]= 100\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    #trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    #trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    trainer=Trainer(config, model_mode=\"continue\", extra_epoch=50,check_val_every_n_epoch =4)\n",
    "    trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bbff93-c5a5-4b9e-877c-08f6142347ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f8dcd-3682-4d9f-9d9e-21607b93a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for baseline in  [ \"BYOL\"]: # #[\"SimSiam\",\"BarlowTwins\",\"VICReg\",\"DCL\",\"MoCo\", \"BYOL\", \"SimCLR\" ]\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy() \n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config=load_config(config,baseline)\n",
    "    config[\"view\"]=4\n",
    "    config[\"view_model\"]= \"mean_n\"\n",
    "    config[\"name\"]=f\"Con_md{baseline}_VMNone_ep50_v{config['view']}_Our\"\n",
    "    config[\"batch_size\"]=256\n",
    "    config[\"max_epochs\"]=100\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    #trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    #trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    trainer=Trainer(config, model_mode=\"continue\", extra_epoch=50,check_val_every_n_epoch =4)\n",
    "    trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5213ac-c21f-4a34-aae7-bffb7853151f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config 1.3 Continue train on 800 BYOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e57726-7d2b-4f2e-a9c4-b191d1eda3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28f391d-6214-4acb-95b1-3012fa177c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "del pdata\n",
    "del pmodel\n",
    "del trainer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c38865-bf06-4bf8-86d8-671c74e4d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightly.transforms.multi_view_transform import MultiViewTransform\n",
    "from lightly.transforms import SimCLRViewTransform ,SimSiamViewTransform\n",
    "sv = SimCLRViewTransform(\n",
    "    input_size=32,\n",
    "    cj_strength=0.5,\n",
    "    gaussian_blur=0.0,\n",
    ")\n",
    "st = SimSiamViewTransform(\n",
    "    input_size=32,\n",
    "    gaussian_blur=0.2,\n",
    ")\n",
    "from lightly.transforms.gaussian_blur import GaussianBlur\n",
    "Ens4=T.Compose([\n",
    "    GaussianBlur(prob=0.5),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# learn about Blur\n",
    "\n",
    "Ens1=T.Compose([\n",
    "    T.RandomResizedCrop(size=16, scale=(0.08, 1.0)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    \n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    \n",
    "]) \n",
    "# learn about Local\n",
    "Ens2=T.Compose([\n",
    "    T.RandomApply([T.RandomRotation(degrees=90)], p=0.5),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# learn about Rotation\n",
    "\n",
    "Ens3=T.Compose([\n",
    "    T.RandomResizedCrop(size=16, scale=(0.08, 1.0)),\n",
    "    T.RandomApply([T.RandomRotation(degrees=90)], p=0.5),\n",
    "    T.RandomHorizontalFlip(p= 0.5),\n",
    "    T.RandomVerticalFlip(p=0.5),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# learn about Rotation\n",
    "\n",
    "\n",
    "\n",
    "Aug1=T.Compose([\n",
    "    T.RandomResizedCrop(size=global_config[\"input_size\"], scale=(0.08, 1.0)),\n",
    "    T.RandomApply([T.RandomRotation(degrees=90)], p=0),\n",
    "    T.RandomHorizontalFlip(p= 0.5),\n",
    "    T.RandomVerticalFlip(p=0),\n",
    "    T.RandomApply([T.ColorJitter(\n",
    "            brightness=0.4,contrast=0.4,saturation=0.4, hue=0.1)], p=0.8),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "De1=T.Compose([\n",
    "    T.RandomResizedCrop(size=global_config[\"input_size\"], scale=(0.08, 1.0)),\n",
    "\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "De2=T.Compose([\n",
    "\n",
    "    T.RandomHorizontalFlip(p= 0.5),\n",
    "\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "De3=T.Compose([\n",
    "\n",
    "    T.RandomApply([T.ColorJitter(\n",
    "            brightness=0.4,contrast=0.4,saturation=0.4, hue=0.1)], p=0.8),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "De4=T.Compose([\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "De5=T.Compose([\n",
    "    GaussianBlur(prob=0.5),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "#Ens1=trans2multi(Ens1,view=3)\n",
    "#Ens2=trans2multi(Ens2,view=3)\n",
    "#Ens3=trans2multi(Ens3,view=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ba04f5-deeb-49ed-9917-1a4cec87feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from lightly.models import ResNetGenerator, modules, utils\n",
    "import torch.nn as nn\n",
    "from lightly.models.modules import heads\n",
    "import copy\n",
    "from lightly.loss import NegativeCosineSimilarity\n",
    "baseline =\"BYOL\"\n",
    "# MAKE YOUR OWN CONFIG\n",
    "config=global_config.copy() \n",
    "SSL_augmentation=MultiViewTransform([De1,De2,De3,De4,De5])\n",
    "config=load_config(config,baseline)\n",
    "config[\"view\"]=4\n",
    "config[\"view_model\"]= \"mean_n\"\n",
    "config[\"name\"]=f\"Epoch800_BYOL_Mean\"\n",
    "config[\"batch_size\"]=512\n",
    "config[\"max_epochs\"]= 100\n",
    "config[\"backbone\"]=\"resnet18_5layer\"\n",
    "config[\"learn_rate\"]=6e-2 * 256 / 128/30\n",
    "# THIS IS THE CODE TO LOAD DATASET\n",
    "pdata= PipeDataset(config=config,augmentation=SSL_augmentation)\n",
    "\n",
    "# THIS IS THE CODE TO LOAD MODEL\n",
    "#pmodel=pipe_model(config=config) \n",
    "checkpoint = torch.load(r'experiment_checkpoints\\Magic_cube\\Epoch800_BYOL_Mean\\epoch=399-step=38800.ckpt')\n",
    "#checkpoint = torch.load(r'experiment_checkpoints\\Magic_cube\\Epoch800_BYOL_Mean\\checkpoints-epoch=00-train_loss=-0.90.ckpt')\n",
    "pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "pmodel.load_state_dict(checkpoint[\"state_dict\"])\n",
    "#trainer.validate(pmodel, dataloaders=p_knndata[2])   \n",
    "# Use this if you want to START a train\n",
    "trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =1) #precision='16-mixed',\n",
    "#trainer.validate(pmodel, dataloaders=p_knndata[2])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40511168-8aaf-474d-8e38-64db51620f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2723ddc6-3199-477d-bad2-d6f801de2576",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(pmodel, pdata.dataloader,)  \n",
    "\n",
    "# Use this if you want to CONTINUE a train\n",
    "#trainer=Trainer(config, model_mode=\"fromOther\", extra_epoch=50,check_val_every_n_epoch =4)\n",
    "#trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "print(pmodel.debug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02125c8a-f786-4e8d-a745-99febd783aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a869f3e-3e94-4d05-bec5-6fdd31695b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config2: Augmentation VS accuracy (UA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c3b5a-d63b-4964-8e48-e06bb4d2be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aug1=T.Compose([\n",
    "    T.RandomResizedCrop(size=global_config[\"input_size\"], scale=(0.08, 1.0)),\n",
    "    T.RandomApply([T.RandomRotation(degrees=90)], p=0),\n",
    "    T.RandomHorizontalFlip(p= 0.5),\n",
    "    T.RandomVerticalFlip(p=0),\n",
    "    T.RandomApply([T.ColorJitter(\n",
    "            brightness=0.4,contrast=0.4,saturation=0.4, hue=0.1)], p=0.8),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "Aug2=T.Compose([\n",
    "    T.RandomResizedCrop(size=global_config[\"input_size\"], scale=(0.08, 1.0)),\n",
    "    T.RandomApply([T.RandomRotation(degrees=90)], p=0),\n",
    "    T.RandomHorizontalFlip(p= 0.5),\n",
    "    T.RandomApply([T.ColorJitter(\n",
    "            brightness=0.4,contrast=0.4,saturation=0.4, hue=0.1)], p=0.8),\n",
    "\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "Aug3=T.Compose([\n",
    "    T.RandomResizedCrop(size=global_config[\"input_size\"], scale=(0.08, 1.0)),\n",
    "    T.RandomApply([T.RandomRotation(degrees=90)], p=0),\n",
    "    T.RandomApply([T.ColorJitter(\n",
    "            brightness=0.4,contrast=0.4,saturation=0.4, hue=0.1)], p=0.8),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "Aug4=T.Compose([\n",
    "    T.RandomResizedCrop(size=global_config[\"input_size\"], scale=(0.08, 1.0)),\n",
    "    T.RandomApply([T.ColorJitter(\n",
    "            brightness=0.4,contrast=0.4,saturation=0.4, hue=0.1)], p=0.8),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    " \n",
    "Aug5=T.Compose([\n",
    "    T.RandomResizedCrop(size=global_config[\"input_size\"], scale=(0.08, 1.0)),\n",
    "    T.RandomHorizontalFlip(p= 0.5),\n",
    "    T.RandomVerticalFlip(p=0),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    " \n",
    "Aug6=T.Compose([\n",
    "    T.RandomResizedCrop(size=global_config[\"input_size\"], scale=(0.08, 1.0)),\n",
    "    T.RandomVerticalFlip(p=0),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "Aug7=T.Compose([\n",
    "    T.RandomResizedCrop(size=global_config[\"input_size\"], scale=(0.08, 1.0)),\n",
    "    T.RandomApply([T.RandomRotation(degrees=90)], p=0), \n",
    "    T.RandomHorizontalFlip(p= 0.5),\n",
    "    T.RandomVerticalFlip(p=0.5),\n",
    "    T.RandomApply([T.ColorJitter(\n",
    "            brightness=0.4,contrast=0.4,saturation=0.4, hue=0.1)], p=0.8),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "Aug8=T.Compose([\n",
    "    T.RandomResizedCrop(size=global_config[\"input_size\"], scale=(0.08, 1.0)),\n",
    "    T.RandomApply([T.RandomRotation(degrees=90)], p=0.5), \n",
    "    T.RandomHorizontalFlip(p= 0.5),\n",
    "    T.RandomVerticalFlip(p=0.5),\n",
    "    T.RandomApply([T.ColorJitter(\n",
    "            brightness=0.4,contrast=0.4,saturation=0.4, hue=0.1)], p=0.8),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "Aug9=T.Compose([\n",
    "    T.RandomResizedCrop(size=global_config[\"input_size\"], scale=(0.08, 1.0)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb7f894-cdf9-4795-837f-31da1da1d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for baseline, No in  zip([Aug9],[9]): # #[\"SimSiam\",\"BarlowTwins\",\"VICReg\",\"DCL\",\"MoCo\", \"BYOL\", \"SimCLR\" ]\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    \n",
    "    config=global_config.copy() \n",
    "    SSL_augmentation=baseline\n",
    "    config=load_config(config,\"SimSiam\")\n",
    "    config[\"view\"]=4\n",
    "    config[\"view_model\"]= \"mean_n\"\n",
    "    config[\"name\"]=f\"UA_md{'SimSiam'}_Aug{No}_VM{config['view_model']}_ep50_BA128_v{config['view']}\"\n",
    "    config[\"batch_size\"]=128\n",
    "    config[\"max_epochs\"]= 50\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer  \n",
    "    \n",
    "for baseline, No in  zip([Aug9],[9]): #Aug1,Aug2,Aug3,Aug4,\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    \n",
    "    config=global_config.copy() \n",
    "    SSL_augmentation=baseline\n",
    "    config=load_config(config,\"SimSiam\")\n",
    "    config[\"view\"]=2\n",
    "    config[\"view_model\"]= \"None\"\n",
    "    config[\"name\"]=f\"UA_md{'SimSiam'}_Aug{No}_VM{config['view_model']}_ep50_BA128_v{config['view']}\"\n",
    "    config[\"batch_size\"]=128\n",
    "    config[\"max_epochs\"]= 50\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    \n",
    "    \n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010739d6-05ec-4156-9982-01d6694d0131",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config2.1: Ensemble_Aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586b5e3-d033-4f57-937b-1091e4013aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aug1=T.Compose([\n",
    "    T.RandomResizedCrop(size=global_config[\"input_size\"], scale=(0.08, 1.0)),\n",
    "    T.RandomApply([T.RandomRotation(degrees=90)], p=0),\n",
    "    T.RandomHorizontalFlip(p= 0.5),\n",
    "    T.RandomVerticalFlip(p=0),\n",
    "    T.RandomApply([T.ColorJitter(\n",
    "            brightness=0.4,contrast=0.4,saturation=0.4, hue=0.1)], p=0.8),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "Ens1=T.Compose([\n",
    "    T.RandomResizedCrop(size=global_config[\"input_size\"], scale=(0.08, 1.0)),\n",
    "    T.RandomApply([T.ColorJitter(\n",
    "            brightness=0.4,contrast=0.4,saturation=0.4, hue=0.1)], p=0.8),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "Ens2=T.Compose([\n",
    "    T.RandomHorizontalFlip(p= 0.5),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "Ens3=T.Compose([\n",
    "    T.RandomResizedCrop(size=16, scale=(0.08, 1.0)),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "Ens1=trans2multi(Ens1,view=3)  #39\n",
    "Ens2=trans2multi(Ens2,view=3) #34\n",
    "Ens3=trans2multi(Ens3,view=3)  # 21 Collapse\n",
    "#(En1,En1,En1)\n",
    "#(En1,En2,En3)  #44\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a763af-b728-427a-9787-d50565fcebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from lightly.transforms.multi_view_transform import MultiViewTransform\n",
    "for baseline, No in  zip([Ens3],[\"3\"]): #Ens1,Ens2,MultiViewTransform([Ens1,Ens2,Ens3]), \"1\",\"2\",\"123\",#[\"SimSiam\",\"BarlowTwins\",\"VICReg\",\"DCL\",\"MoCo\", \"BYOL\", \"SimCLR\" ]\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    \n",
    "    config=global_config.copy() \n",
    "    SSL_augmentation=baseline\n",
    "    config=load_config(config,\"SimSiam\")\n",
    "    config[\"view\"]=3\n",
    "    config[\"view_model\"]= \"mean_n\"\n",
    "    config[\"name\"]=f\"Ensemble_md{'SimSiam'}_Aug{No}_VM{config['view_model']}_ep50_BA128_v{config['view']}\"\n",
    "    config[\"batch_size\"]=128\n",
    "    config[\"max_epochs\"]= 10\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=SSL_augmentation)\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer  \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4e6970-4604-485e-bd37-60383de96b81",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config3: ImageNet Acc vs Model vs View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e842f90-be6c-4b0c-a6b5-862e531b5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML file\n",
    "with open('global.yaml', 'r') as file:\n",
    "    global_config = yaml.safe_load(file)\n",
    "\n",
    "# Write your experiment notebook name here\n",
    "global_config[\"experiment\"]=\"Magic_cube\"   \n",
    "global_config[\"dataset_dir\"]=\"D:/Datasets/TinyImageNet/train/\"\n",
    "global_config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,None,None]]     \n",
    "global_config[\"loss_func\"]=\"BarlowTwinsLoss\"     \n",
    "global_config[\"view_model\"]=\"None\"     \n",
    "global_config[\"view\"]=2 \n",
    "global_config[\"stop_gradient\"]=False   \n",
    "global_config[\"optimizer\"]=\"SGD\"      \n",
    "global_config[\"schedule\"]=\"cos\"   \n",
    "global_config[\"model\"]=\"Toymodel\"\n",
    "global_config[\"batch_size\"]=256\n",
    "global_config[\"input_size\"]=64\n",
    "global_config[\"max_epochs\"]=50\n",
    " \n",
    "#global_config[\"backbone\"]=\"resnet18_pretrained\"\n",
    "global_SSL_augmentation=T.Compose([\n",
    "    T.RandomResizedCrop(size=global_config[\"input_size\"], scale=(0.08, 1.0)),\n",
    "    T.RandomApply([T.RandomRotation(degrees=90)], p=0),\n",
    "    T.RandomHorizontalFlip(p= 0.5),\n",
    "    T.RandomVerticalFlip(p=0),\n",
    "    T.RandomApply([T.ColorJitter(\n",
    "            brightness=0.4,contrast=0.4,saturation=0.4, hue=0.1)], p=0.8),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Define global Training Augmentation\n",
    "\n",
    "\n",
    "\n",
    "# Define the Testing Augmentation\n",
    "test_SSL_augmentation=T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "p_knndata= PipeDataset(input_dir=global_config[\"path_to_test_cifar10\"], \n",
    "        augmentation=trans2multi(test_SSL_augmentation,view=1), \n",
    "        batch_size=global_config[\"batch_size\"],num_workers=global_config[\"num_workers\"]).dataloader\n",
    "p_knndata=[p_knndata,200]  # The second number is the classes number of this datasets\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e7ad0-fb2a-48e4-adc0-1507f6bfb41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "for baseline in  [\"DCL\",\"MoCo\", \"BYOL\", \"SimCLR\" ]: # #[\"SimSiam\",\"BarlowTwins\",\"VICReg\",\"DCL\",\"MoCo\", \"BYOL\", \"SimCLR\" ]\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy() \n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config=load_config(config,baseline)\n",
    "    config[\"view\"]=2\n",
    "    config[\"view_model\"]= \"None\"\n",
    "    config[\"name\"]=f\"Base_ImageNet_md{baseline}_VM{config['view_model']}_ep50_v{config['view']}\"\n",
    "    config[\"batch_size\"]=256\n",
    "    config[\"max_epochs\"]= 50\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf520375-26f2-4694-b5e2-6a19fdcec91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for baseline in  [ \"SimCLR\", \"BarlowTwins\"]: # #[\"SimSiam\",\"BarlowTwins\",\"VICReg\",\"DCL\",\"MoCo\", \"BYOL\", \"SimCLR\" ]\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy() \n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config=load_config(config,baseline)\n",
    "    config[\"view\"]=4\n",
    "    config[\"view_model\"]= \"mean_n\"\n",
    "    config[\"name\"]=f\"Base_ImageNet_md{baseline}_VM{config['view_model']}_ep50_v{config['view']}\"\n",
    "    config[\"batch_size\"]=256\n",
    "    config[\"max_epochs\"]= 50\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a70d5bb-279d-47b1-8850-dd2288e5e568",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config4: Check different view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d877c7-b5f9-4dd9-ae01-04f29327fe52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Config5: Check how it improve the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b96d25f-650f-4f34-bbb6-506951b05885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish Baseline Good job, then take a rest, do experiment, and go study\n",
    "# Make static augmentation\n",
    "# Finish Experiment \n",
    "# Finish Decision Tree   go engage here\n",
    "# Find pattern\n",
    "# Try to explain the improve   \n",
    "# Try to explain the model Collapse\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50686839-6777-4c26-b7f6-02b871ac184a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88791b3e-d746-4ca8-b501-f4c3edd3ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"C:/Users/isxzl/OneDrive/Code/AutoSSL\") \n",
    "from sklearn.preprocessing import normalize\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from autoSSL.models.Backbone import pipe_backbone\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def embed(x, embedding_model, device):\n",
    "    embedding_model.eval()\n",
    "    embedding_model.to(device)\n",
    "    x = x.float().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = embedding_model(x)\n",
    "        pooled_embeddings = torch.nn.functional.adaptive_avg_pool2d(embeddings, (1, 1))\n",
    "\n",
    "    return pooled_embeddings.view(pooled_embeddings.size(0), -1).cpu().numpy()\n",
    "\n",
    "def embed_list(X_train, embedding_model,device):\n",
    "    X_train_embedding = [embed(x, embedding_model, device) for x in DataLoader(X_train, batch_size=16)]\n",
    "    X_train_embedding = np.concatenate(X_train_embedding)\n",
    "    #X_train_embedding = normalize(X_train_embedding)  # Normalize train embeddings\n",
    "    return X_train_embedding\n",
    "\n",
    "    \n",
    "def evaluate_accuracy(X_train_embedding,X_test_embedding,y_train,y_test):\n",
    "    clf = SGDClassifier(loss='log_loss')\n",
    "\n",
    "    clf.fit(X_train_embedding, y_train)\n",
    "\n",
    "    # Get class probabilities for each sample\n",
    "    class_probs = clf.predict_proba(X_test_embedding)\n",
    "\n",
    "    # Get the top 1 predictions\n",
    "    top1_preds = np.argmax(class_probs, axis=1)\n",
    "    top3_preds = np.argpartition(class_probs, -3, axis=1)[:,-3:]\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    confusion = confusion_matrix(y_test, top1_preds)\n",
    "    # Normalize confusion matrix by row (i.e by the number of samples in each class)\n",
    "    confusion = confusion.astype('float') / confusion.sum(axis=1)[:, np.newaxis]\n",
    "    # Average accuracy is the mean of the diagonal elements (the correctly classified instances)\n",
    "    top1_average_accuracy = np.mean(np.diag(confusion))\n",
    "\n",
    "    # Calculate accuracy\n",
    "    top1_accuracy = accuracy_score(y_test, top1_preds)\n",
    "    top3_accuracy = np.mean([1 if y in top3 else 0 for y, top3 in zip(y_test, top3_preds)])\n",
    "\n",
    "    # K-Nearest Neighbors classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=20)\n",
    "    knn.fit(X_train_embedding, y_train)\n",
    "    knn_preds = knn.predict(X_test_embedding)\n",
    "    knn_accuracy = accuracy_score(y_test, knn_preds)\n",
    "\n",
    "    accuracy = {\n",
    "        \"Top-1 Accuracy\": top1_accuracy,\n",
    "        \"Top-3 Accuracy\": top3_accuracy,\n",
    "        \"Top-1 Average Accuracy\": top1_average_accuracy,\n",
    "        \"KNN Top-1 Accuracy\": knn_accuracy  # KNN accuracy\n",
    "    }\n",
    "    return accuracy, confusion\n",
    "    \n",
    "    \n",
    "def eval_everything(X_train,y_train,X_test,y_test, models, device='cuda',baseline=None):\n",
    "    \n",
    "    \n",
    "    # COnsider the input model is list or model is a single model\n",
    "    if isinstance(models, torch.nn.Module):\n",
    "        models = {'name': ['model_0'], 'model': [models], 'address': None} \n",
    "    elif isinstance(models, list):\n",
    "        models = {'name': ['model_'+str(i) for i in range(len(models))], 'model': models, 'address': None}\n",
    "\n",
    "    results = []\n",
    "    baselines_name=[]\n",
    "    if baseline:\n",
    "        for base in baseline:\n",
    "            baseline_backbone, _ = pipe_backbone(backbone=base)\n",
    "            models['model'].append(baseline_backbone)\n",
    "            models['name'].append('baseline_' + base)\n",
    "            baselines_name.append('baseline_' + base)\n",
    "        \n",
    "    writer = pd.ExcelWriter(models['address'].replace('.csv', '_confusion.xlsx'))\n",
    "\n",
    "    for i, embedding_model in enumerate(tqdm(models['model'])):\n",
    "\n",
    "        X_train_embedding = embed_list(X_train,embedding_model,device)  # Normalize train embeddings\n",
    "        X_test_embedding = embed_list(X_test,embedding_model,device)  # Normalize train embeddings\n",
    "\n",
    "        if X_test_embedding is None:\n",
    "            accuracy = 'model_collapse'\n",
    "            confusion = None\n",
    "        else:\n",
    "            accuracy,confusion=evaluate_accuracy(X_train_embedding,X_test_embedding,y_train,y_test)\n",
    "            \n",
    "        namee = models[\"name\"][i]\n",
    "        results.append((namee, accuracy))\n",
    "\n",
    "        del embedding_model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Save confusion matrix to Excel file\n",
    "        if confusion is not None:\n",
    "            df_confusion = pd.DataFrame(confusion)\n",
    "            df_confusion.to_excel(writer, sheet_name=namee)\n",
    "\n",
    "    writer.save()\n",
    "\n",
    "    if models['address'] is not None:\n",
    "        df = pd.read_csv(models['address'])\n",
    "\n",
    "        # If baselines are present, add new rows in the dataframe for them\n",
    "        if baselines_name:\n",
    "            for base_name in  baselines_name:\n",
    "                # Initialize a new row with default values\n",
    "                new_row = {col: None for col in df.columns}\n",
    "                # Update the values we know\n",
    "                new_row.update({\n",
    "                    'dir_name': base_name,\n",
    "                })\n",
    "                # Append the new row to the dataframe\n",
    "                df = df.append(new_row, ignore_index=True)\n",
    "\n",
    "        # Assuming the results are in the same order as the models in the dataframe\n",
    "        df['linear_top1_accuracy'] = [result[1][\"Top-1 Accuracy\"] for result in results]\n",
    "        df['linear_top3_accuracy'] = [result[1][\"Top-3 Accuracy\"] for result in results]\n",
    "        df['linear_top1_average_accuracy'] = [result[1][\"Top-1 Average Accuracy\"] for result in results]\n",
    "        df['linear_knn_top1_accuracy'] = [result[1][\"KNN Top-1 Accuracy\"] for result in results]\n",
    "\n",
    "        df.to_csv(models['address'], index=False)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa46e344-ee4f-447b-bf94-53e0275fa1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmark_logs/800epoch\\BYOL\\checkpoints\\epoch=799-step=77600.ckpt\n",
      "Successfully load resnet18_5layer\n",
      "benchmark_logs/800epoch\\BYOL_Mean\\checkpoints\\epoch=799-step=77600.ckpt\n",
      "Successfully load resnet18_5layer\n",
      "benchmark_logs/800epoch\\Moco\\checkpoints\\epoch=799-step=77600.ckpt\n",
      "Successfully load resnet18_5layer_split8\n",
      "benchmark_logs/800epoch\\Moco_mean\\checkpoints\\epoch=799-step=77600.ckpt\n",
      "Successfully load resnet18_5layer_split8\n",
      "benchmark_logs/800epoch\\Moco_mean_Shuffle\\checkpoints\\epoch=799-step=77600.ckpt\n",
      "Successfully load resnet18_5layer_split8\n",
      "benchmark_logs/800epoch\\Moco_vanilla\\checkpoints\\epoch=799-step=77600.ckpt\n",
      "Successfully load resnet18_5layer_split8\n",
      "benchmark_logs/800epoch\\SimCLR\\checkpoints\\epoch=799-step=77600.ckpt\n",
      "Successfully load resnet18_5layer\n",
      "benchmark_logs/800epoch\\SimCLR_mean\\checkpoints\\epoch=799-step=77600.ckpt\n",
      "Successfully load resnet18_5layer\n",
      "benchmark_logs/800epoch\\SimSiam\\checkpoints\\epoch=799-step=77600.ckpt\n",
      "Successfully load resnet18_5layer\n",
      "benchmark_logs/800epoch\\SimSiam_mean\\checkpoints\\epoch=799-step=77600.ckpt\n",
      "Successfully load resnet18_5layer\n",
      "benchmark_logs/800epoch\\VICReg\\checkpoints\\epoch=799-step=77600.ckpt\n",
      "Successfully load resnet18\n",
      "benchmark_logs/800epoch\\VICReg_mean\\checkpoints\\epoch=799-step=77600.ckpt\n",
      "Successfully load resnet18\n",
      "Collating the models' (evaluating) information to benchmark_logs/800epoch/._.csv\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append(\"C:/Users/isxzl/OneDrive/Code/AutoSSL\")\n",
    "from autoSSL.data import PipeDataset \n",
    "from autoSSL.utils import  dict2transformer,trans2multi  \n",
    "import torchvision.transforms as T\n",
    "from autoSSL.evaluate import eval_KNN,eval_linear,eval_KNNplot,pipe_collate#,eval_everything\n",
    "import yaml\n",
    "# Load the YAML file\n",
    "with open('global.yaml', 'r') as file:\n",
    "    global_config = yaml.safe_load(file)\n",
    "\n",
    "\n",
    "test_augmentation=T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# \"LNeg_VM.*_PRTruewithBN_stTrue_b128_m50_v4\"\n",
    "\n",
    "\n",
    "collate =pipe_collate(address=\"benchmark_logs/800epoch/\", reg=\".*\",autoDL=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc8aa1-467f-493a-a3aa-d0ee02b35516",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b53198b8-55d5-4130-9eb8-7b45a91af0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|                                                | 28861/80000 [02:02<03:37, 235.20it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pdata = PipeDataset(input_dir=r\"D:\\Datasets\\TinyImageNet\\test\",\n",
    "    augmentation=trans2multi(test_augmentation,view=1))\n",
    "train_data, test_data = pdata.split(0.8)\n",
    "X_train,y_train,X_test,y_test=train_data.array[0], train_data.array[1], test_data.array[0], test_data.array[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "386b7677-2479-441d-ac25-26a51378474f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 12/12 [00:31<00:00,  2.63s/it]\n",
      "C:\\Users\\isxzl\\AppData\\Local\\Temp\\ipykernel_9568\\3143246462.py:117: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
      "  writer.save()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aaa=eval_everything(X_train,y_train,X_test,y_test,\n",
    "                    models=collate, device=global_config[\"device\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c583c068-1bae-478b-8ab7-961f3bf4612d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e72bed1-ecca-4c85-b978-3af44daeb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef441cb7-c3a8-4851-ba60-e0dcbf70738a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
