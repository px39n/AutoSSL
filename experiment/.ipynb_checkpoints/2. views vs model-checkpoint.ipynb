{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4533ee81-d579-4867-b15b-aa7f3f8f385f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiment Introduction\n",
    "\n",
    "Background, \n",
    "In order to do ablation experiment, we have diffculty like\n",
    "1. most of them are same, we have to repeat many times.\n",
    "2. too many model, config, code hard to manage and save. too messy\n",
    "\n",
    "\n",
    "For all kind of SSL training workflow, we have to define the hyperparameters includes 4 aspect,\n",
    "Dataset\n",
    "Model\n",
    "Training\n",
    "Saving COnfig\n",
    "\n",
    "\n",
    "\n",
    "How to Use this Experiment?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff5e60f-6c7d-47b6-8b30-5ad2e6c94b09",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64951e3b-e012-4ed5-8846-03eed103c696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71222088-d62f-40fc-b935-f9c8ee99a579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import config\n",
    "import sys \n",
    "sys.path.append(\"C:/Users/isxzl/OneDrive/Code/AutoSSL\")\n",
    "import yaml\n",
    "from torchvision.transforms import RandomRotation,GaussianBlur,ColorJitter\n",
    "from autoSSL.evaluate import eval_KNN,eval_linear,eval_KNNplot,pipe_collate\n",
    "from autoSSL.models import BarlowTwins, BYOL, MoCo, SimCLR, SimSiam, VICReg ,Toymodel, pipe_model \n",
    "from autoSSL.utils import embedding_feature,ck_callback,dict2transformer,trans2multi,join_dir,ContinuousCSVLogger  \n",
    "from autoSSL.data import PipeDataset\n",
    "from autoSSL.train import Trainer\n",
    "\n",
    "from lightly.transforms import SimCLRTransform\n",
    "from lightly.transforms.rotation import random_rotation_transform\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eba85e-c2d4-4ed3-a276-2b03ff82f764",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Global Baseline Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16334f28-7fca-42b0-afbc-f19c2b4ee76f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb0e5479-43a3-4b63-b9e2-b8ce70103290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML file\n",
    "with open('global.yaml', 'r') as file:\n",
    "    global_config = yaml.safe_load(file)\n",
    "\n",
    "# Write your experiment notebook name here\n",
    "global_config[\"experiment\"]=\"views VS model\"   \n",
    "global_config[\"dataset_dir\"]=\"../Datasets/cifar10/train/\"\n",
    "global_config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,None,None]]    \n",
    "\n",
    "global_config[\"loss_func\"]=\"BarlowTwinsLoss\"     \n",
    "global_config[\"view_model\"]=\"None\"     \n",
    "global_config[\"view\"]=2 \n",
    "global_config[\"stop_gradient\"]=False   \n",
    "global_config[\"optimizer\"]=\"SGD\"      \n",
    "global_config[\"schedule\"]=\"cos\"   \n",
    "global_config[\"model\"]=\"Toymodel\"\n",
    "global_config[\"batch_size\"]=128\n",
    "global_config[\"input_size\"]=32\n",
    "global_config[\"max_epochs\"]=50\n",
    "\n",
    "#global_config[\"backbone\"]=\"resnet18_pretrained\"\n",
    "\n",
    "# Define global Training Augmentation\n",
    "global_SSL_augmentation=T.Compose([\n",
    "    T.RandomResizedCrop(size=global_config[\"input_size\"], scale=(0.08, 1.0)),\n",
    "    T.RandomApply([T.RandomRotation(degrees=90)], p=0),\n",
    "    T.RandomHorizontalFlip(p= 0.5),\n",
    "    T.RandomVerticalFlip(p=0),\n",
    "    T.RandomApply([T.ColorJitter(\n",
    "            brightness=0.4,contrast=0.4,saturation=0.4, hue=0.1)], p=0.8),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "# Define the Testing Augmentation\n",
    "test_SSL_augmentation=T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "p_knndata= PipeDataset(input_dir=global_config[\"path_to_test_cifar10\"], \n",
    "        augmentation=trans2multi(test_SSL_augmentation,view=1), \n",
    "        batch_size=global_config[\"batch_size\"],num_workers=global_config[\"num_workers\"]).dataloader\n",
    "p_knndata=[p_knndata,10]  # The second number is the classes number of this datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f8a6e-bfc8-4b5b-98b0-4c3797a6f437",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Config 1 Baseline for everything (Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa842a-b5ad-4667-b051-2d91f4358bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "st=True\n",
    "\n",
    "\n",
    "for baseline in [[pd,st,True],[[],False,False],[pd,False,True],[[],st,False]]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=2\n",
    "    config[\"view_model\"]= \"None\"\n",
    "    config[\"name\"]=f\"LNeg_VMNone_PR{baseline[2]}_st{baseline[1]}_b128_m50_v{4}\"\n",
    "    config[\"loss_func\"]=\"NegativeCosineSimilarity\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= baseline[1]\n",
    "    config[\"predhead_dim\"]= baseline[0]\n",
    "    config[\"max_epochs\"]= 50\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a869f3e-3e94-4d05-bec5-6fdd31695b58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Config2 Grid Strategy View 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dd33db-b0f5-4406-a9d7-42f5d3b0b566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:615: UserWarning: Checkpoint directory C:\\Users\\isxzl\\OneDrive\\Code\\AutoSSL\\experiment\\experiment_checkpoints\\views VS model\\LNeg_VMvar_view_PRNoBN_stTrue_b128_m50_v4 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                     | Params\n",
      "-------------------------------------------------------------\n",
      "0 | backbone        | Sequential               | 11.2 M\n",
      "1 | criterion       | NegativeCosineSimilarity | 0     \n",
      "2 | projection_head | ProjectionHead           | 5.3 M \n",
      "3 | prediction_head | ProjectionHead           | 2.1 M \n",
      "-------------------------------------------------------------\n",
      "18.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "18.5 M    Total params\n",
      "74.111    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  74%|█████████████████████████████████████████████▋                | 287/390 [00:27<00:09, 10.55it/s, v_num=1]"
     ]
    }
   ],
   "source": [
    "\n",
    "for baseline0 in [\"var_view\",\"Stop\", \"mean_n\" ,\"fastsim\", \"pair-pair\" , \"1_n\" ,\"1_fastsim\"]:   \n",
    "    \n",
    "    \n",
    "    pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    st=True\n",
    "    pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]  \n",
    "    for baseline in [[pd,st,\"NoBN\"]]: #,[[],False,False],[pd,False,\"TruewithBN\"],[[],st,False]\n",
    "        # MAKE YOUR OWN CONFIG\n",
    "        config=global_config.copy()\n",
    "        # Fill the config\n",
    "        SSL_augmentation=global_SSL_augmentation\n",
    "        config[\"view\"]=4\n",
    "        config[\"view_model\"]= baseline0\n",
    "        config[\"name\"]=f\"LNeg_VM{baseline0}_PR{baseline[2]}_st{baseline[1]}_b128_m50_v{4}\"\n",
    "        config[\"loss_func\"]=\"NegativeCosineSimilarity\" \n",
    "        config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "        config[\"stop_gradient\"]= baseline[1]\n",
    "        config[\"predhead_dim\"]= baseline[0]\n",
    "        config[\"max_epochs\"]= 50\n",
    "         \n",
    "        # THIS IS THE CODE TO LOAD DATASET\n",
    "        pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "\n",
    "        # THIS IS THE CODE TO LOAD MODEL\n",
    "        #pmodel=pipe_model(config=config) \n",
    "        pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "\n",
    "\n",
    "        # Use this if you want to START a train\n",
    "        trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =1) #precision='16-mixed',\n",
    "        trainer.fit(pmodel, pdata.dataloader,)  \n",
    "\n",
    "        # Use this if you want to CONTINUE a train\n",
    "        #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "        #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "        print(pmodel.debug)\n",
    "        del pdata\n",
    "        del pmodel\n",
    "        del trainer    \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c3b5a-d63b-4964-8e48-e06bb4d2be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "for baseline0 in [\"var_view\"]:   \n",
    "    \n",
    "    \n",
    "    pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    st=True\n",
    "    pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]  \n",
    "    for baseline in [[pd,st,\"NoBN\"]]: #,[[],False,False],[pd,False,\"TruewithBN\"],[[],st,False]\n",
    "        # MAKE YOUR OWN CONFIG\n",
    "        config=global_config.copy()\n",
    "        # Fill the config\n",
    "        SSL_augmentation=global_SSL_augmentation\n",
    "        config[\"view\"]=4\n",
    "        config[\"view_model\"]= baseline0\n",
    "        config[\"name\"]=f\"Test_adam_LNeg_VM{baseline0}_PR{baseline[2]}_st{baseline[1]}_b128_m50_v{4}\"\n",
    "        config[\"loss_func\"]=\"NegativeCosineSimilarity\"\n",
    "        config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "        config[\"stop_gradient\"]= baseline[1]\n",
    "        config[\"predhead_dim\"]= baseline[0]\n",
    "        config[\"max_epochs\"]= 50\n",
    "        config[\"optimizer\"]= \"Adam\"\n",
    "        # THIS IS THE CODE TO LOAD DATASET\n",
    "        pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "\n",
    "        # THIS IS THE CODE TO LOAD MODEL\n",
    "        #pmodel=pipe_model(config=config) \n",
    "        pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "\n",
    "\n",
    "        # Use this if you want to START a train\n",
    "        trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =1) #precision='16-mixed',\n",
    "        trainer.fit(pmodel, pdata.dataloader,)  \n",
    "\n",
    "        # Use this if you want to CONTINUE a train\n",
    "        #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "        #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "        print(pmodel.debug)\n",
    "        del pdata\n",
    "        del pmodel\n",
    "        del trainer    \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4e6970-4604-485e-bd37-60383de96b81",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Config3: view can fix stop or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1352c0-9e27-470a-919d-af85ba9365b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]    \n",
    "st=True\n",
    "for baseline in [[pd2,\"TrueBN\"]]:  #[pd,\"TruenoBN\"],\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=4\n",
    "    config[\"view_model\"]= \"mean_n\"\n",
    "    config[\"name\"]=f\"LNeg_VMmean_n_PR{baseline[1]}_stFalse_b128_m50_v{4}\"\n",
    "    config[\"loss_func\"]=\"NegativeCosineSimilarity\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= False\n",
    "    config[\"predhead_dim\"]= baseline[0]\n",
    "    config[\"max_epochs\"]= 50\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =1) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a70d5bb-279d-47b1-8850-dd2288e5e568",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Config4: Check different view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb66c1a-89fa-4ce2-9f35-92ba04726170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]    \n",
    "st=True\n",
    "for baseline in [6]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=baseline\n",
    "    config[\"view_model\"]= \"mean_n\" #\"1_fastsim\"  \n",
    "    config[\"name\"]=f\"LNeg_VMmean_n_PRTruenoBN_stTrue_b128_m50_v{baseline}\"\n",
    "    config[\"loss_func\"]=\"NegativeCosineSimilarity\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= True\n",
    "    config[\"predhead_dim\"]= [[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    config[\"max_epochs\"]= 50\n",
    "\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d877c7-b5f9-4dd9-ae01-04f29327fe52",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config5: Check how it improve the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "428c066a-22d5-459d-9e6a-5262202bc83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:615: UserWarning: Checkpoint directory C:\\Users\\isxzl\\OneDrive\\Code\\AutoSSL\\experiment\\experiment_checkpoints\\views VS model\\Model_VICREG_VMNone_v2 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type           | Params\n",
      "---------------------------------------------------\n",
      "0 | backbone        | Sequential     | 11.2 M\n",
      "1 | criterion       | VICRegLoss     | 0     \n",
      "2 | projection_head | ProjectionHead | 5.3 M \n",
      "---------------------------------------------------\n",
      "16.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "16.4 M    Total params\n",
      "65.710    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                 | 0/390 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Use this if you want to START a train\u001b[39;00m\n\u001b[0;32m     28\u001b[0m trainer\u001b[38;5;241m=\u001b[39mTrainer(config, model_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m,check_val_every_n_epoch \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m) \u001b[38;5;66;03m#precision='16-mixed',\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Use this if you want to CONTINUE a train\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(pmodel\u001b[38;5;241m.\u001b[39mdebug)\n",
      "File \u001b[1;32mC:\\Users/isxzl/OneDrive/Code/AutoSSL\\autoSSL\\train\\pipe_train.py:78\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, dataloader, val_dataloaders, ckpt_path)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, current epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     76\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader_kNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruning time(min)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:531\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    529\u001b[0m model \u001b[38;5;241m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 531\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:42\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     45\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:570\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[0;32m    561\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[0;32m    562\u001b[0m )\n\u001b[0;32m    564\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    566\u001b[0m     ckpt_path,\n\u001b[0;32m    567\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    568\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    569\u001b[0m )\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:975\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    980\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1018\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1018\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:201\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:354\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(combined_loader)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:133\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:218\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:185\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m         closure()\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:260\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:140\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 140\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    143\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\core\\module.py:1256\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1220\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1223\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1224\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1225\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer`\u001b[39;00m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03m    calls the optimizer.\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;124;03m                    pg[\"lr\"] = lr_scale * self.learning_rate\u001b[39;00m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1256\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:155\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy\u001b[38;5;241m.\u001b[39moptimizer_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer, closure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:225\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[1;32m--> 225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39moptimizer_step(optimizer, model\u001b[38;5;241m=\u001b[39mmodel, closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:114\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    113\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     68\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\torch\\optim\\sgd.py:67\u001b[0m, in \u001b[0;36mSGD.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m---> 67\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m     70\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:101\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     91\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     92\u001b[0m     optimizer: Optimizer,\n\u001b[0;32m     93\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m     94\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     95\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 126\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:307\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m    306\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[1;32m--> 307\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:287\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 287\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    290\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:367\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mtrain_step_context():\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, TrainingStep)\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtraining_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Users/isxzl/OneDrive/Code/AutoSSL\\autoSSL\\models\\Toymodel.py:104\u001b[0m, in \u001b[0;36mToymodel.training_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mview_model\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 104\u001b[0m         (x0, x1), _, _ \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_gradient \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredhead_dim:\n\u001b[0;32m    106\u001b[0m             z0, p0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_with_p(x0)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]    \n",
    "st=True\n",
    "\n",
    "for baseline in [[\"None\",2]]: #\"mean_n\",\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=baseline[1]\n",
    "    config[\"view_model\"]= baseline[0]#\"1_fastsim\"  \n",
    "    config[\"name\"]=f\"Model_{'VICREG'}_VM{baseline[0]}_v{baseline[1]}\"\n",
    "    config[\"loss_func\"]=\"VICRegLoss\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= False\n",
    "    #config[\"predhead_dim\"]= [[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    config[\"max_epochs\"]= 25\n",
    "\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67682be1-2079-464a-97e5-26c474f5bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "for baseline in [[\"mean_n\",4],[\"None\",2]]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=4\n",
    "    config[\"view_model\"]= \"mean_n\" #\"1_fastsim\"  \n",
    "    config[\"name\"]=f\"Model_{'BT'}_VM{baseline}_v{4}\"\n",
    "    config[\"loss_func\"]=\"BarlowTwinsLoss\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= False\n",
    "    #config[\"predhead_dim\"]= [[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    config[\"max_epochs\"]= 25\n",
    "\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9f088-0da9-46fc-ae90-6bbc2a94ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for baseline in [[\"mean_n\",4],[\"None\",2]]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=4\n",
    "    config[\"view_model\"]= \"mean_n\" #\"1_fastsim\"  \n",
    "    config[\"name\"]=f\"Model_{'SimCLR'}_VM{baseline}_v{4}\"\n",
    "    config[\"loss_func\"]=\"SimCLR\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= False\n",
    "    #config[\"predhead_dim\"]= [[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    config[\"max_epochs\"]= 25\n",
    "\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24c6a03-29f7-449a-af3a-05075be370e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Improved Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d224f-296a-4bbf-9fbd-d89c516521b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for baseline in [4,12]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"name\"]=f\"mean_n_BT_Baseline_b128_m50_v{str(baseline)}\"\n",
    "\n",
    "    config[\"view\"]=baseline\n",
    "    config[\"view_model\"]= \"mean_n\"   # 1_n # mean_n #1_fastsim\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=dict2transformer(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c000950-f854-4901-b001-79a712b2fcc2",
   "metadata": {},
   "source": [
    "## Config 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e13b52-9b87-4072-9103-b8fd93f3de24",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "for baseline in [4,12]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"name\"]=f\"1_fastsim_BT_Baseline_b128_m50_v{str(baseline)}\"\n",
    "\n",
    "    config[\"view\"]=baseline\n",
    "    config[\"view_model\"]= \"1_fastsim\"   # 1_n # mean_n #\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=dict2transformer(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50686839-6777-4c26-b7f6-02b871ac184a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "404d98da-b3a5-4e58-8628-eb1acb25fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from sklearn.preprocessing import normalize\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from autoSSL.models.Backbone import pipe_backbone\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def embed(x, embedding_model, device):\n",
    "    embedding_model.eval()\n",
    "    embedding_model.to(device)\n",
    "    x = x.float().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = embedding_model(x)\n",
    "        pooled_embeddings = torch.nn.functional.adaptive_avg_pool2d(embeddings, (1, 1))\n",
    "\n",
    "    return pooled_embeddings.view(pooled_embeddings.size(0), -1).cpu().numpy()\n",
    "\n",
    "def eval_everything(pipe_data, models, device='cuda', split=None, test=None, baseline=None):\n",
    "    if split is not None:\n",
    "        train_data, test_data = pipe_data.split(split)\n",
    "    elif test is None:\n",
    "        train_data = pipe_data\n",
    "        test_data = pipe_data\n",
    "    else:         \n",
    "        train_data = pipe_data\n",
    "        test_data = test\n",
    "\n",
    "    print(\"Load the training and testing dataset\")\n",
    "    X_train, y_train = train_data.array[0], train_data.array[1]\n",
    "    X_test, y_test = test_data.array[0], test_data.array[1]\n",
    "\n",
    "    if isinstance(models, torch.nn.Module):\n",
    "        models = {'name': ['model_0'], 'model': [models], 'address': None} \n",
    "    elif isinstance(models, list):\n",
    "        models = {'name': ['model_'+str(i) for i in range(len(models))], 'model': models, 'address': None}\n",
    "     \n",
    "    baselines=[]\n",
    "    results = []\n",
    "    baselines_name=[]\n",
    "    if baseline:\n",
    "        for base in baseline:\n",
    "            baseline_backbone, _ = pipe_backbone(backbone=base)\n",
    "            models['model'].append(baseline_backbone)\n",
    "            models['name'].append('baseline_' + base)\n",
    "            baselines_name.append('baseline_' + base)\n",
    "            baselines.append(baseline_backbone)\n",
    "        \n",
    "    writer = pd.ExcelWriter(models['address'].replace('.csv', '_confusion.xlsx'))\n",
    "\n",
    "    for i, embedding_model in enumerate(tqdm(models['model'])):\n",
    "        if embedding_model in baselines:\n",
    "            pass\n",
    "        else:\n",
    "            embedding_model=embedding_model.backbone\n",
    "        X_train_embedding = [embed(x, embedding_model, device) for x in DataLoader(X_train, batch_size=16)]\n",
    "        X_train_embedding = np.concatenate(X_train_embedding)\n",
    "\n",
    "        X_test_embedding = [embed(x, embedding_model, device) for x in DataLoader(X_test, batch_size=16)]\n",
    "        X_test_embedding = np.concatenate(X_test_embedding)\n",
    "\n",
    "        X_train_embedding = normalize(X_train_embedding)  # Normalize train embeddings\n",
    "\n",
    "        X_test_embedding = normalize(X_test_embedding)  # Normalize test embeddings\n",
    "\n",
    "        \n",
    "        if X_test_embedding is None:\n",
    "            accuracy = 'model_collapse'\n",
    "            confusion = None\n",
    "        else:\n",
    "            clf = SGDClassifier(loss='log_loss')\n",
    "\n",
    "            clf.fit(X_train_embedding, y_train)\n",
    "\n",
    "            # Get class probabilities for each sample\n",
    "            class_probs = clf.predict_proba(X_test_embedding)\n",
    "\n",
    "            # Get the top 1 predictions\n",
    "            top1_preds = np.argmax(class_probs, axis=1)\n",
    "            top3_preds = np.argpartition(class_probs, -3, axis=1)[:,-3:]\n",
    "\n",
    "            # Calculate confusion matrix\n",
    "            confusion = confusion_matrix(y_test, top1_preds)\n",
    "            # Normalize confusion matrix by row (i.e by the number of samples in each class)\n",
    "            confusion = confusion.astype('float') / confusion.sum(axis=1)[:, np.newaxis]\n",
    "            # Average accuracy is the mean of the diagonal elements (the correctly classified instances)\n",
    "            top1_average_accuracy = np.mean(np.diag(confusion))\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            top1_accuracy = accuracy_score(y_test, top1_preds)\n",
    "            top3_accuracy = np.mean([1 if y in top3 else 0 for y, top3 in zip(y_test, top3_preds)])\n",
    "                            \n",
    "            # K-Nearest Neighbors classifier\n",
    "            knn = KNeighborsClassifier(n_neighbors=20)\n",
    "            knn.fit(X_train_embedding, y_train)\n",
    "            knn_preds = knn.predict(X_test_embedding)\n",
    "            knn_accuracy = accuracy_score(y_test, knn_preds)\n",
    "\n",
    "            accuracy = {\n",
    "                \"Top-1 Accuracy\": top1_accuracy,\n",
    "                \"Top-3 Accuracy\": top3_accuracy,\n",
    "                \"Top-1 Average Accuracy\": top1_average_accuracy,\n",
    "                \"KNN Top-1 Accuracy\": knn_accuracy  # KNN accuracy\n",
    "            }\n",
    "\n",
    "        namee = models[\"name\"][i]\n",
    "        results.append((namee, accuracy))\n",
    "\n",
    "        \n",
    "        del embedding_model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Save confusion matrix to Excel file\n",
    "        if confusion is not None:\n",
    "            df_confusion = pd.DataFrame(confusion)\n",
    "            df_confusion.to_excel(writer, sheet_name=namee)\n",
    "\n",
    "    writer.save()\n",
    "\n",
    "    if models['address'] is not None:\n",
    "        df = pd.read_csv(models['address'])\n",
    "\n",
    "        # If baselines are present, add new rows in the dataframe for them\n",
    "        if baselines:\n",
    "            for base, base_name in zip(baselines, baselines_name):\n",
    "                # Initialize a new row with default values\n",
    "                new_row = {col: None for col in df.columns}\n",
    "                # Update the values we know\n",
    "                new_row.update({\n",
    "                    'dir_name': base_name,\n",
    "                })\n",
    "                # Append the new row to the dataframe\n",
    "                df = df.append(new_row, ignore_index=True)\n",
    "\n",
    "        # Assuming the results are in the same order as the models in the dataframe\n",
    "        df['linear_top1_accuracy'] = [result[1][\"Top-1 Accuracy\"] for result in results]\n",
    "        df['linear_top3_accuracy'] = [result[1][\"Top-3 Accuracy\"] for result in results]\n",
    "        df['linear_top1_average_accuracy'] = [result[1][\"Top-1 Average Accuracy\"] for result in results]\n",
    "        df['linear_knn_top1_accuracy'] = [result[1][\"KNN Top-1 Accuracy\"] for result in results]\n",
    "\n",
    "        df.to_csv(models['address'], index=False)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8ad6455-b8c3-4d5d-b6eb-5627bd52fbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collating the models' (evaluating) information to experiment_checkpoints/views VS model/LNeg_VMmean_n_PRTruenoBN_stTrue_b128_m50_v_.csv\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append(\"C:/Users/isxzl/OneDrive/Code/AutoSSL\")\n",
    "from autoSSL.data import PipeDataset \n",
    "from autoSSL.utils import  dict2transformer,trans2multi  \n",
    "import torchvision.transforms as T\n",
    "from autoSSL.evaluate import eval_KNN,eval_linear,eval_KNNplot,pipe_collate#,eval_everything\n",
    "import yaml\n",
    "# Load the YAML file\n",
    "with open('global.yaml', 'r') as file:\n",
    "    global_config = yaml.safe_load(file)\n",
    "\n",
    "\n",
    "test_augmentation=T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# \"LNeg_VM.*_PRTruewithBN_stTrue_b128_m50_v4\"\n",
    "\n",
    "collate =pipe_collate(address=\"experiment_checkpoints/views VS model/\", reg=\"Model_*\")\n",
    "\n",
    "pdata = PipeDataset(input_dir=global_config[\"path_to_test_cifar10\"],\n",
    "    augmentation=trans2multi(test_augmentation,view=1))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca362de1-b68b-45c8-94f6-853afff878aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the training and testing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10001/10001 [00:03<00:00, 3028.21it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 10001/10001 [00:03<00:00, 3081.02it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 10001/10001 [00:03<00:00, 3058.15it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 10001/10001 [00:03<00:00, 2964.14it/s]\n",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      " 20%|████████████████▊                                                                   | 1/5 [00:04<00:18,  4.63s/it]C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:08<00:13,  4.47s/it]C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:13<00:08,  4.39s/it]C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:17<00:04,  4.45s/it]C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:22<00:00,  4.42s/it]\n",
      "C:\\Users\\isxzl\\AppData\\Local\\Temp\\ipykernel_2508\\357591954.py:127: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
      "  writer.save()\n"
     ]
    }
   ],
   "source": [
    "aaa=eval_everything(pdata, models=collate, device=global_config[\"device\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f9096-89ab-4926-8c60-cca73920c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed89fa6-f9b9-4af6-8531-da9c2729d020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a19fd-fa88-4c31-a829-e689313f04d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a881acc-f9b8-4f25-a5e9-1ab011d77b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01778bba-2d6b-4cf2-b94c-356887aac840",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
