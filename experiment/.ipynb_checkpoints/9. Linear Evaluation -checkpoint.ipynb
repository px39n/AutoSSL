{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f60f99c9-6b85-40f6-8c0f-a3934044da62",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af985175-ec09-4cf7-a2df-5bededfbb6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset=torchvision.datasets.CocoDetection(root=\"D:/Datasets/CoCo/Data\",annFile=\"D:/Datasets/CoCo/ann\", download = True)\n",
    "#train_dataset=torchvision.datasets.INaturalist(root=\"D:/Datasets/INaturalist\",version=\"2021_valid\", download = True)\n",
    "#train_dataset=torchvision.datasets.INaturalist(root=\"D:/Datasets/INaturalist\",version=\"2021_train\", download = True)\n",
    "#train_dataset=torchvision.datasets.VOCDetection(root=\"D:/Datasets/VOC07\",year=\"2007\", download = True)\n",
    "#train_dataset=torchvision.datasets.VOCDetection(root=\"D:/Datasets/VOC07-12\",year=\"train\", download = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pipe_loadckpt(ckpt):\n",
    "\n",
    "    checkpoint = torch.load(ckpt)\n",
    "    # Extract only backbone state_dict\n",
    "    state_dict_backbone = {k: v for k, v in checkpoint['state_dict'].items() if 'backbone' in k and 'momentum' not in k}\n",
    "    # Remove 'backbone.' prefix in state_dict keys\n",
    "    state_dict_backbone = {k.replace('backbone.', ''): v for k, v in state_dict_backbone.items()}\n",
    "    #state_dict_backbone.keys()\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        model_temp=pipe_backbone(\"resnet18_5layer\")[0]\n",
    "        model_temp.load_state_dict(state_dict_backbone)\n",
    "        model=model_temp\n",
    "        print(\"Successfully load resnet18_5layer\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        model_temp=pipe_backbone(\"resnet18_5layer_split8\")[0]\n",
    "        model_temp.load_state_dict(state_dict_backbone)\n",
    "        model=model_temp\n",
    "        print(\"Successfully load resnet18_5layer_split8\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        model_temp=pipe_backbone(\"resnet18\")[0]\n",
    "        model_temp.load_state_dict(state_dict_backbone)\n",
    "        model=model_temp\n",
    "        print(\"Successfully load resnet18\")\n",
    "    except:\n",
    "        #raise ValueError(\"Cannot detect the current ckpt\")\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        return model\n",
    "    except:\n",
    "        raise ValueError(\"Cannot detect the current ckpt\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def pipe_eval_linear(max_epochs,\n",
    "                     accelerator,\n",
    "                     devices,\n",
    "                     log_dir,\n",
    "                     model,\n",
    "                     num_classes,\n",
    "                     feature_dim,\n",
    "                     batch_size,\n",
    "                     check_val_every_n_epoch,\n",
    "                     train_dataloader,\n",
    "                     val_dataloader,\n",
    "                    ):\n",
    "    from pytorch_lightning import LightningModule, Trainer\n",
    "    # Train linear classifier.\n",
    "    metric_callback = MetricCallback()\n",
    "    trainer = Trainer(\n",
    "        max_epochs=15,\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "        callbacks=[\n",
    "            LearningRateMonitor(),\n",
    "            DeviceStatsMonitor(),\n",
    "            metric_callback,\n",
    "        ],\n",
    "        logger=TensorBoardLogger(save_dir=str(log_dir), name=\"linear_eval\"),\n",
    "        check_val_every_n_epoch =check_val_every_n_epoch,\n",
    "\n",
    "\n",
    "    )\n",
    "    classifier = LinearClassifier(\n",
    "        model=model,\n",
    "        batch_size_per_device=batch_size,\n",
    "        feature_dim=feature_dim,\n",
    "        num_classes=num_classes,\n",
    "        freeze_model=True,\n",
    "    )\n",
    "    trainer.fit(\n",
    "        model=classifier,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )\n",
    "    for metric in [\"val_top1\", \"val_top5\"]:\n",
    "        print(f\"max linear {metric}: {max(metric_callback.val_metrics[metric])}\")\n",
    "        \n",
    "    return max(metric_callback.val_metrics[\"val_top1\"]),max(metric_callback.val_metrics[\"val_top5\"])\n",
    "\n",
    "\n",
    "def pipe_eval_knn(\n",
    "                     accelerator,\n",
    "                     devices,\n",
    "                     log_dir,\n",
    "                     model,\n",
    "                     num_classes,\n",
    "                     feature_dim,\n",
    "                     batch_size,\n",
    "                     train_dataloader,\n",
    "                     val_dataloader,\n",
    "                    ):\n",
    "    from pytorch_lightning import LightningModule, Trainer\n",
    "    classifier = KNNClassifier(\n",
    "        model=model,\n",
    "        num_classes=num_classes,\n",
    "        feature_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    # Run KNN evaluation.\n",
    "    metric_callback = MetricCallback()\n",
    "    trainer = Trainer(\n",
    "        max_epochs=1,\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "        logger=TensorBoardLogger(save_dir=str(log_dir), name=\"knn_eval\"),\n",
    "        callbacks=[\n",
    "            DeviceStatsMonitor(),\n",
    "            metric_callback,\n",
    "        ],\n",
    "    )\n",
    "    trainer.fit(\n",
    "        model=classifier,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )\n",
    "    for metric in [\"val_top1\", \"val_top5\"]:\n",
    "        print(f\"knn {metric}: {max(metric_callback.val_metrics[metric])}\")\n",
    "    return max(metric_callback.val_metrics[\"val_top1\"]),max(metric_callback.val_metrics[\"val_top5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2089115a-30d4-41cc-b63a-8ddb49e832ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully load resnet18\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    " \n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import DeviceStatsMonitor, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.transforms.utils import IMAGENET_NORMALIZE\n",
    "from lightly.utils.benchmarking import LinearClassifier, MetricCallback,KNNClassifier\n",
    "import torchvision\n",
    "import sys \n",
    "import torch\n",
    "sys.path.append(\"C:/Users/isxzl/OneDrive/Code/AutoSSL\")\n",
    "from autoSSL.models import pipe_backbone\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "inatur_train_dir= r\"D:\\Datasets\\INaturalist\\2021_valid\\val\" # r\"D:\\Datasets\\VOC2007\\train\" #\"D:/Datasets/TinyImageNet/train/\"\n",
    "inatur_val_dir=  r\"D:\\Datasets\\INaturalist\\2021_valid\\val\"\n",
    "natur=[inatur_train_dir,inatur_val_dir, 10000]\n",
    "cifar_train_dir=r\"D:\\Datasets\\cifar10\\train\"\n",
    "cifar_test_dir=r\"D:\\Datasets\\cifar10\\test\"\n",
    "cifar=[cifar_train_dir,cifar_test_dir,10,32]\n",
    "\n",
    "log_dir=\"Linear_log\"\n",
    "batch_size= 256\n",
    "num_workers=6\n",
    "accelerator=\"cuda\"\n",
    "devices=1\n",
    "feature_dim=512\n",
    "max_epochs=5\n",
    "check_val_every_n_epoch=5\n",
    "# [\"BYOL\",\"BYOL_Mean\",\"SimCLR\",\"SimCLR\",\"SimCLR\",\"SimCLR\",\"SimCLR\",\"SimCLR\"]\n",
    "md_name=\"VICREG\"\n",
    "ckpt_dir=f\"C:/Users/isxzl/OneDrive/Code/AutoSSL/experiment/benchmark_logs/800epoch/{md_name}/checkpoints/epoch=799-step=77600.ckpt\"\n",
    "train_dir,val_dir,num_classes,input_size=cifar\n",
    "\n",
    "model=pipe_loadckpt(ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc81ff3-9a1a-4201-8904-188ab11858c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42e1b24d-31b9-4a8a-9330-82fd81c65ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully load resnet18_5layer_split8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "checkpoint = torch.load(ckpt_dir)\n",
    "checkpoint['state_dict'].keys()\n",
    "state_dict_backbone = {k: v for k, v in checkpoint['state_dict'].items() if 'backbone' in k and 'momentum' not in k}\n",
    "    # Remove 'backbone.' prefix in state_dict keys\n",
    "state_dict_backbone = {k.replace('backbone.', ''): v for k, v in state_dict_backbone.items()}\n",
    "\n",
    "\n",
    "model=pipe_backbone(\"resnet18_5layer\")[0]\n",
    "#print(checkpoint[\"state_dict\"].keys())\n",
    "len(checkpoint[\"state_dict\"][\"backbone.2.0.bn1.running_mean\"])\n",
    "model=pipe_loadckpt(ckpt_dir)\n",
    "#len(state_dict_backbone[\"2.0.bn1.running_mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5ada2-05de-4181-9131-e2311a39b2f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0391f783-c325-45ea-9b0c-57060ec3933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def pipe_eval_dataloader(input_size,batch_size,train_dir,val_dir):\n",
    "\n",
    "    train_transform = T.Compose(\n",
    "        [\n",
    "            T.Resize(input_size),\n",
    "            #T.RandomResizedCrop(input_size),\n",
    "            #T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=IMAGENET_NORMALIZE[\"mean\"], std=IMAGENET_NORMALIZE[\"std\"]),\n",
    "        ]\n",
    "    )\n",
    "    train_dataset = LightlyDataset(input_dir=train_dir, transform=train_transform)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "\n",
    "    # Setup validation data.\n",
    "    val_transform = T.Compose(\n",
    "        [\n",
    "            T.Resize(input_size), # This forsmall datasets\n",
    "            #T.RandomResizedCrop(input_size),\n",
    "            #T.CenterCrop(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=IMAGENET_NORMALIZE[\"mean\"], std=IMAGENET_NORMALIZE[\"std\"]),\n",
    "        ]\n",
    "    )\n",
    "    val_dataset = LightlyDataset(input_dir=str(val_dir), transform=val_transform)\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True,\n",
    "        shuffle=False, \n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "\n",
    "    return train_dataloader,val_dataloader\n",
    "trainl_loaders,val_loader=pipe_eval_dataloader(input_size,batch_size=256,train_dir=train_dir,val_dir=val_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "151a43e2-bea6-468d-af98-f6ed18bd8036",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=pipe_loadckpt(ckpt_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c3fb1-a9da-4423-a7a9-a602fa8abe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name                | Type             | Params\n",
      "---------------------------------------------------------\n",
      "0 | model               | Sequential       | 11.2 M\n",
      "1 | classification_head | Linear           | 5.1 K \n",
      "2 | criterion           | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.696    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  90%|██████████████████████████████████████▊    | 177/196 [00:06<00:00, 29.39it/s, v_num=14, train_loss=2.200]"
     ]
    }
   ],
   "source": [
    "result=pipe_eval_linear(max_epochs=max_epochs,\n",
    "                     accelerator=accelerator, \n",
    "                     devices=devices, \n",
    "                     log_dir=log_dir,\n",
    "                     model=model,\n",
    "                     num_classes=num_classes,\n",
    "                     feature_dim=feature_dim ,\n",
    "                     batch_size=batch_size,\n",
    "                     check_val_every_n_epoch=check_val_every_n_epoch,\n",
    "                     train_dataloader=trainl_loaders,\n",
    "                     val_dataloader=val_loader,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eebec5a4-97ec-4bb2-b211-5049ac0cecbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:171: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 11.2 M\n",
      "-------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.675    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   5%|███▏                                                          | 10/196 [00:02<00:42,  4.38it/s, v_num=24]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:129: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n",
      "  self.warning_cache.warn(\"`training_step` returned `None`. If this was on purpose, ignore this warning...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████| 196/196 [00:06<00:00, 28.30it/s, v_num=24]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|█▍                                                        | 1/40 [00:00<00:00, 50.00it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|██▉                                                       | 2/40 [00:00<00:00, 55.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|████▎                                                     | 3/40 [00:00<00:00, 61.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█████▊                                                    | 4/40 [00:00<00:00, 71.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|███████▎                                                  | 5/40 [00:00<00:00, 57.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|████████▋                                                 | 6/40 [00:00<00:00, 58.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|██████████▏                                               | 7/40 [00:00<00:00, 39.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███████████▌                                              | 8/40 [00:00<00:00, 39.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|█████████████                                             | 9/40 [00:00<00:00, 40.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██████████████▎                                          | 10/40 [00:00<00:00, 42.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|███████████████▋                                         | 11/40 [00:00<00:00, 42.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████████████████                                        | 12/40 [00:00<00:00, 43.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|██████████████████▌                                      | 13/40 [00:00<00:00, 40.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|███████████████████▉                                     | 14/40 [00:00<00:00, 41.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|█████████████████████▍                                   | 15/40 [00:00<00:00, 41.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████████████████████▊                                  | 16/40 [00:00<00:00, 41.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|████████████████████████▏                                | 17/40 [00:00<00:00, 41.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|█████████████████████████▋                               | 18/40 [00:00<00:00, 42.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|███████████████████████████                              | 19/40 [00:00<00:00, 39.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████████████████████████▌                            | 20/40 [00:00<00:00, 39.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████████████████████████▉                           | 21/40 [00:00<00:00, 39.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|███████████████████████████████▎                         | 22/40 [00:00<00:00, 39.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|████████████████████████████████▊                        | 23/40 [00:00<00:00, 39.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████████████████████████████▏                      | 24/40 [00:00<00:00, 40.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|███████████████████████████████████▋                     | 25/40 [00:00<00:00, 41.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|█████████████████████████████████████                    | 26/40 [00:00<00:00, 39.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████████████████████████████████▍                  | 27/40 [00:00<00:00, 40.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████████████████████████████████▉                 | 28/40 [00:00<00:00, 41.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|█████████████████████████████████████████▎               | 29/40 [00:00<00:00, 40.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|██████████████████████████████████████████▊              | 30/40 [00:00<00:00, 40.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████████████████████████████████████▏            | 31/40 [00:00<00:00, 41.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|█████████████████████████████████████████████▌           | 32/40 [00:00<00:00, 40.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|███████████████████████████████████████████████          | 33/40 [00:00<00:00, 41.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|████████████████████████████████████████████████▍        | 34/40 [00:00<00:00, 42.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|█████████████████████████████████████████████████▉       | 35/40 [00:00<00:00, 40.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|███████████████████████████████████████████████████▎     | 36/40 [00:00<00:00, 41.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|████████████████████████████████████████████████████▋    | 37/40 [00:00<00:00, 41.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|██████████████████████████████████████████████████████▏  | 38/40 [00:00<00:00, 41.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████████████████████████████████████████████▌ | 39/40 [00:00<00:00, 42.58it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████████████████████████| 196/196 [00:08<00:00, 24.48it/s, v_num=24, val_top1=0.906, val_top5=0.994]\u001b[A\n",
      "Epoch 0: 100%|█████████████████████████████| 196/196 [00:08<00:00, 24.48it/s, v_num=24, val_top1=0.906, val_top5=0.994]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████████████████████████| 196/196 [00:08<00:00, 24.21it/s, v_num=24, val_top1=0.906, val_top5=0.994]\n",
      "knn val_top1: 0.9060093760490417\n",
      "knn val_top5: 0.9939005970954895\n"
     ]
    }
   ],
   "source": [
    "result=pipe_eval_knn(\n",
    "                     accelerator=accelerator, \n",
    "                     devices=devices, \n",
    "                     log_dir=log_dir,\n",
    "                     model=model,\n",
    "                     num_classes=num_classes,\n",
    "                     feature_dim=feature_dim ,\n",
    "                     batch_size=batch_size,\n",
    "                     train_dataloader=trainl_loaders,\n",
    "                     val_dataloader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be53f3e-3f87-4bd9-8b17-89597b00d015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:171: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 11.2 M\n",
      "-------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.675    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████| 196/196 [00:05<00:00, 37.13it/s, v_num=25]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                               | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                  | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|█▍                                                        | 1/40 [00:00<00:02, 17.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|██▉                                                       | 2/40 [00:00<00:01, 28.99it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|████▎                                                     | 3/40 [00:00<00:01, 28.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█████▊                                                    | 4/40 [00:00<00:01, 28.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|███████▎                                                  | 5/40 [00:00<00:01, 32.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|████████▋                                                 | 6/40 [00:00<00:00, 37.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|██████████▏                                               | 7/40 [00:00<00:00, 37.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███████████▌                                              | 8/40 [00:00<00:00, 38.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|█████████████                                             | 9/40 [00:00<00:00, 39.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██████████████▎                                          | 10/40 [00:00<00:00, 39.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|███████████████▋                                         | 11/40 [00:00<00:00, 41.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████████████████                                        | 12/40 [00:00<00:00, 36.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|██████████████████▌                                      | 13/40 [00:00<00:00, 33.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|███████████████████▉                                     | 14/40 [00:00<00:00, 34.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|█████████████████████▍                                   | 15/40 [00:00<00:00, 35.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████████████████████▊                                  | 16/40 [00:00<00:00, 36.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|████████████████████████▏                                | 17/40 [00:00<00:00, 35.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|█████████████████████████▋                               | 18/40 [00:00<00:00, 35.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|███████████████████████████                              | 19/40 [00:00<00:00, 36.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████████████████████████▌                            | 20/40 [00:00<00:00, 36.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████████████████████████▉                           | 21/40 [00:00<00:00, 36.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|███████████████████████████████▎                         | 22/40 [00:00<00:00, 36.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|████████████████████████████████▊                        | 23/40 [00:00<00:00, 36.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████████████████████████████▏                      | 24/40 [00:00<00:00, 37.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|███████████████████████████████████▋                     | 25/40 [00:00<00:00, 38.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|█████████████████████████████████████                    | 26/40 [00:00<00:00, 37.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████████████████████████████████▍                  | 27/40 [00:00<00:00, 37.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████████████████████████████████▉                 | 28/40 [00:00<00:00, 37.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|█████████████████████████████████████████▎               | 29/40 [00:00<00:00, 38.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|██████████████████████████████████████████▊              | 30/40 [00:00<00:00, 39.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████████████████████████████████████▏            | 31/40 [00:00<00:00, 39.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|█████████████████████████████████████████████▌           | 32/40 [00:00<00:00, 38.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|███████████████████████████████████████████████          | 33/40 [00:00<00:00, 39.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|████████████████████████████████████████████████▍        | 34/40 [00:00<00:00, 40.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|█████████████████████████████████████████████████▉       | 35/40 [00:00<00:00, 41.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|███████████████████████████████████████████████████▎     | 36/40 [00:00<00:00, 41.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|████████████████████████████████████████████████████▋    | 37/40 [00:00<00:00, 41.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|██████████████████████████████████████████████████████▏  | 38/40 [00:00<00:00, 41.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████████████████████████████████████████████▌ | 39/40 [00:00<00:00, 41.89it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████████████████████████| 196/196 [00:06<00:00, 30.83it/s, v_num=25, val_top1=0.895, val_top5=0.995]\u001b[A\n",
      "Epoch 0: 100%|█████████████████████████████| 196/196 [00:06<00:00, 30.82it/s, v_num=25, val_top1=0.895, val_top5=0.995]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████████████████████████| 196/196 [00:06<00:00, 30.38it/s, v_num=25, val_top1=0.895, val_top5=0.995]\n",
      "knn val_top1: 0.8954104781150818\n",
      "knn val_top5: 0.9945005774497986\n"
     ]
    }
   ],
   "source": [
    "result=pipe_eval_knn(\n",
    "                     accelerator=accelerator, \n",
    "                     devices=devices, \n",
    "                     log_dir=log_dir,\n",
    "                     model=model,\n",
    "                     num_classes=num_classes,\n",
    "                     feature_dim=feature_dim ,\n",
    "                     batch_size=batch_size,\n",
    "                     train_dataloader=trainl_loaders,\n",
    "                     val_dataloader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e410670-4581-480a-a7ff-89a1570a50ac",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff4d47-e560-490f-8621-4d4a9d08795b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddc38413-ab89-4be3-b819-19cf46d2b0d8",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87129ce0-6f50-4d6d-8577-6069f6917418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import DeviceStatsMonitor, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.transforms.utils import IMAGENET_NORMALIZE\n",
    "from lightly.utils.benchmarking import LinearClassifier, MetricCallback\n",
    "from lightly.utils.scheduler import CosineWarmupScheduler\n",
    "\n",
    "\n",
    "class FinetuneLinearClassifier(LinearClassifier):\n",
    "    def configure_optimizers(self):\n",
    "        parameters = list(self.classification_head.parameters())\n",
    "        parameters += self.model.parameters()\n",
    "        optimizer = SGD(\n",
    "            parameters,\n",
    "            lr=0.05 * self.batch_size_per_device * self.trainer.world_size / 256,\n",
    "            momentum=0.9,\n",
    "            weight_decay=0.0,\n",
    "        )\n",
    "        scheduler = {\n",
    "            \"scheduler\": CosineWarmupScheduler(\n",
    "                optimizer=optimizer,\n",
    "                warmup_epochs=0,\n",
    "                max_epochs=self.trainer.estimated_stepping_batches,\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    " \n",
    "print(\"Running fine-tune evaluation...\")\n",
    "\n",
    "# Setup training data.\n",
    "train_transform = T.Compose(\n",
    "    [\n",
    "        T.RandomResizedCrop(input_size),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_NORMALIZE[\"mean\"], std=IMAGENET_NORMALIZE[\"std\"]),\n",
    "    ]\n",
    ")\n",
    "train_dataset = LightlyDataset(input_dir=str(train_dir), transform=train_transform)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size_per_device,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "# Setup validation data.\n",
    "val_transform = T.Compose(\n",
    "    [\n",
    "        T.Resize(input_size),\n",
    "        #T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_NORMALIZE[\"mean\"], std=IMAGENET_NORMALIZE[\"std\"]),\n",
    "    ]\n",
    ")\n",
    "val_dataset = LightlyDataset(input_dir=str(val_dir), transform=val_transform)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size_per_device,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "# Train linear classifier.\n",
    "metric_callback = MetricCallback()\n",
    "trainer = Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator=accelerator,\n",
    "    devices=devices,\n",
    "    callbacks=[\n",
    "        LearningRateMonitor(),\n",
    "        DeviceStatsMonitor(),\n",
    "        metric_callback,\n",
    "    ],\n",
    "    logger=TensorBoardLogger(save_dir=str(log_dir), name=\"finetune_eval\"),\n",
    "    check_val_every_n_epoch =5,\n",
    "\n",
    ")\n",
    "classifier = FinetuneLinearClassifier(\n",
    "    model=model,\n",
    "    batch_size_per_device=batch_size_per_device,\n",
    "    feature_dim=feature_dim,\n",
    "    num_classes=num_classes,\n",
    "    freeze_model=False,\n",
    ")\n",
    "trainer.fit(\n",
    "    model=classifier,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")\n",
    "for metric in [\"val_top1\", \"val_top5\"]:\n",
    "    print(f\"max finetune {metric}: {max(metric_callback.val_metrics[metric])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02530b06-7a77-419f-b543-a385584e4b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
