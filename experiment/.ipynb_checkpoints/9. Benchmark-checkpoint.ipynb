{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf10fc43-e497-407f-a5cc-1f7453293a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a7fe22c-777f-4b2a-846d-cc4c356fbdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Benchmark Results\n",
    "\n",
    "Updated: 27.03.2023 (42a6a924b1b6d5b6cc89a6b2a0a0942cc4af93ab)\n",
    "\n",
    "------------------------------------------------------------------------------------------\n",
    "| Model         | Batch Size | Epochs |  KNN Test Accuracy |       Time | Peak GPU Usage |\n",
    "------------------------------------------------------------------------------------------\n",
    "| BarlowTwins   |        128 |    200 |              0.842 |  375.9 Min |      1.7 GByte |\n",
    "| BYOL          |        128 |    200 |              0.869 |  121.9 Min |      1.6 GByte |\n",
    "| DCL           |        128 |    200 |              0.844 |  102.2 Min |      1.5 GByte |\n",
    "| DCLW          |        128 |    200 |              0.833 |  100.4 Min |      1.5 GByte |\n",
    "| DINO          |        128 |    200 |              0.840 |  120.3 Min |      1.6 GByte |\n",
    "| FastSiam      |        128 |    200 |              0.906 |  164.0 Min |      2.7 GByte |\n",
    "| Moco          |        128 |    200 |              0.838 |  128.8 Min |      1.7 GByte |\n",
    "| NNCLR         |        128 |    200 |              0.834 |  101.5 Min |      1.5 GByte |\n",
    "| SimCLR        |        128 |    200 |              0.847 |   97.7 Min |      1.5 GByte |\n",
    "| SimSiam       |        128 |    200 |              0.819 |   97.3 Min |      1.6 GByte |\n",
    "| SwaV          |        128 |    200 |              0.812 |   99.6 Min |      1.5 GByte |\n",
    "| SMoG          |        128 |    200 |              0.743 |  192.2 Min |      1.2 GByte |\n",
    "------------------------------------------------------------------------------------------\n",
    "| BarlowTwins   |        512 |    200 |              0.819 |  153.3 Min |      5.1 GByte |\n",
    "| BYOL          |        512 |    200 |              0.868 |  108.3 Min |      5.6 GByte |\n",
    "| DCL           |        512 |    200 |              0.840 |   88.2 Min |      4.9 GByte |\n",
    "| DCLW          |        512 |    200 |              0.824 |   87.9 Min |      4.9 GByte |\n",
    "| DINO          |        512 |    200 |              0.813 |  108.6 Min |      5.0 GByte |\n",
    "| FastSiam      |        512 |    200 |              0.788 |  146.9 Min |      9.5 GByte |\n",
    "| Moco (*)      |        512 |    200 |              0.847 |  112.2 Min |      5.6 GByte |\n",
    "| NNCLR (*)     |        512 |    200 |              0.815 |   88.1 Min |      5.0 GByte |\n",
    "| SimCLR        |        512 |    200 |              0.848 |   87.1 Min |      4.9 GByte |\n",
    "| SimSiam       |        512 |    200 |              0.764 |   87.8 Min |      5.0 GByte |\n",
    "| SwaV          |        512 |    200 |              0.842 |   88.7 Min |      4.9 GByte |\n",
    "| SMoG          |        512 |    200 |              0.686 |  110.0 Min |      3.4 GByte |\n",
    "------------------------------------------------------------------------------------------\n",
    "| BarlowTwins   |        512 |    800 |              0.859 |  517.5 Min |      7.9 GByte |\n",
    "| BYOL          |        512 |    800 |              0.910 |  400.9 Min |      5.4 GByte |\n",
    "| DCL           |        512 |    800 |              0.874 |  334.6 Min |      4.9 GByte |\n",
    "| DCLW          |        512 |    800 |              0.871 |  333.3 Min |      4.9 GByte |\n",
    "| DINO          |        512 |    800 |              0.848 |  405.2 Min |      5.0 GByte |\n",
    "| FastSiam      |        512 |    800 |              0.902 |  582.0 Min |      9.5 GByte |\n",
    "| Moco (*)      |        512 |    800 |              0.899 |  417.8 Min |      5.4 GByte |\n",
    "| NNCLR (*)     |        512 |    800 |              0.892 |  335.0 Min |      5.0 GByte |\n",
    "| SimCLR        |        512 |    800 |              0.879 |  331.1 Min |      4.9 GByte |\n",
    "| SimSiam       |        512 |    800 |              0.904 |  333.7 Min |      5.1 GByte |\n",
    "| SwaV          |        512 |    800 |              0.884 |  330.5 Min |      5.0 GByte |\n",
    "| SMoG          |        512 |    800 |              0.800 |  415.6 Min |      3.2 GByte |\n",
    "------------------------------------------------------------------------------------------\n",
    "\n",
    "(*): Increased size of memory bank from 4096 to 8192 to avoid too quickly \n",
    "changing memory bank due to larger batch size.\n",
    "\n",
    "The benchmarks were created on a single NVIDIA RTX A6000.\n",
    "\n",
    "Note that this benchmark also supports a multi-GPU setup. If you run it on\n",
    "a system with multiple GPUs make sure that you kill all the processes when\n",
    "killing the application. Due to the way we setup this benchmark the distributed\n",
    "processes might continue the benchmark if one of the nodes is killed.\n",
    "If you know how to fix this don't hesitate to create an issue or PR :)\n",
    "\n",
    "\"\"\"\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.loss import (\n",
    "    BarlowTwinsLoss,\n",
    "    DCLLoss,\n",
    "    DCLWLoss,\n",
    "    DINOLoss,\n",
    "    NegativeCosineSimilarity,\n",
    "    NTXentLoss,\n",
    "    SwaVLoss,\n",
    "    memory_bank,\n",
    ")\n",
    "from lightly.models import ResNetGenerator, modules, utils\n",
    "from lightly.models.modules import heads\n",
    "from lightly.transforms import (\n",
    "    DINOTransform,\n",
    "    FastSiamTransform,\n",
    "    SimCLRTransform,\n",
    "    SimSiamTransform,\n",
    "    SMoGTransform,\n",
    "    SwaVTransform,\n",
    ")\n",
    "from lightly.transforms.utils import IMAGENET_NORMALIZE\n",
    "from lightly.utils.benchmarking import BenchmarkModule\n",
    "\n",
    "logs_root_dir = os.path.join(os.getcwd(), \"benchmark_logs1\")\n",
    "\n",
    "# set max_epochs to 800 for long run (takes around 10h on a single V100)\n",
    "max_epochs = 800\n",
    "num_workers = 4\n",
    "knn_k = 200\n",
    "knn_t = 0.1\n",
    "classes = 100\n",
    "\n",
    "# Set to True to enable Distributed Data Parallel training.\n",
    "distributed = False\n",
    "\n",
    "# Set to True to enable Synchronized Batch Norm (requires distributed=True).\n",
    "# If enabled the batch norm is calculated over all gpus, otherwise the batch\n",
    "# norm is only calculated from samples on the same gpu.\n",
    "sync_batchnorm = False\n",
    "\n",
    "# Set to True to gather features from all gpus before calculating\n",
    "# the loss (requires distributed=True).\n",
    "# If enabled then the loss on every gpu is calculated with features from all\n",
    "# gpus, otherwise only features from the same gpu are used.\n",
    "gather_distributed = False\n",
    "\n",
    "# benchmark\n",
    "n_runs = 1  # optional, increase to create multiple runs and report mean + std\n",
    "batch_size = 512\n",
    "lr_factor = batch_size / 128  # scales the learning rate linearly with batch size\n",
    "\n",
    "# Number of devices and hardware to use for training.\n",
    "devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "path_to_train = \"D:/Datasets/Cifar100/train/\"\n",
    "path_to_test = \"D:/Datasets/Cifar100/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b82f2c03-99b1-430a-a5d3-8bb268af924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if distributed:\n",
    "    strategy = \"ddp\"\n",
    "    # reduce batch size for distributed training\n",
    "    batch_size = batch_size // devices\n",
    "else:\n",
    "    strategy = \"auto\"  # Set to \"auto\" if using PyTorch Lightning >= 2.0\n",
    "    # limit to single device if not using distributed training\n",
    "    devices = min(devices, 1)\n",
    "\n",
    "# Adapted from our MoCo Tutorial on CIFAR-10\n",
    "#\n",
    "# Replace the path with the location of your CIFAR-10 dataset.\n",
    "# We assume we have a train folder with subfolders\n",
    "# for each class and .png images inside.\n",
    "#\n",
    "# You can download `CIFAR-10 in folders from kaggle\n",
    "# <https://www.kaggle.com/swaroopkml/cifar10-pngs-in-folders>`_.\n",
    "\n",
    "# The dataset structure should be like this:\n",
    "# cifar10/train/\n",
    "#  L airplane/\n",
    "#    L 10008_airplane.png\n",
    "#    L ...\n",
    "#  L automobile/\n",
    "#  L bird/\n",
    "#  L cat/\n",
    "#  L deer/\n",
    "#  L dog/\n",
    "#  L frog/\n",
    "#  L horse/\n",
    "#  L ship/\n",
    "#  L truck/\n",
    "\n",
    "\n",
    "# Use SimCLR augmentations\n",
    "simclr_transform = SimCLRTransform(\n",
    "    input_size=32,\n",
    "    cj_strength=0.5,\n",
    "    gaussian_blur=0.0,\n",
    ")\n",
    "\n",
    "# Use SimSiam augmentations\n",
    "simsiam_transform = SimSiamTransform(\n",
    "    input_size=32,\n",
    "    gaussian_blur=0.0,\n",
    ")\n",
    "\n",
    "# Multi crop augmentation for FastSiam\n",
    "fast_siam_transform = FastSiamTransform(input_size=32, gaussian_blur=0.0)\n",
    "\n",
    "# Multi crop augmentation for SwAV, additionally, disable blur for cifar10\n",
    "swav_transform = SwaVTransform(\n",
    "    crop_sizes=[32],\n",
    "    crop_counts=[2],  # 2 crops @ 32x32px\n",
    "    crop_min_scales=[0.14],\n",
    "    cj_strength=0.5,\n",
    "    gaussian_blur=0,\n",
    ")\n",
    "\n",
    "# Multi crop augmentation for DINO, additionally, disable blur for cifar10\n",
    "dino_transform = DINOTransform(\n",
    "    global_crop_size=32,\n",
    "    n_local_views=0,\n",
    "    cj_strength=0.5,\n",
    "    gaussian_blur=(0, 0, 0),\n",
    ")\n",
    "\n",
    "# Two crops for SMoG\n",
    "smog_transform = SMoGTransform(\n",
    "    crop_sizes=(32, 32),\n",
    "    crop_counts=(1, 1),\n",
    "    cj_strength=0.5,\n",
    "    gaussian_blur_probs=(0.0, 0.0),\n",
    "    crop_min_scales=(0.2, 0.2),\n",
    "    crop_max_scales=(1.0, 1.0),\n",
    ")\n",
    "\n",
    "# No additional augmentations for the test set\n",
    "test_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=IMAGENET_NORMALIZE[\"mean\"],\n",
    "            std=IMAGENET_NORMALIZE[\"std\"],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# we use test transformations for getting the feature for kNN on train data\n",
    "dataset_train_kNN = LightlyDataset(input_dir=path_to_train, transform=test_transforms)\n",
    "\n",
    "dataset_test = LightlyDataset(input_dir=path_to_test, transform=test_transforms)\n",
    "\n",
    "from lightly.transforms.multi_view_transform import MultiViewTransform\n",
    "from lightly.transforms import SimCLRViewTransform   ,SimSiamViewTransform\n",
    "\n",
    "sv = SimCLRViewTransform(\n",
    "    input_size=32,\n",
    "    cj_strength=0.5,\n",
    "    gaussian_blur=0.0,\n",
    ")\n",
    "st = SimSiamViewTransform(\n",
    "    input_size=32,\n",
    "    gaussian_blur=0.0,\n",
    ")\n",
    "\n",
    "def create_dataset_train_ssl(model):\n",
    "    \"\"\"Helper method to apply the correct transform for ssl.\n",
    "\n",
    "    Args:\n",
    "        model:\n",
    "            Model class for which to select the transform.\n",
    "    \"\"\"\n",
    "    model_to_transform = {\n",
    "        BarlowTwinsModel: simclr_transform,\n",
    "        BYOLModel: MultiViewTransform([sv for _ in range(2)]),#MultiViewTransform([st for _ in range(2)]),\n",
    "        BYOLModel_Mean: MultiViewTransform([sv for _ in range(4)]),\n",
    "        DCL: simclr_transform,\n",
    "        DCLW: simclr_transform,\n",
    "        DINOModel: dino_transform,\n",
    "        FastSiamModel: fast_siam_transform,\n",
    "        Mean_Model: fast_siam_transform,\n",
    "        STD_Model:fast_siam_transform,\n",
    "        MocoModel: simclr_transform,\n",
    "        NNCLRModel: simclr_transform,\n",
    "        SimCLRModel: simclr_transform,\n",
    "        SimSiamModel: simsiam_transform,\n",
    "        SwaVModel: swav_transform,\n",
    "        SMoGModel: smog_transform,\n",
    "    }\n",
    "    transform = model_to_transform[model]\n",
    "    return LightlyDataset(input_dir=path_to_train, transform=transform)\n",
    "\n",
    "\n",
    "def get_data_loaders(batch_size: int, dataset_train_ssl):\n",
    "    \"\"\"Helper method to create dataloaders for ssl, kNN train and kNN test.\n",
    "\n",
    "    Args:\n",
    "        batch_size: Desired batch size for all dataloaders.\n",
    "    \"\"\"\n",
    "    dataloader_train_ssl = torch.utils.data.DataLoader(\n",
    "        dataset_train_ssl,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    dataloader_train_kNN = torch.utils.data.DataLoader(\n",
    "        dataset_train_kNN,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    dataloader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return dataloader_train_ssl, dataloader_train_kNN, dataloader_test\n",
    "\n",
    "\n",
    "class MocoModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        num_splits = 0 if sync_batchnorm else 8\n",
    "        resnet = ResNetGenerator(\"resnet-18\", num_splits=num_splits)\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        # create a moco model based on ResNet\n",
    "        self.projection_head = heads.MoCoProjectionHead(512, 512, 128)\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        # create our loss with the optional memory bank\n",
    "        self.criterion = NTXentLoss(\n",
    "            temperature=0.1,\n",
    "            memory_bank_size=4096,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        return self.projection_head(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1), _, _ = batch\n",
    "\n",
    "        # update momentum\n",
    "        utils.update_momentum(self.backbone, self.backbone_momentum, 0.99)\n",
    "        utils.update_momentum(self.projection_head, self.projection_head_momentum, 0.99)\n",
    "\n",
    "        def step(x0_, x1_):\n",
    "            x1_, shuffle = utils.batch_shuffle(x1_, distributed=distributed)\n",
    "            x0_ = self.backbone(x0_).flatten(start_dim=1)\n",
    "            x0_ = self.projection_head(x0_)\n",
    "\n",
    "            x1_ = self.backbone_momentum(x1_).flatten(start_dim=1)\n",
    "            x1_ = self.projection_head_momentum(x1_)\n",
    "            x1_ = utils.batch_unshuffle(x1_, shuffle, distributed=distributed)\n",
    "            return x0_, x1_\n",
    "\n",
    "        # We use a symmetric loss (model trains faster at little compute overhead)\n",
    "        # https://colab.research.google.com/github/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb\n",
    "        loss_1 = self.criterion(*step(x0, x1))\n",
    "        loss_2 = self.criterion(*step(x1, x0))\n",
    "\n",
    "        loss = 0.5 * (loss_1 + loss_2)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = list(self.backbone.parameters()) + list(\n",
    "            self.projection_head.parameters()\n",
    "        )\n",
    "        optim = torch.optim.SGD(\n",
    "            params,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class SimCLRModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = ResNetGenerator(\"resnet-18\")\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.projection_head = heads.SimCLRProjectionHead(512, 512, 128)\n",
    "        self.criterion = NTXentLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2 * lr_factor, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class SimSiamModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = ResNetGenerator(\"resnet-18\")\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.prediction_head = heads.SimSiamPredictionHead(2048, 512, 2048)\n",
    "        # use a 2-layer projection head for cifar10 as described in the paper\n",
    "        self.projection_head = heads.ProjectionHead(\n",
    "            [\n",
    "                (512, 2048, nn.BatchNorm1d(2048), nn.ReLU(inplace=True)),\n",
    "                (2048, 2048, nn.BatchNorm1d(2048), None),\n",
    "            ]\n",
    "        )\n",
    "        self.criterion = NegativeCosineSimilarity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(f)\n",
    "        p = self.prediction_head(z)\n",
    "        z = z.detach() \n",
    "        return z, p\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        \n",
    "        (x0, x1), _, _ = batch\n",
    "        z0, p0 = self.forward(x0)\n",
    "        z1, p1 = self.forward(x1)\n",
    "        loss = 0.5 * (self.criterion(z0, p1) + self.criterion(z1, p0))\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=6e-2,  # no lr-scaling, results in better training stability\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class FastSiamModel(SimSiamModel):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        views, _, _ = batch\n",
    "        features = [self.forward(view) for view in views]\n",
    "        zs = torch.stack([z for z, _ in features])\n",
    "        ps = torch.stack([p for _, p in features])\n",
    "\n",
    "        variance=0\n",
    "        mean=torch.mean(zs, dim=0)\n",
    "        for i in range(len(views)):\n",
    "            distance=torch.exp(2* self.criterion(ps[i], mean))\n",
    "            variance += distance / (len(views))  \n",
    "        loss=torch.sqrt(variance)     \n",
    "        \n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "    \n",
    "class Mean_Model(SimSiamModel):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        views, _, _ = batch\n",
    "        features = [self.forward(view) for view in views]\n",
    "        zs = torch.stack([z for z, _ in features])\n",
    "        ps = torch.stack([p for _, p in features])\n",
    "\n",
    "        variance=0\n",
    "        mean=torch.mean(zs, dim=0)\n",
    "        for i in range(len(views)):\n",
    "            distance=self.criterion(ps[i], mean)\n",
    "            variance += distance / (len(views))  \n",
    "        loss=variance    \n",
    "        \n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "    \n",
    "class STD_Model(SimSiamModel):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        views, _, _ = batch\n",
    "        features = [self.forward(view) for view in views]\n",
    "        zs = torch.stack([z for z, _ in features])\n",
    "        ps = torch.stack([p for _, p in features])\n",
    "\n",
    "        variance=0\n",
    "        mean=torch.mean(zs, dim=0)\n",
    "        for i in range(len(views)):\n",
    "            distance=torch.exp(2* self.criterion(ps[i], mean))\n",
    "            variance += distance / (len(views))  \n",
    "        loss=torch.sqrt(variance) \n",
    "        \n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "    \n",
    "class BarlowTwinsModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = ResNetGenerator(\"resnet-18\")\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        # use a 2-layer projection head for cifar10 as described in the paper\n",
    "        self.projection_head = heads.ProjectionHead(\n",
    "            [\n",
    "                (512, 2048, nn.BatchNorm1d(2048), nn.ReLU(inplace=True)),\n",
    "                (2048, 2048, None, None),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.criterion = BarlowTwinsLoss(gather_distributed=gather_distributed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2 * lr_factor, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    " \n",
    "\n",
    "class BYOLModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = ResNetGenerator(\"resnet-18\")\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        # create a byol model based on ResNet\n",
    "        self.projection_head = heads.BYOLProjectionHead(512, 1024, 256)\n",
    "        self.prediction_head = heads.BYOLPredictionHead(256, 1024, 256)\n",
    "\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        self.criterion = NegativeCosineSimilarity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(y)\n",
    "        p = self.prediction_head(z)\n",
    "        return p\n",
    "\n",
    "    def forward_momentum(self, x):\n",
    "        y = self.backbone_momentum(x).flatten(start_dim=1)\n",
    "        z = self.projection_head_momentum(y)\n",
    "        z = z.detach()\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.backbone_momentum, m=0.99)\n",
    "        utils.update_momentum(\n",
    "            self.projection_head, self.projection_head_momentum, m=0.99\n",
    "        )\n",
    "        (x0, x1), _, _ = batch\n",
    "        p0 = self.forward(x0)\n",
    "        z0 = self.forward_momentum(x0)\n",
    "        p1 = self.forward(x1)\n",
    "        z1 = self.forward_momentum(x1)\n",
    "        loss = 0.5 * (self.criterion(p0, z1) + self.criterion(p1, z0))\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = (\n",
    "            list(self.backbone.parameters())\n",
    "            + list(self.projection_head.parameters())\n",
    "            + list(self.prediction_head.parameters())\n",
    "        )\n",
    "        optim = torch.optim.SGD(\n",
    "            params,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "class BYOLModel_Mean(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = ResNetGenerator(\"resnet-18\")\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        # create a byol model based on ResNet\n",
    "        self.projection_head = heads.BYOLProjectionHead(512, 1024, 256)\n",
    "        self.prediction_head = heads.BYOLPredictionHead(256, 1024, 256)\n",
    "\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        self.criterion = NegativeCosineSimilarity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(y)\n",
    "        p = self.prediction_head(z)\n",
    "        \n",
    "        y1 = self.backbone_momentum(x).flatten(start_dim=1)\n",
    "        z = self.projection_head_momentum(y1)\n",
    "        z = z.detach()\n",
    "        return p,z\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.backbone_momentum, m=0.99)\n",
    "        utils.update_momentum(\n",
    "            self.projection_head, self.projection_head_momentum, m=0.99\n",
    "        )\n",
    "        views, _, _ = batch\n",
    "        features = [self.forward(view) for view in views]   # features= [...,batch, feature for view_i]\n",
    "        zs = torch.stack([z for _,z  in features])     # zs= [...,embedding of batch, feature for view_i]\n",
    "        ps = torch.stack([p for p,_   in features])     # zs= [...,projection of batch, feature for view_i]\n",
    "        loss=0\n",
    "        mean_embed=torch.mean(zs, dim=0)\n",
    "        for i in range(len(views)):\n",
    "            loss += self.criterion(ps[i], mean_embed) / (len(views))    \n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        params = (\n",
    "            list(self.backbone.parameters())\n",
    "            + list(self.projection_head.parameters())\n",
    "            + list(self.prediction_head.parameters())\n",
    "        )\n",
    "        optim = torch.optim.SGD(\n",
    "            params,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "class SwaVModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = ResNetGenerator(\"resnet-18\")\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        self.projection_head = heads.SwaVProjectionHead(512, 512, 128)\n",
    "        self.prototypes = heads.SwaVPrototypes(128, 512)  # use 512 prototypes\n",
    "\n",
    "        self.criterion = SwaVLoss(sinkhorn_gather_distributed=gather_distributed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        x = self.projection_head(x)\n",
    "        x = nn.functional.normalize(x, dim=1, p=2)\n",
    "        return self.prototypes(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # normalize the prototypes so they are on the unit sphere\n",
    "        self.prototypes.normalize()\n",
    "\n",
    "        # the multi-crop dataloader returns a list of image crops where the\n",
    "        # first two items are the high resolution crops and the rest are low\n",
    "        # resolution crops\n",
    "        multi_crops, _, _ = batch\n",
    "        multi_crop_features = [self.forward(x) for x in multi_crops]\n",
    "\n",
    "        # split list of crop features into high and low resolution\n",
    "        high_resolution_features = multi_crop_features[:2]\n",
    "        low_resolution_features = multi_crop_features[2:]\n",
    "\n",
    "        # calculate the SwaV loss\n",
    "        loss = self.criterion(high_resolution_features, low_resolution_features)\n",
    "\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=1e-3 * lr_factor,\n",
    "            weight_decay=1e-6,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class NNCLRModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = ResNetGenerator(\"resnet-18\")\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.prediction_head = heads.NNCLRPredictionHead(256, 4096, 256)\n",
    "        # use only a 2-layer projection head for cifar10\n",
    "        self.projection_head = heads.ProjectionHead(\n",
    "            [\n",
    "                (512, 2048, nn.BatchNorm1d(2048), nn.ReLU(inplace=True)),\n",
    "                (2048, 256, nn.BatchNorm1d(256), None),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.criterion = NTXentLoss()\n",
    "        self.memory_bank = modules.NNMemoryBankModule(size=4096)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(y)\n",
    "        p = self.prediction_head(z)\n",
    "        z = z.detach()\n",
    "        return z, p\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0, p0 = self.forward(x0)\n",
    "        z1, p1 = self.forward(x1)\n",
    "        z0 = self.memory_bank(z0, update=False)\n",
    "        z1 = self.memory_bank(z1, update=True)\n",
    "        loss = 0.5 * (self.criterion(z0, p1) + self.criterion(z1, p0))\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class DINOModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = ResNetGenerator(\"resnet-18\")\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.head = self._build_projection_head()\n",
    "        self.teacher_backbone = copy.deepcopy(self.backbone)\n",
    "        self.teacher_head = self._build_projection_head()\n",
    "\n",
    "        utils.deactivate_requires_grad(self.teacher_backbone)\n",
    "        utils.deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "        self.criterion = DINOLoss(output_dim=2048)\n",
    "\n",
    "    def _build_projection_head(self):\n",
    "        head = heads.DINOProjectionHead(512, 2048, 256, 2048, batch_norm=True)\n",
    "        # use only 2 layers for cifar10\n",
    "        head.layers = heads.ProjectionHead(\n",
    "            [\n",
    "                (512, 2048, nn.BatchNorm1d(2048), nn.GELU()),\n",
    "                (2048, 256, None, None),\n",
    "            ]\n",
    "        ).layers\n",
    "        return head\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.teacher_backbone, m=0.99)\n",
    "        utils.update_momentum(self.head, self.teacher_head, m=0.99)\n",
    "        views, _, _ = batch\n",
    "        views = [view.to(self.device) for view in views]\n",
    "        global_views = views[:2]\n",
    "        teacher_out = [self.forward_teacher(view) for view in global_views]\n",
    "        student_out = [self.forward(view) for view in views]\n",
    "        loss = self.criterion(teacher_out, student_out, epoch=self.current_epoch)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param = list(self.backbone.parameters()) + list(self.head.parameters())\n",
    "        optim = torch.optim.SGD(\n",
    "            param,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class DCL(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = ResNetGenerator(\"resnet-18\")\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.projection_head = heads.SimCLRProjectionHead(512, 512, 128)\n",
    "        self.criterion = DCLLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2 * lr_factor, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class DCLW(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = ResNetGenerator(\"resnet-18\")\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.projection_head = heads.SimCLRProjectionHead(512, 512, 128)\n",
    "        self.criterion = DCLWLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2 * lr_factor, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "class SMoGModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = ResNetGenerator(\"resnet-18\")\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        # create a model based on ResNet\n",
    "        self.projection_head = heads.SMoGProjectionHead(512, 2048, 128)\n",
    "        self.prediction_head = heads.SMoGPredictionHead(128, 2048, 128)\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        # smog\n",
    "        self.n_groups = 300\n",
    "        memory_bank_size = 10000\n",
    "        self.memory_bank = memory_bank.MemoryBankModule(size=memory_bank_size)\n",
    "        # create our loss\n",
    "        group_features = torch.nn.functional.normalize(\n",
    "            torch.rand(self.n_groups, 128), dim=1\n",
    "        )\n",
    "        self.smog = heads.SMoGPrototypes(group_features=group_features, beta=0.99)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def _cluster_features(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        features = features.cpu().numpy()\n",
    "        kmeans = KMeans(self.n_groups).fit(features)\n",
    "        clustered = torch.from_numpy(kmeans.cluster_centers_).float()\n",
    "        clustered = torch.nn.functional.normalize(clustered, dim=1)\n",
    "        return clustered\n",
    "\n",
    "    def _reset_group_features(self):\n",
    "        # see https://arxiv.org/pdf/2207.06167.pdf Table 7b)\n",
    "        features = self.memory_bank.bank\n",
    "        group_features = self._cluster_features(features.t())\n",
    "        self.smog.set_group_features(group_features)\n",
    "\n",
    "    def _reset_momentum_weights(self):\n",
    "        # see https://arxiv.org/pdf/2207.06167.pdf Table 7b)\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        if self.global_step > 0 and self.global_step % 300 == 0:\n",
    "            # reset group features and weights every 300 iterations\n",
    "            self._reset_group_features()\n",
    "            self._reset_momentum_weights()\n",
    "        else:\n",
    "            # update momentum\n",
    "            utils.update_momentum(self.backbone, self.backbone_momentum, 0.99)\n",
    "            utils.update_momentum(\n",
    "                self.projection_head, self.projection_head_momentum, 0.99\n",
    "            )\n",
    "\n",
    "        (x0, x1), _, _ = batch\n",
    "\n",
    "        if batch_idx % 2:\n",
    "            # swap batches every second iteration\n",
    "            x0, x1 = x1, x0\n",
    "\n",
    "        x0_features = self.backbone(x0).flatten(start_dim=1)\n",
    "        x0_encoded = self.projection_head(x0_features)\n",
    "        x0_predicted = self.prediction_head(x0_encoded)\n",
    "        x1_features = self.backbone_momentum(x1).flatten(start_dim=1)\n",
    "        x1_encoded = self.projection_head_momentum(x1_features)\n",
    "\n",
    "        # update group features and get group assignments\n",
    "        assignments = self.smog.assign_groups(x1_encoded)\n",
    "        group_features = self.smog.get_updated_group_features(x0_encoded)\n",
    "        logits = self.smog(x0_predicted, group_features, temperature=0.1)\n",
    "        self.smog.set_group_features(group_features)\n",
    "\n",
    "        loss = self.criterion(logits, assignments)\n",
    "\n",
    "        # use memory bank to periodically reset the group features with k-means\n",
    "        self.memory_bank(x0_encoded, update=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = (\n",
    "            list(self.backbone.parameters())\n",
    "            + list(self.projection_head.parameters())\n",
    "            + list(self.prediction_head.parameters())\n",
    "        )\n",
    "        optim = torch.optim.SGD(\n",
    "            params,\n",
    "            lr=0.01,\n",
    "            momentum=0.9,\n",
    "            weight_decay=1e-6,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364e9016-5b69-423b-a491-155b3c0e7cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                     | Type                     | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | backbone                 | Sequential               | 11.2 M\n",
      "1 | projection_head          | BYOLProjectionHead       | 788 K \n",
      "2 | prediction_head          | BYOLPredictionHead       | 526 K \n",
      "3 | backbone_momentum        | Sequential               | 11.2 M\n",
      "4 | projection_head_momentum | BYOLProjectionHead       | 788 K \n",
      "5 | criterion                | NegativeCosineSimilarity | 0     \n",
      "----------------------------------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "12.0 M    Non-trainable params\n",
      "24.4 M    Total params\n",
      "97.767    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|                                                              | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:42\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:570\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    564\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    566\u001b[0m     ckpt_path,\n\u001b[0;32m    567\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    568\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    569\u001b[0m )\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:975\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1016\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1016\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1045\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1045\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1047\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:375\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[0;32m    374\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 375\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:287\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 287\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:379\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, ValidationStep)\n\u001b[1;32m--> 379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\lightly\\utils\\benchmarking\\benchmark_module.py:147\u001b[0m, in \u001b[0;36mBenchmarkModule.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    145\u001b[0m     targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(dist\u001b[38;5;241m.\u001b[39mgather(targets), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_val_predicted_labels\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpredicted_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_val_targets\u001b[38;5;241m.\u001b[39mappend(targets\u001b[38;5;241m.\u001b[39mcpu())\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 60\u001b[0m\n\u001b[0;32m     48\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[0;32m     49\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mmax_epochs,\n\u001b[0;32m     50\u001b[0m     devices\u001b[38;5;241m=\u001b[39mdevices,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     check_val_every_n_epoch \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     58\u001b[0m )\n\u001b[0;32m     59\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 60\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_train_ssl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     66\u001b[0m run \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_name,\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m     74\u001b[0m }\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:531\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    529\u001b[0m model \u001b[38;5;241m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 531\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:66\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m logger \u001b[38;5;129;01min\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mloggers:\n\u001b[0;32m     65\u001b[0m     logger\u001b[38;5;241m.\u001b[39mfinalize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_teardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# teardown might access the stage so we reset it after\u001b[39;00m\n\u001b[0;32m     68\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:998\u001b[0m, in \u001b[0;36mTrainer._teardown\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_teardown\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    996\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and\u001b[39;00m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;124;03m    Callback; those are handled by :meth:`_call_teardown_hook`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 998\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mteardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    999\u001b[0m     loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_active_loop\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;66;03m# loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:476\u001b[0m, in \u001b[0;36mStrategy.teardown\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    475\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: moving model to CPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 476\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mteardown()\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\lightning_fabric\\utilities\\device_dtype_mixin.py:78\u001b[0m, in \u001b[0;36m_DeviceDtypeModuleMixin.cpu\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See :meth:`torch.nn.Module.cpu`.\"\"\"\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__update_properties(device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\torch\\nn\\modules\\module.py:954\u001b[0m, in \u001b[0;36mModule.cpu\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcpu\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    946\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[0;32m    947\u001b[0m \n\u001b[0;32m    948\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\torch\\nn\\modules\\module.py:954\u001b[0m, in \u001b[0;36mModule.cpu.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcpu\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    946\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[0;32m    947\u001b[0m \n\u001b[0;32m    948\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = [\n",
    "     #STD_Model\n",
    "     #BYOLModel_Mean\n",
    "     #BYOLModel_Mean,   \n",
    "     BYOLModel,\n",
    "    # FastSiamModel,\n",
    "     #NBarlowTwinsModel,\n",
    "#    DCL,\n",
    "#    DCLW,\n",
    "#    DINOModel,\n",
    "#    MocoModel,\n",
    "#    NNCLRModel,\n",
    "#    SimCLRModel,\n",
    "#    SimSiamModel,\n",
    "#    SwaVModel,\n",
    "#    SMoGModel,\n",
    "]\n",
    "bench_results = dict()\n",
    "\n",
    "experiment_version = None\n",
    "# loop through configurations and train models\n",
    "for BenchmarkModel in models:\n",
    "    runs = []\n",
    "    model_name = BenchmarkModel.__name__.replace(\"Model\", \"\")\n",
    "    for seed in range(n_runs):\n",
    "        pl.seed_everything(seed)\n",
    "        dataset_train_ssl = create_dataset_train_ssl(BenchmarkModel)\n",
    "        dataloader_train_ssl, dataloader_train_kNN, dataloader_test = get_data_loaders(\n",
    "            batch_size=batch_size, dataset_train_ssl=dataset_train_ssl\n",
    "        )\n",
    "        benchmark_model = BenchmarkModel(dataloader_train_kNN, classes)\n",
    "\n",
    "        # Save logs to: {CWD}/benchmark_logs/cifar10/{experiment_version}/{model_name}/\n",
    "        # If multiple runs are specified a subdirectory for each run is created.\n",
    "        sub_dir = model_name if n_runs <= 1 else f\"{model_name}/run{seed}\"\n",
    "        logger = TensorBoardLogger(\n",
    "            save_dir=os.path.join(logs_root_dir, \"cifar10\"),\n",
    "            name=\"\",\n",
    "            sub_dir=sub_dir,\n",
    "            version=experiment_version,\n",
    "        )\n",
    "        if experiment_version is None:\n",
    "            # Save results of all models under same version directory\n",
    "            experiment_version = logger.version\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=os.path.join(logger.log_dir, \"checkpoints\")\n",
    "        )\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=max_epochs,\n",
    "            devices=devices,\n",
    "            accelerator=accelerator,\n",
    "            default_root_dir=logs_root_dir,\n",
    "            strategy=strategy,\n",
    "            sync_batchnorm=sync_batchnorm,\n",
    "            logger=logger,\n",
    "            callbacks=[checkpoint_callback],\n",
    "            check_val_every_n_epoch =5,\n",
    "        )\n",
    "        start = time.time()\n",
    "        trainer.fit(\n",
    "            benchmark_model,\n",
    "            train_dataloaders=dataloader_train_ssl,\n",
    "            val_dataloaders=dataloader_test,\n",
    "        )\n",
    "        end = time.time()\n",
    "        run = {\n",
    "            \"model\": model_name,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"epochs\": max_epochs,\n",
    "            \"max_accuracy\": benchmark_model.max_accuracy,\n",
    "            \"runtime\": end - start,\n",
    "            \"gpu_memory_usage\": torch.cuda.max_memory_allocated(),\n",
    "            \"seed\": seed,\n",
    "        }\n",
    "        runs.append(run)\n",
    "        print(run)\n",
    "\n",
    "        # delete model and trainer + free up cuda memory\n",
    "        del benchmark_model\n",
    "        del trainer\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    bench_results[model_name] = runs\n",
    "\n",
    "# print results table\n",
    "header = (\n",
    "    f\"| {'Model':<13} | {'Batch Size':>10} | {'Epochs':>6} \"\n",
    "    f\"| {'KNN Test Accuracy':>18} | {'Time':>10} | {'Peak GPU Usage':>14} |\"\n",
    ")\n",
    "print(\"-\" * len(header))\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for model, results in bench_results.items():\n",
    "    runtime = np.array([result[\"runtime\"] for result in results])\n",
    "    runtime = runtime.mean() / 60  # convert to min\n",
    "    accuracy = np.array([result[\"max_accuracy\"] for result in results])\n",
    "    gpu_memory_usage = np.array([result[\"gpu_memory_usage\"] for result in results])\n",
    "    gpu_memory_usage = gpu_memory_usage.max() / (1024**3)  # convert to gbyte\n",
    "\n",
    "    if len(accuracy) > 1:\n",
    "        accuracy_msg = f\"{accuracy.mean():>8.3f} +- {accuracy.std():>4.3f}\"\n",
    "    else:\n",
    "        accuracy_msg = f\"{accuracy.mean():>18.3f}\"\n",
    "\n",
    "    print(\n",
    "        f\"| {model:<13} | {batch_size:>10} | {max_epochs:>6} \"\n",
    "        f\"| {accuracy_msg} | {runtime:>6.1f} Min \"\n",
    "        f\"| {gpu_memory_usage:>8.1f} GByte |\",\n",
    "        flush=True,\n",
    "    )\n",
    "print(\"-\" * len(header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6995d-8129-4fd3-b5e4-82406ce72d80",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9111fb3-5991-482f-95d6-60513f73d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir benchmark_logs/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
