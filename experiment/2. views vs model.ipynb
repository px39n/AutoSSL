{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4533ee81-d579-4867-b15b-aa7f3f8f385f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiment Introduction\n",
    "\n",
    "Background, \n",
    "In order to do ablation experiment, we have diffculty like\n",
    "1. most of them are same, we have to repeat many times.\n",
    "2. too many model, config, code hard to manage and save. too messy\n",
    "\n",
    "\n",
    "For all kind of SSL training workflow, we have to define the hyperparameters includes 4 aspect,\n",
    "Dataset\n",
    "Model\n",
    "Training\n",
    "Saving COnfig\n",
    "\n",
    "\n",
    "\n",
    "How to Use this Experiment?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff5e60f-6c7d-47b6-8b30-5ad2e6c94b09",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64951e3b-e012-4ed5-8846-03eed103c696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71222088-d62f-40fc-b935-f9c8ee99a579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import config\n",
    "import sys \n",
    "sys.path.append(\"C:/Users/isxzl/OneDrive/Code/AutoSSL\")\n",
    "import yaml\n",
    "from torchvision.transforms import RandomRotation,GaussianBlur,ColorJitter\n",
    "from autoSSL.evaluate import eval_KNN,eval_linear,eval_KNNplot,pipe_collate\n",
    "from autoSSL.models import BarlowTwins, BYOL, MoCo, SimCLR, SimSiam, VICReg ,Toymodel, pipe_model \n",
    "from autoSSL.utils import embedding_feature,ck_callback,dict2transformer,trans2multi,join_dir,ContinuousCSVLogger  \n",
    "from autoSSL.data import PipeDataset\n",
    "from autoSSL.train import Trainer\n",
    "\n",
    "from lightly.transforms import SimCLRTransform\n",
    "from lightly.transforms.rotation import random_rotation_transform\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eba85e-c2d4-4ed3-a276-2b03ff82f764",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Global Baseline Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb0e5479-43a3-4b63-b9e2-b8ce70103290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML file\n",
    "with open('global.yaml', 'r') as file:\n",
    "    global_config = yaml.safe_load(file)\n",
    "\n",
    "# Write your experiment notebook name here\n",
    "global_config[\"experiment\"]=\"views VS model\"   \n",
    "global_config[\"dataset_dir\"]=\"../Datasets/cifar10/train/\"\n",
    "global_config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,None,None]]    \n",
    "\n",
    "global_config[\"loss_func\"]=\"BarlowTwinsLoss\"     \n",
    "global_config[\"view_model\"]=\"None\"     \n",
    "global_config[\"view\"]=2 \n",
    "global_config[\"stop_gradient\"]=False   \n",
    "global_config[\"optimizer\"]=\"SGD\"      \n",
    "global_config[\"schedule\"]=\"cos\"   \n",
    "global_config[\"model\"]=\"Toymodel\"\n",
    "global_config[\"batch_size\"]=128\n",
    "global_config[\"input_size\"]=32\n",
    "global_config[\"max_epochs\"]=50\n",
    "\n",
    "#global_config[\"backbone\"]=\"resnet18_pretrained\"\n",
    "\n",
    "# Define global Training Augmentation\n",
    "global_SSL_augmentation=T.Compose([\n",
    "    T.RandomResizedCrop(size=global_config[\"input_size\"], scale=(0.08, 1.0)),\n",
    "    T.RandomApply([T.RandomRotation(degrees=90)], p=0),\n",
    "    T.RandomHorizontalFlip(p= 0.5),\n",
    "    T.RandomVerticalFlip(p=0),\n",
    "    T.RandomApply([T.ColorJitter(\n",
    "            brightness=0.4,contrast=0.4,saturation=0.4, hue=0.1)], p=0.8),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "# Define the Testing Augmentation\n",
    "test_SSL_augmentation=T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "p_knndata= PipeDataset(input_dir=global_config[\"path_to_test_cifar10\"], \n",
    "        augmentation=trans2multi(test_SSL_augmentation,view=1), \n",
    "        batch_size=global_config[\"batch_size\"],num_workers=global_config[\"num_workers\"]).dataloader\n",
    "p_knndata=[p_knndata,10]  # The second number is the classes number of this datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f8a6e-bfc8-4b5b-98b0-4c3797a6f437",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Config 1 Baseline for everything (Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa842a-b5ad-4667-b051-2d91f4358bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "st=True\n",
    "\n",
    "\n",
    "for baseline in [[pd,st,True],[[],False,False],[pd,False,True],[[],st,False]]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=2\n",
    "    config[\"view_model\"]= \"None\"\n",
    "    config[\"name\"]=f\"LNeg_VMNone_PR{baseline[2]}_st{baseline[1]}_b128_m50_v{4}\"\n",
    "    config[\"loss_func\"]=\"NegativeCosineSimilarity\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= baseline[1]\n",
    "    config[\"predhead_dim\"]= baseline[0]\n",
    "    config[\"max_epochs\"]= 50\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a869f3e-3e94-4d05-bec5-6fdd31695b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config2 Grid Strategy View 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6c1cd7-85d5-4520-8af3-b00238920a97",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| View Model | Description | Formula |\n",
    "|:---:|:---:|:---:|\n",
    "|`fastsim`| Calculate the mean embedding of all other views, compare each view embedding with this mean embedding. | $$\\text{loss} = \\frac{1}{n} \\sum_{i=0}^{n} \\text{distance}(p_{i}, z_{\\text{mean}_{-i}}),$$ where $$z_{\\text{mean}_{-i}} = \\frac{1}{n-1} \\sum_{j \\neq i} z_{j}.$$ |\n",
    "|`pair-pair`| Compare each pair of different views. | $$\\text{loss} = \\frac{2}{n(n-1)} \\sum_{i=0}^{n-1} \\sum_{j=i+1}^{n} \\text{distance}(z_{i}, z_{j}).$$ |\n",
    "|`1_n`| Compare the first view with all other views. | $$\\text{loss} = \\frac{1}{n-1} \\sum_{i=1}^{n} \\text{distance}(z_{0}, z_{i}).$$ |\n",
    "|`1_fastsim`| Calculate the mean embedding of all views except the first one, compare the first view embedding with this mean embedding. | $$\\text{loss} = \\text{distance}(z_{0}, z_{\\text{mean}_{-0}}),$$ where $$z_{\\text{mean}_{-0}} = \\frac{1}{n-1} \\sum_{i \\neq 0} z_{i}.$$ |\n",
    "|`mean_n`| Calculate the mean embedding of all views, compare each view embedding with this mean embedding. | $$\\text{loss} = \\frac{1}{n} \\sum_{i=0}^{n} \\text{distance}(z_{i}, z_{\\text{mean}}),$$ where $$z_{\\text{mean}} = \\frac{1}{n} \\sum_{j=0}^{n} z_{j}.$$ |\n",
    "|`std_view`| Similar to `mean_n`, but the distance is squared, summed, and then the square root is taken (RMS). | $$\\text{loss} = \\sqrt{\\frac{1}{n} \\sum_{i=0}^{n} \\text{distance}(z_{i}, z_{\\text{mean}})^{2}},$$ where $$z_{\\text{mean}} = \\frac{1}{n} \\sum_{j=0}^{n} z_{j}.$$ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dd33db-b0f5-4406-a9d7-42f5d3b0b566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:615: UserWarning: Checkpoint directory C:\\Users\\isxzl\\OneDrive\\Code\\AutoSSL\\experiment\\experiment_checkpoints\\views VS model\\LNeg_VMvar_view_PRNoBN_stTrue_b128_m50_v4 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                     | Params\n",
      "-------------------------------------------------------------\n",
      "0 | backbone        | Sequential               | 11.2 M\n",
      "1 | criterion       | NegativeCosineSimilarity | 0     \n",
      "2 | projection_head | ProjectionHead           | 5.3 M \n",
      "3 | prediction_head | ProjectionHead           | 2.1 M \n",
      "-------------------------------------------------------------\n",
      "18.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "18.5 M    Total params\n",
      "74.111    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  74%|█████████████████████████████████████████████▋                | 287/390 [00:27<00:09, 10.55it/s, v_num=1]"
     ]
    }
   ],
   "source": [
    "\n",
    "for baseline0 in [\"var_view\",\"Stop\", \"mean_n\" ,\"fastsim\", \"pair-pair\" , \"1_n\" ,\"1_fastsim\"]:   \n",
    "    \n",
    "    \n",
    "    pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    st=True\n",
    "    pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]  \n",
    "    for baseline in [[pd,st,\"NoBN\"]]: #,[[],False,False],[pd,False,\"TruewithBN\"],[[],st,False]\n",
    "        # MAKE YOUR OWN CONFIG\n",
    "        config=global_config.copy()\n",
    "        # Fill the config\n",
    "        SSL_augmentation=global_SSL_augmentation\n",
    "        config[\"view\"]=4\n",
    "        config[\"view_model\"]= baseline0\n",
    "        config[\"name\"]=f\"LNeg_VM{baseline0}_PR{baseline[2]}_st{baseline[1]}_b128_m50_v{4}\"\n",
    "        config[\"loss_func\"]=\"NegativeCosineSimilarity\" \n",
    "        config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "        config[\"stop_gradient\"]= baseline[1]\n",
    "        config[\"predhead_dim\"]= baseline[0]\n",
    "        config[\"max_epochs\"]= 50\n",
    "         \n",
    "        # THIS IS THE CODE TO LOAD DATASET\n",
    "        pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "\n",
    "        # THIS IS THE CODE TO LOAD MODEL\n",
    "        #pmodel=pipe_model(config=config) \n",
    "        pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "\n",
    "\n",
    "        # Use this if you want to START a train\n",
    "        trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =1) #precision='16-mixed',\n",
    "        trainer.fit(pmodel, pdata.dataloader,)  \n",
    "\n",
    "        # Use this if you want to CONTINUE a train\n",
    "        #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "        #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "        print(pmodel.debug)\n",
    "        del pdata\n",
    "        del pmodel\n",
    "        del trainer    \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c3b5a-d63b-4964-8e48-e06bb4d2be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "for baseline0 in [\"var_view\"]:   \n",
    "    \n",
    "    \n",
    "    pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    st=True\n",
    "    pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]  \n",
    "    for baseline in [[pd,st,\"NoBN\"]]: #,[[],False,False],[pd,False,\"TruewithBN\"],[[],st,False]\n",
    "        # MAKE YOUR OWN CONFIG\n",
    "        config=global_config.copy()\n",
    "        # Fill the config\n",
    "        SSL_augmentation=global_SSL_augmentation\n",
    "        config[\"view\"]=4\n",
    "        config[\"view_model\"]= baseline0\n",
    "        config[\"name\"]=f\"Test_adam_LNeg_VM{baseline0}_PR{baseline[2]}_st{baseline[1]}_b128_m50_v{4}\"\n",
    "        config[\"loss_func\"]=\"NegativeCosineSimilarity\"\n",
    "        config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "        config[\"stop_gradient\"]= baseline[1]\n",
    "        config[\"predhead_dim\"]= baseline[0]\n",
    "        config[\"max_epochs\"]= 50\n",
    "        config[\"optimizer\"]= \"Adam\"\n",
    "        # THIS IS THE CODE TO LOAD DATASET\n",
    "        pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "\n",
    "        # THIS IS THE CODE TO LOAD MODEL\n",
    "        #pmodel=pipe_model(config=config) \n",
    "        pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "\n",
    "\n",
    "        # Use this if you want to START a train\n",
    "        trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =1) #precision='16-mixed',\n",
    "        trainer.fit(pmodel, pdata.dataloader,)  \n",
    "\n",
    "        # Use this if you want to CONTINUE a train\n",
    "        #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "        #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "        print(pmodel.debug)\n",
    "        del pdata\n",
    "        del pmodel\n",
    "        del trainer    \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4e6970-4604-485e-bd37-60383de96b81",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Config3: view can fix stop or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1352c0-9e27-470a-919d-af85ba9365b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]    \n",
    "st=True\n",
    "for baseline in [[pd2,\"TrueBN\"]]:  #[pd,\"TruenoBN\"],\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=4\n",
    "    config[\"view_model\"]= \"mean_n\"\n",
    "    config[\"name\"]=f\"LNeg_VMmean_n_PR{baseline[1]}_stFalse_b128_m50_v{4}\"\n",
    "    config[\"loss_func\"]=\"NegativeCosineSimilarity\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= False\n",
    "    config[\"predhead_dim\"]= baseline[0]\n",
    "    config[\"max_epochs\"]= 50\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =1) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a70d5bb-279d-47b1-8850-dd2288e5e568",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Config4: Check different view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb66c1a-89fa-4ce2-9f35-92ba04726170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]    \n",
    "st=True\n",
    "for baseline in [6]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=baseline\n",
    "    config[\"view_model\"]= \"mean_n\" #\"1_fastsim\"  \n",
    "    config[\"name\"]=f\"LNeg_VMmean_n_PRTruenoBN_stTrue_b128_m50_v{baseline}\"\n",
    "    config[\"loss_func\"]=\"NegativeCosineSimilarity\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= True\n",
    "    config[\"predhead_dim\"]= [[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    config[\"max_epochs\"]= 50\n",
    "\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d877c7-b5f9-4dd9-ae01-04f29327fe52",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config5: Check how it improve the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c066a-22d5-459d-9e6a-5262202bc83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd=[[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "pd2=[[2048,512, \"BN\",\"RELU\"],[512,2048,\"BN\",None]]    \n",
    "st=True\n",
    "\n",
    "for baseline in [[\"None\",2]]: #\"mean_n\",\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=baseline[1]\n",
    "    config[\"view_model\"]= baseline[0]#\"1_fastsim\"  \n",
    "    config[\"name\"]=f\"Model_{'VICREG'}_VM{baseline[0]}_v{baseline[1]}\"\n",
    "    config[\"loss_func\"]=\"VICRegLoss\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= False\n",
    "    #config[\"predhead_dim\"]= [[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    config[\"max_epochs\"]= 25\n",
    "\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67682be1-2079-464a-97e5-26c474f5bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "for baseline in [[\"mean_n\",4],[\"None\",2]]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=baseline[1]\n",
    "    config[\"view_model\"]= baseline[0] #\"1_fastsim\"  \n",
    "    config[\"name\"]=f\"Model_{'BT'}_VM{baseline[0]}_v{baseline[1]}\"\n",
    "    config[\"loss_func\"]=\"BarlowTwinsLoss\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= False\n",
    "    #config[\"predhead_dim\"]= [[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    config[\"max_epochs\"]= 25\n",
    "\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9f088-0da9-46fc-ae90-6bbc2a94ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for baseline in [[\"mean_n\",4],[\"None\",2]]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"view\"]=baseline[1]\n",
    "    config[\"view_model\"]=baseline[0] #\"1_fastsim\"  \n",
    "    config[\"name\"]=f\"Model_{'SimCLR'}_VM{baseline[0]}_v{baseline[1]}\"\n",
    "    config[\"loss_func\"]=\"SimCLR\" \n",
    "    config[\"prjhead_dim\"]=[[512,2048, \"BN\",\"RELU\"],[2048,2048,\"BN\",None]]    \n",
    "    config[\"stop_gradient\"]= False\n",
    "    #config[\"predhead_dim\"]= [[2048,512, \"BN\",\"RELU\"],[512,2048,None,None]]    \n",
    "    config[\"max_epochs\"]= 25\n",
    "\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=trans2multi(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24c6a03-29f7-449a-af3a-05075be370e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Improved Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d224f-296a-4bbf-9fbd-d89c516521b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for baseline in [4,12]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"name\"]=f\"mean_n_BT_Baseline_b128_m50_v{str(baseline)}\"\n",
    "\n",
    "    config[\"view\"]=baseline\n",
    "    config[\"view_model\"]= \"mean_n\"   # 1_n # mean_n #1_fastsim\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=dict2transformer(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c000950-f854-4901-b001-79a712b2fcc2",
   "metadata": {},
   "source": [
    "## Config 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e13b52-9b87-4072-9103-b8fd93f3de24",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "for baseline in [4,12]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation\n",
    "    config[\"name\"]=f\"1_fastsim_BT_Baseline_b128_m50_v{str(baseline)}\"\n",
    "\n",
    "    config[\"view\"]=baseline\n",
    "    config[\"view_model\"]= \"1_fastsim\"   # 1_n # mean_n #\n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=dict2transformer(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    print(pmodel.debug)\n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50686839-6777-4c26-b7f6-02b871ac184a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "404d98da-b3a5-4e58-8628-eb1acb25fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from sklearn.preprocessing import normalize\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from autoSSL.models.Backbone import pipe_backbone\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def embed(x, embedding_model, device):\n",
    "    embedding_model.eval()\n",
    "    embedding_model.to(device)\n",
    "    x = x.float().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = embedding_model(x)\n",
    "        pooled_embeddings = torch.nn.functional.adaptive_avg_pool2d(embeddings, (1, 1))\n",
    "\n",
    "    return pooled_embeddings.view(pooled_embeddings.size(0), -1).cpu().numpy()\n",
    "\n",
    "def eval_everything(pipe_data, models, device='cuda', split=None, test=None, baseline=None):\n",
    "    if split is not None:\n",
    "        train_data, test_data = pipe_data.split(split)\n",
    "    elif test is None:\n",
    "        train_data = pipe_data\n",
    "        test_data = pipe_data\n",
    "    else:         \n",
    "        train_data = pipe_data\n",
    "        test_data = test\n",
    "\n",
    "    print(\"Load the training and testing dataset\")\n",
    "    X_train, y_train = train_data.array[0], train_data.array[1]\n",
    "    X_test, y_test = test_data.array[0], test_data.array[1]\n",
    "\n",
    "    if isinstance(models, torch.nn.Module):\n",
    "        models = {'name': ['model_0'], 'model': [models], 'address': None} \n",
    "    elif isinstance(models, list):\n",
    "        models = {'name': ['model_'+str(i) for i in range(len(models))], 'model': models, 'address': None}\n",
    "     \n",
    "    baselines=[]\n",
    "    results = []\n",
    "    baselines_name=[]\n",
    "    if baseline:\n",
    "        for base in baseline:\n",
    "            baseline_backbone, _ = pipe_backbone(backbone=base)\n",
    "            models['model'].append(baseline_backbone)\n",
    "            models['name'].append('baseline_' + base)\n",
    "            baselines_name.append('baseline_' + base)\n",
    "            baselines.append(baseline_backbone)\n",
    "        \n",
    "    writer = pd.ExcelWriter(models['address'].replace('.csv', '_confusion.xlsx'))\n",
    "\n",
    "    for i, embedding_model in enumerate(tqdm(models['model'])):\n",
    "        if embedding_model in baselines:\n",
    "            pass\n",
    "        else:\n",
    "            embedding_model=embedding_model.backbone\n",
    "        X_train_embedding = [embed(x, embedding_model, device) for x in DataLoader(X_train, batch_size=16)]\n",
    "        X_train_embedding = np.concatenate(X_train_embedding)\n",
    "\n",
    "        X_test_embedding = [embed(x, embedding_model, device) for x in DataLoader(X_test, batch_size=16)]\n",
    "        X_test_embedding = np.concatenate(X_test_embedding)\n",
    "\n",
    "        X_train_embedding = normalize(X_train_embedding)  # Normalize train embeddings\n",
    "\n",
    "        X_test_embedding = normalize(X_test_embedding)  # Normalize test embeddings\n",
    "\n",
    "        \n",
    "        if X_test_embedding is None:\n",
    "            accuracy = 'model_collapse'\n",
    "            confusion = None\n",
    "        else:\n",
    "            clf = SGDClassifier(loss='log_loss')\n",
    "\n",
    "            clf.fit(X_train_embedding, y_train)\n",
    "\n",
    "            # Get class probabilities for each sample\n",
    "            class_probs = clf.predict_proba(X_test_embedding)\n",
    "\n",
    "            # Get the top 1 predictions\n",
    "            top1_preds = np.argmax(class_probs, axis=1)\n",
    "            top3_preds = np.argpartition(class_probs, -3, axis=1)[:,-3:]\n",
    "\n",
    "            # Calculate confusion matrix\n",
    "            confusion = confusion_matrix(y_test, top1_preds)\n",
    "            # Normalize confusion matrix by row (i.e by the number of samples in each class)\n",
    "            confusion = confusion.astype('float') / confusion.sum(axis=1)[:, np.newaxis]\n",
    "            # Average accuracy is the mean of the diagonal elements (the correctly classified instances)\n",
    "            top1_average_accuracy = np.mean(np.diag(confusion))\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            top1_accuracy = accuracy_score(y_test, top1_preds)\n",
    "            top3_accuracy = np.mean([1 if y in top3 else 0 for y, top3 in zip(y_test, top3_preds)])\n",
    "                            \n",
    "            # K-Nearest Neighbors classifier\n",
    "            knn = KNeighborsClassifier(n_neighbors=20)\n",
    "            knn.fit(X_train_embedding, y_train)\n",
    "            knn_preds = knn.predict(X_test_embedding)\n",
    "            knn_accuracy = accuracy_score(y_test, knn_preds)\n",
    "\n",
    "            accuracy = {\n",
    "                \"Top-1 Accuracy\": top1_accuracy,\n",
    "                \"Top-3 Accuracy\": top3_accuracy,\n",
    "                \"Top-1 Average Accuracy\": top1_average_accuracy,\n",
    "                \"KNN Top-1 Accuracy\": knn_accuracy  # KNN accuracy\n",
    "            }\n",
    "\n",
    "        namee = models[\"name\"][i]\n",
    "        results.append((namee, accuracy))\n",
    "\n",
    "        \n",
    "        del embedding_model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Save confusion matrix to Excel file\n",
    "        if confusion is not None:\n",
    "            df_confusion = pd.DataFrame(confusion)\n",
    "            df_confusion.to_excel(writer, sheet_name=namee)\n",
    "\n",
    "    writer.save()\n",
    "\n",
    "    if models['address'] is not None:\n",
    "        df = pd.read_csv(models['address'])\n",
    "\n",
    "        # If baselines are present, add new rows in the dataframe for them\n",
    "        if baselines:\n",
    "            for base, base_name in zip(baselines, baselines_name):\n",
    "                # Initialize a new row with default values\n",
    "                new_row = {col: None for col in df.columns}\n",
    "                # Update the values we know\n",
    "                new_row.update({\n",
    "                    'dir_name': base_name,\n",
    "                })\n",
    "                # Append the new row to the dataframe\n",
    "                df = df.append(new_row, ignore_index=True)\n",
    "\n",
    "        # Assuming the results are in the same order as the models in the dataframe\n",
    "        df['linear_top1_accuracy'] = [result[1][\"Top-1 Accuracy\"] for result in results]\n",
    "        df['linear_top3_accuracy'] = [result[1][\"Top-3 Accuracy\"] for result in results]\n",
    "        df['linear_top1_average_accuracy'] = [result[1][\"Top-1 Average Accuracy\"] for result in results]\n",
    "        df['linear_knn_top1_accuracy'] = [result[1][\"KNN Top-1 Accuracy\"] for result in results]\n",
    "\n",
    "        df.to_csv(models['address'], index=False)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8ad6455-b8c3-4d5d-b6eb-5627bd52fbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collating the models' (evaluating) information to experiment_checkpoints/views VS model/Model__.csv\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append(\"C:/Users/isxzl/OneDrive/Code/AutoSSL\")\n",
    "from autoSSL.data import PipeDataset \n",
    "from autoSSL.utils import  dict2transformer,trans2multi  \n",
    "import torchvision.transforms as T\n",
    "from autoSSL.evaluate import eval_KNN,eval_linear,eval_KNNplot,pipe_collate#,eval_everything\n",
    "import yaml\n",
    "# Load the YAML file\n",
    "with open('global.yaml', 'r') as file:\n",
    "    global_config = yaml.safe_load(file)\n",
    "\n",
    "\n",
    "test_augmentation=T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# \"LNeg_VM.*_PRTruewithBN_stTrue_b128_m50_v4\"\n",
    "\n",
    "collate =pipe_collate(address=\"experiment_checkpoints/views VS model/\", reg=\"Model_*\")\n",
    "\n",
    "pdata = PipeDataset(input_dir=global_config[\"path_to_test_cifar10\"],\n",
    "    augmentation=trans2multi(test_augmentation,view=1))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca362de1-b68b-45c8-94f6-853afff878aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the training and testing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10001/10001 [00:03<00:00, 2935.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 10001/10001 [00:03<00:00, 2951.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 10001/10001 [00:03<00:00, 2928.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 10001/10001 [00:03<00:00, 3016.76it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:37<00:00,  4.66s/it]\n",
      "C:\\Users\\isxzl\\AppData\\Local\\Temp\\ipykernel_27348\\357591954.py:127: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
      "  writer.save()\n"
     ]
    }
   ],
   "source": [
    "aaa=eval_everything(pdata, models=collate, device=global_config[\"device\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff8f9096-89ab-4926-8c60-cca73920c337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Model_BT_VMmean_n_v4',\n",
       "  {'Top-1 Accuracy': 0.42515748425157485,\n",
       "   'Top-3 Accuracy': 0.7487251274872513,\n",
       "   'Top-1 Average Accuracy': 0.425151948051948,\n",
       "   'KNN Top-1 Accuracy': 0.4520547945205479}),\n",
       " ('Model_BT_VMNone_v2',\n",
       "  {'Top-1 Accuracy': 0.4844515548445156,\n",
       "   'Top-3 Accuracy': 0.8134186581341866,\n",
       "   'Top-1 Average Accuracy': 0.48445434565434564,\n",
       "   'KNN Top-1 Accuracy': 0.50994900509949}),\n",
       " ('Model_SimCLR_VMmean_n_v4',\n",
       "  {'Top-1 Accuracy': 0.6398360163983602,\n",
       "   'Top-3 Accuracy': 0.9048095190480951,\n",
       "   'Top-1 Average Accuracy': 0.6398301698301698,\n",
       "   'KNN Top-1 Accuracy': 0.650934906509349}),\n",
       " ('Model_SimCLR_VMNone_v2',\n",
       "  {'Top-1 Accuracy': 0.5495450454954505,\n",
       "   'Top-3 Accuracy': 0.8661133886611339,\n",
       "   'Top-1 Average Accuracy': 0.5495375624375625,\n",
       "   'KNN Top-1 Accuracy': 0.5729427057294271}),\n",
       " ('Model_Simsiam_VMmean_n_v4',\n",
       "  {'Top-1 Accuracy': 0.6964303569643036,\n",
       "   'Top-3 Accuracy': 0.9244075592440756,\n",
       "   'Top-1 Average Accuracy': 0.6964298701298702,\n",
       "   'KNN Top-1 Accuracy': 0.7037296270372962}),\n",
       " ('Model_Simsiam_VMNone_n_v2',\n",
       "  {'Top-1 Accuracy': 0.49945005499450057,\n",
       "   'Top-3 Accuracy': 0.8277172282771723,\n",
       "   'Top-1 Average Accuracy': 0.49945574425574424,\n",
       "   'KNN Top-1 Accuracy': 0.5223477652234777}),\n",
       " ('Model_VICREG_VMmean_n_v4',\n",
       "  {'Top-1 Accuracy': 0.3528647135286471,\n",
       "   'Top-3 Accuracy': 0.6904309569043096,\n",
       "   'Top-1 Average Accuracy': 0.352840959040959,\n",
       "   'KNN Top-1 Accuracy': 0.4243575642435756}),\n",
       " ('Model_VICREG_VMNone_v2',\n",
       "  {'Top-1 Accuracy': 0.3210678932106789,\n",
       "   'Top-3 Accuracy': 0.6591340865913409,\n",
       "   'Top-1 Average Accuracy': 0.32105214785214786,\n",
       "   'KNN Top-1 Accuracy': 0.3915608439156084})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed89fa6-f9b9-4af6-8531-da9c2729d020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a19fd-fa88-4c31-a829-e689313f04d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a881acc-f9b8-4f25-a5e9-1ab011d77b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01778bba-2d6b-4cf2-b94c-356887aac840",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
