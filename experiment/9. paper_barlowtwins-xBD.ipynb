{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4533ee81-d579-4867-b15b-aa7f3f8f385f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiment Introduction\n",
    "\n",
    "Background, \n",
    "In order to do ablation experiment, we have diffculty like\n",
    "1. most of them are same, we have to repeat many times.\n",
    "2. too many model, config, code hard to manage and save. too messy\n",
    "\n",
    "\n",
    "For all kind of SSL training workflow, we have to define the hyperparameters includes 4 aspect,\n",
    "Dataset\n",
    "Model\n",
    "Training\n",
    "Saving COnfig\n",
    "\n",
    "\n",
    "\n",
    "How to Use this Experiment?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff5e60f-6c7d-47b6-8b30-5ad2e6c94b09",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64951e3b-e012-4ed5-8846-03eed103c696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71222088-d62f-40fc-b935-f9c8ee99a579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import config\n",
    "import sys \n",
    "sys.path.append(\"C:/Users/isxzl/OneDrive/Code/AutoSSL\")\n",
    "import yaml\n",
    "from torchvision.transforms import RandomRotation,GaussianBlur,ColorJitter\n",
    "from autoSSL.evaluate import eval_KNN,eval_linear,eval_KNNplot,pipe_collate\n",
    "from autoSSL.models import BarlowTwins, BYOL, MoCo, SimCLR, SimSiam, VICReg ,Toymodel, pipe_model \n",
    "from autoSSL.utils import embedding_feature,ck_callback,dict2transformer,join_dir,ContinuousCSVLogger  \n",
    "from autoSSL.data import PipeDataset\n",
    "from autoSSL.train import Trainer\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eba85e-c2d4-4ed3-a276-2b03ff82f764",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Global Baseline Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16334f28-7fca-42b0-afbc-f19c2b4ee76f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0e5479-43a3-4b63-b9e2-b8ce70103290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML file\n",
    "with open('global.yaml', 'r') as file:\n",
    "    global_config = yaml.safe_load(file)\n",
    "\n",
    "# Write your experiment notebook name here\n",
    "global_config[\"experiment\"]=\"paper_barlowtwins\"   \n",
    "global_config[\"experiment\"]=\"paper_barlowtwins_xBD\"   \n",
    "\n",
    "global_config[\"prjhead_dim\"]=[2048,2048]    \n",
    "global_config[\"predhead_dim\"]=[]\n",
    "global_config[\"loss_func\"]=\"BarlowTwinsLoss\"     \n",
    "global_config[\"view_model\"]=\"None\"     \n",
    "global_config[\"view\"]=2 \n",
    "global_config[\"stop_gradient\"]=False   \n",
    "global_config[\"optimizer\"]=\"LARS\"      \n",
    "global_config[\"schedule\"]=\"cos\"   \n",
    "global_config[\"model\"]=\"Toymodel\"\n",
    "global_config[\"batch_size\"]=128\n",
    "global_config[\"input_size\"]=128\n",
    "global_config[\"backbone\"]=\"resnet18_pretrained\"\n",
    "\n",
    "# Define global view function\n",
    "global_SSL_augmentation = {\n",
    "    'RandomResizedCrop': {'size': global_config[\"input_size\"], \"scale\":(0.2,1.0)},\n",
    "    'RandomApply':{'transforms':[RandomRotation(degrees=90)], 'p':0},\n",
    "    'RandomHorizontalFlip': {'p': 0.5},\n",
    "    'RandomVerticalFlip':  {'p':0},\n",
    "    'RandomApply':{'transforms': [ColorJitter(brightness=0.4,contrast=0.4,saturation=0.4,hue=0.1)], 'p':0.8},\n",
    "    'RandomGrayscale' :{'p':0.2},\n",
    "    #'RandomSolarize':{'threshold':128, 'p':0.1},\n",
    "    'RandomApply':{'transforms':[GaussianBlur(kernel_size=3,sigma=(0.1, 2))],'p':0},\n",
    "    'ToTensor': {},\n",
    "    'Normalize': {\"mean\": [0.485, 0.456, 0.406], \"std\": [0.229, 0.224, 0.225]}\n",
    "}\n",
    "\n",
    "test_SSL_augmentation = {\n",
    "    'RandomResizedCrop': {'size': global_config[\"input_size\"], \"scale\":(0.2,1.0)},\n",
    "    'ToTensor': {},\n",
    "    'Normalize': {\"mean\": [0.485, 0.456, 0.406], \"std\": [0.229, 0.224, 0.225]}\n",
    "} \n",
    "\n",
    "# THIS IS THE CODE TO Monitor the KNN accuracy for each epoch\n",
    "p_knndata= PipeDataset(input_dir=global_config[\"path_to_test_xBD\"], \n",
    "        augmentation=dict2transformer(test_SSL_augmentation,view=1), \n",
    "        batch_size=global_config[\"batch_size\"],num_workers=global_config[\"num_workers\"]).dataloader\n",
    "p_knndata=[p_knndata,5]  # The second number is the classes number of this datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bdbd76-a430-4e54-903f-d9ca1d69e199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a869f3e-3e94-4d05-bec5-6fdd31695b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Grid Run Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6193d5-6e9b-449e-8f33-c0f4dd0e07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4e6970-4604-485e-bd37-60383de96b81",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config1: Baseline of Cifar with Batch 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1352c0-9e27-470a-919d-af85ba9365b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for sample in in [1]:\n",
    "    \n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation.copy()\n",
    "    config[\"name\"]=f\"BT_res18_128_200\"\n",
    "    config[\"batch_size\"]=128\n",
    "    config[\"max_epochs\"]=200\n",
    "    config[\"input_size\"]=512\n",
    "    config[\"dataset_dir\"]=\"../Datasets/xBD/train/\"\n",
    "    \n",
    "    p_knndata= PipeDataset(input_dir=config[\"path_to_test_xBD\"], \n",
    "        augmentation=dict2transformer(test_SSL_augmentation,view=1), \n",
    "        batch_size=config[\"batch_size\"],num_workers=config[\"num_workers\"]).dataloader\n",
    "    p_knndata=[p_knndata,5]  # The second number is the classes number of this datasets\n",
    "    \n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=dict2transformer(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5,log_every_n_steps=10) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    \n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67682be1-2079-464a-97e5-26c474f5bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for baseline in [0.5,0.25,0.1]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation.copy()\n",
    "    config[\"name\"]=f\"BT_res50_64_100_{baseline}\"\n",
    "    config[\"batch_size\"]=32\n",
    "    config[\"max_epochs\"]=100\n",
    "    config[\"input_size\"]=512\n",
    "    config[\"dataset_dir\"]=\"../Datasets/xBD/train/\"\n",
    "    config[\"samples\"]= int(baseline*2799)\n",
    "    global_config[\"backbone\"]=\"resnet18_pretrained\"\n",
    "    p_knndata= PipeDataset(input_dir=config[\"path_to_test_xBD\"], \n",
    "    augmentation=dict2transformer(test_SSL_augmentation,view=1), \n",
    "    batch_size=config[\"batch_size\"],num_workers=config[\"num_workers\"],samples=8*32).dataloader\n",
    "    p_knndata=[p_knndata,5]  # The second number is the classes number of this datasets\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=dict2transformer(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5,log_every_n_steps=int(2799*baseline/32/2)) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    \n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a70d5bb-279d-47b1-8850-dd2288e5e568",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config2: Without pretrained for 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b13c1f-9991-4e76-99ca-c1bf53f60829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for baseline in [[\"res18_random\",\"resnet18\"],[\"res50_random\",\"resnet50\"]]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation.copy()\n",
    "    config[\"name\"]=f\"BT_{baseline[0]}_64_100_0.5\"\n",
    "    config[\"batch_size\"]=32\n",
    "    config[\"max_epochs\"]=100\n",
    "    config[\"input_size\"]=512\n",
    "    config[\"dataset_dir\"]=\"../Datasets/xBD/train/\"\n",
    "    config[\"samples\"]= int(0.5*2799)\n",
    "    config[\"backbone\"]=baseline[1]\n",
    "    p_knndata= PipeDataset(input_dir=config[\"path_to_test_xBD\"], \n",
    "    augmentation=dict2transformer(test_SSL_augmentation,view=1), \n",
    "    batch_size=config[\"batch_size\"],num_workers=config[\"num_workers\"],samples=8*32).dataloader\n",
    "    p_knndata=[p_knndata,5]  # The second number is the classes number of this datasets\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=dict2transformer(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5,log_every_n_steps=int(2799*0.5/32/2)) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    \n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24c6a03-29f7-449a-af3a-05075be370e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Backbone RES 50: 100, 50, 25, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fed92c-0d7a-47ec-9b75-0cfbe6ba03aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for baseline in [0.1,0.25,0.5,1]:\n",
    "    # MAKE YOUR OWN CONFIG\n",
    "    config=global_config.copy()\n",
    "    # Fill the config\n",
    "    SSL_augmentation=global_SSL_augmentation.copy()\n",
    "    config[\"name\"]=f\"BT_res50_64_100_{baseline}\"\n",
    "    config[\"batch_size\"]=32\n",
    "    config[\"max_epochs\"]=100\n",
    "    config[\"input_size\"]=512\n",
    "    config[\"dataset_dir\"]=\"../Datasets/xBD/train/\"\n",
    "    config[\"samples\"]= int(baseline*2799)\n",
    "    global_config[\"backbone\"]=\"resnet50_pretrained\"\n",
    "    p_knndata= PipeDataset(input_dir=config[\"path_to_test_xBD\"], \n",
    "    augmentation=dict2transformer(test_SSL_augmentation,view=1), \n",
    "    batch_size=config[\"batch_size\"],num_workers=config[\"num_workers\"],samples=8*32).dataloader\n",
    "    p_knndata=[p_knndata,5]  # The second number is the classes number of this datasets\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD DATASET\n",
    "    pdata= PipeDataset(config=config,augmentation=dict2transformer(SSL_augmentation,view=config[\"view\"]))\n",
    "    \n",
    "    # THIS IS THE CODE TO LOAD MODEL\n",
    "    #pmodel=pipe_model(config=config) \n",
    "    pmodel=pipe_model(config=config,MonitoringbyKNN=p_knndata) # All save the validation\n",
    "    \n",
    " \n",
    "    # Use this if you want to START a train\n",
    "    trainer=Trainer(config, model_mode=\"start\",check_val_every_n_epoch =5,log_every_n_steps=int(2799*baseline/32/2)) #precision='16-mixed',\n",
    "    trainer.fit(pmodel, pdata.dataloader,)  \n",
    "    \n",
    "    # Use this if you want to CONTINUE a train\n",
    "    #trainer=Trainer(config, model_mode=\"continue\", extra_epoch=1,precision='16-mixed')\n",
    "    #trainer.fit(pmodel, pdata.dataloader,ckpt_path=\"latest\")  \n",
    "    \n",
    "    del pdata\n",
    "    del pmodel\n",
    "    del trainer\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e13b52-9b87-4072-9103-b8fd93f3de24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50686839-6777-4c26-b7f6-02b871ac184a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ceb7e44-99bd-4770-8815-a4c00dd79d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from autoSSL.models.Backbone import pipe_backbone\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def embed(x, embedding_model, device):\n",
    "    embedding_model.eval()\n",
    "    embedding_model.to(device)\n",
    "    x = x.float().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = embedding_model(x)\n",
    "        pooled_embeddings = torch.nn.functional.adaptive_avg_pool2d(embeddings, (1, 1))\n",
    "\n",
    "    return pooled_embeddings.view(pooled_embeddings.size(0), -1).cpu().numpy()\n",
    "\n",
    "def eval_linear(pipe_data, models, device='cuda', split=None, test=None, baseline=None):\n",
    "    if split is not None:\n",
    "        train_data, test_data = pipe_data.split(split)\n",
    "    elif test is None:\n",
    "        train_data = pipe_data\n",
    "        test_data = pipe_data\n",
    "    else:         \n",
    "        train_data = pipe_data\n",
    "        test_data = test\n",
    "\n",
    "    print(\"Load the training and testing dataset\")\n",
    "    X_train, y_train = train_data.array[0], train_data.array[1]\n",
    "    X_test, y_test = test_data.array[0], test_data.array[1]\n",
    "\n",
    "    if isinstance(models, torch.nn.Module):\n",
    "        models = {'name': ['model_0'], 'model': [models], 'address': None} \n",
    "    elif isinstance(models, list):\n",
    "        models = {'name': ['model_'+str(i) for i in range(len(models))], 'model': models, 'address': None}\n",
    "     \n",
    "    baselines=[]\n",
    "    results = []\n",
    "    baselines_name=[]\n",
    "    if baseline:\n",
    "        for base in baseline:\n",
    "            baseline_backbone, _ = pipe_backbone(backbone=base)\n",
    "            models['model'].append(baseline_backbone)\n",
    "            models['name'].append('baseline_' + base)\n",
    "            baselines_name.append('baseline_' + base)\n",
    "            baselines.append(baseline_backbone)\n",
    "        \n",
    "    writer = pd.ExcelWriter(models['address'].replace('.csv', '_confusion.xlsx'))\n",
    "\n",
    "    for i, embedding_model in enumerate(tqdm(models['model'])):\n",
    "        if embedding_model in baselines:\n",
    "            pass\n",
    "        else:\n",
    "            embedding_model=embedding_model.backbone\n",
    "        X_train_embedding = [embed(x, embedding_model, device) for x in DataLoader(X_train, batch_size=16)]\n",
    "        X_train_embedding = np.concatenate(X_train_embedding)\n",
    "\n",
    "        X_test_embedding = [embed(x, embedding_model, device) for x in DataLoader(X_test, batch_size=16)]\n",
    "        X_test_embedding = np.concatenate(X_test_embedding)\n",
    "\n",
    "        if X_test_embedding is None:\n",
    "            accuracy = 'model_collapse'\n",
    "            confusion = None\n",
    "        else:\n",
    "            clf = SGDClassifier(loss='log_loss')\n",
    "\n",
    "            clf.fit(X_train_embedding, y_train)\n",
    "\n",
    "            # Get class probabilities for each sample\n",
    "            class_probs = clf.predict_proba(X_test_embedding)\n",
    "\n",
    "            # Get the top 1 predictions\n",
    "            top1_preds = np.argmax(class_probs, axis=1)\n",
    "            top3_preds = np.argpartition(class_probs, -3, axis=1)[:,-3:]\n",
    "\n",
    "            # Calculate confusion matrix\n",
    "            confusion = confusion_matrix(y_test, top1_preds)\n",
    "            # Normalize confusion matrix by row (i.e by the number of samples in each class)\n",
    "            confusion = confusion.astype('float') / confusion.sum(axis=1)[:, np.newaxis]\n",
    "            # Average accuracy is the mean of the diagonal elements (the correctly classified instances)\n",
    "            top1_average_accuracy = np.mean(np.diag(confusion))\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            top1_accuracy = accuracy_score(y_test, top1_preds)\n",
    "            top3_accuracy = np.mean([1 if y in top3 else 0 for y, top3 in zip(y_test, top3_preds)])\n",
    "\n",
    "            # K-Nearest Neighbors classifier\n",
    "            knn = KNeighborsClassifier(n_neighbors=5)\n",
    "            knn.fit(X_train_embedding, y_train)\n",
    "            knn_preds = knn.predict(X_test_embedding)\n",
    "            knn_accuracy = accuracy_score(y_test, knn_preds)\n",
    "\n",
    "            accuracy = {\n",
    "                \"Top-1 Accuracy\": top1_accuracy,\n",
    "                \"Top-3 Accuracy\": top3_accuracy,\n",
    "                \"Top-1 Average Accuracy\": top1_average_accuracy,\n",
    "                \"KNN Top-1 Accuracy\": knn_accuracy  # KNN accuracy\n",
    "            }\n",
    "\n",
    "        namee = models[\"name\"][i]\n",
    "        results.append((namee, accuracy))\n",
    "\n",
    "        \n",
    "        del embedding_model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Save confusion matrix to Excel file\n",
    "        if confusion is not None:\n",
    "            df_confusion = pd.DataFrame(confusion)\n",
    "            df_confusion.to_excel(writer, sheet_name=namee)\n",
    "\n",
    "    writer.save()\n",
    "\n",
    "    if models['address'] is not None:\n",
    "        df = pd.read_csv(models['address'])\n",
    "\n",
    "        # If baselines are present, add new rows in the dataframe for them\n",
    "        if baselines:\n",
    "            for base, base_name in zip(baselines, baselines_name):\n",
    "                # Initialize a new row with default values\n",
    "                new_row = {col: None for col in df.columns}\n",
    "                # Update the values we know\n",
    "                new_row.update({\n",
    "                    'dir_name': base_name,\n",
    "                })\n",
    "                # Append the new row to the dataframe\n",
    "                df = df.append(new_row, ignore_index=True)\n",
    "\n",
    "        # Assuming the results are in the same order as the models in the dataframe\n",
    "        df['linear_top1_accuracy'] = [result[1][\"Top-1 Accuracy\"] for result in results]\n",
    "        df['linear_top3_accuracy'] = [result[1][\"Top-3 Accuracy\"] for result in results]\n",
    "        df['linear_top1_average_accuracy'] = [result[1][\"Top-1 Average Accuracy\"] for result in results]\n",
    "        df['linear_knn_top1_accuracy'] = [result[1][\"KNN Top-1 Accuracy\"] for result in results]\n",
    "\n",
    "        df.to_csv(models['address'], index=False)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db575964-025f-4b6f-991c-b8f7894b9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8ad6455-b8c3-4d5d-b6eb-5627bd52fbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collating the models' (evaluating) information to experiment_checkpoints/paper_barlowtwins_xBD/^BT_res50_64_100_.+$.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_augmentation = {\n",
    "    'RandomResizedCrop': {'size': (global_config[\"input_size\"], global_config[\"input_size\"])},\n",
    "    'ToTensor': {},\n",
    "    'Normalize': {\"mean\": [0.485, 0.456, 0.406], \"std\": [0.229, 0.224, 0.225]}\n",
    "}\n",
    "collate =pipe_collate(address=\"experiment_checkpoints/paper_barlowtwins_xBD/\", reg=f\"^BT_res50_64_100_.+$\")\n",
    "test_augmentation = {\n",
    "    #'RandomResizedCrop': {'size': global_config[\"input_size\"], \"scale\":(0.2,1.0)},\n",
    "    'ToTensor': {},\n",
    "    'Normalize': {\"mean\": [0.485, 0.456, 0.406], \"std\": [0.229, 0.224, 0.225]}\n",
    "} \n",
    "\n",
    "pdata = PipeDataset(input_dir=global_config[\"path_to_test_xBD\"],   #\n",
    "    augmentation=dict2transformer(test_augmentation,view=1)) #,samples=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca362de1-b68b-45c8-94f6-853afff878aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the training and testing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:17<00:00, 37.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:17<00:00, 37.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 280/280 [00:07<00:00, 37.55it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 280/280 [00:07<00:00, 38.27it/s]\n",
      " 17%|██████████████                                                                      | 1/6 [00:05<00:27,  5.49s/it]C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\sklearn\\linear_model\\_base.py:440: RuntimeWarning: invalid value encountered in divide\n",
      "  prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))\n",
      " 33%|████████████████████████████                                                        | 2/6 [00:16<00:35,  8.90s/it]C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\sklearn\\linear_model\\_base.py:440: RuntimeWarning: invalid value encountered in divide\n",
      "  prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))\n",
      " 50%|██████████████████████████████████████████                                          | 3/6 [00:28<00:30, 10.06s/it]C:\\Users\\isxzl\\anaconda3\\envs\\AutoGPT\\lib\\site-packages\\sklearn\\linear_model\\_base.py:440: RuntimeWarning: invalid value encountered in divide\n",
      "  prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:49<00:00,  8.17s/it]\n",
      "C:\\Users\\isxzl\\AppData\\Local\\Temp\\ipykernel_1080\\3203676280.py:121: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
      "  writer.save()\n",
      "C:\\Users\\isxzl\\AppData\\Local\\Temp\\ipykernel_1080\\3203676280.py:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(new_row, ignore_index=True)\n",
      "C:\\Users\\isxzl\\AppData\\Local\\Temp\\ipykernel_1080\\3203676280.py:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(new_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "aaa=eval_linear(pdata, models=collate, device=global_config[\"device\"],baseline=[\"resnet18\",\"resnet18_pretrained\"],split=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff8f9096-89ab-4926-8c60-cca73920c337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BT_res50_64_100_0.1',\n",
       "  {'Top-1 Accuracy': 0.32857142857142857,\n",
       "   'Top-3 Accuracy': 0.8285714285714286,\n",
       "   'Top-1 Average Accuracy': 0.35995708947199306,\n",
       "   'KNN Top-1 Accuracy': 0.5}),\n",
       " ('BT_res50_64_100_0.25',\n",
       "  {'Top-1 Accuracy': 0.475,\n",
       "   'Top-3 Accuracy': 0.8714285714285714,\n",
       "   'Top-1 Average Accuracy': 0.46320776437667144,\n",
       "   'KNN Top-1 Accuracy': 0.5428571428571428}),\n",
       " ('BT_res50_64_100_0.5',\n",
       "  {'Top-1 Accuracy': 0.525,\n",
       "   'Top-3 Accuracy': 0.8785714285714286,\n",
       "   'Top-1 Average Accuracy': 0.49519913166903234,\n",
       "   'KNN Top-1 Accuracy': 0.5571428571428572}),\n",
       " ('BT_res50_64_100_1',\n",
       "  {'Top-1 Accuracy': 0.5107142857142857,\n",
       "   'Top-3 Accuracy': 0.8642857142857143,\n",
       "   'Top-1 Average Accuracy': 0.5047861937400219,\n",
       "   'KNN Top-1 Accuracy': 0.5678571428571428}),\n",
       " ('baseline_resnet18',\n",
       "  {'Top-1 Accuracy': 0.3821428571428571,\n",
       "   'Top-3 Accuracy': 0.6678571428571428,\n",
       "   'Top-1 Average Accuracy': 0.4069765029437735,\n",
       "   'KNN Top-1 Accuracy': 0.42142857142857143}),\n",
       " ('baseline_resnet18_pretrained',\n",
       "  {'Top-1 Accuracy': 0.575,\n",
       "   'Top-3 Accuracy': 0.9,\n",
       "   'Top-1 Average Accuracy': 0.4499581268137668,\n",
       "   'KNN Top-1 Accuracy': 0.6357142857142857})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed89fa6-f9b9-4af6-8531-da9c2729d020",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "test_augmentation = {\n",
    "    'RandomResizedCrop': {'size': (global_config[\"input_size\"], global_config[\"input_size\"])},\n",
    "    'ToTensor': {},\n",
    "    'Normalize': {\"mean\": [0.485, 0.456, 0.406], \"std\": [0.229, 0.224, 0.225]}\n",
    "}\n",
    "collate =pipe_collate(address=\"experiment_checkpoints/paper_barlowtwins_xBD/\", reg=\"BT_res50_64_100_[0-9]+\")\n",
    "\n",
    "pdata = PipeDataset(input_dir=global_config[\"path_to_test_xBD\"],\n",
    "    augmentation=dict2transformer(test_augmentation,view=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a19fd-fa88-4c31-a829-e689313f04d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa=eval_linear(pdata, models=collate, device=global_config[\"device\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
